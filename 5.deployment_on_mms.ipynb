{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5. Deployment on MMS(Multi Model Server)\n",
    "---\n",
    "\n",
    "본 모듈에서는 모델의 배포(deployment)를 수행합니다. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Inference script\n",
    "---\n",
    "\n",
    "아래 코드 셀은 `src` 디렉토리에 SageMaker 추론 스크립트인 `inference.py`를 저장합니다.<br>\n",
    "\n",
    "이 스크립트는 SageMaker 상에서 MMS(Multi Model Server)를 쉽고 편하게 배포할 수 이는 high-level 툴킷인 SageMaker inference toolkit의 인터페이스를\n",
    "사용하고 있으며, 여러분께서는 인터페이스에 정의된 핸들러(handler) 함수들만 구현하시면 됩니다.\n",
    "\n",
    "#### MMS(Multi Model Server)란?\n",
    "- [https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server) (2017년 12월 초 MXNet 1.0 릴리스 시 최초 공개, MXNet용 모델 서버로 시작)\n",
    "- Prerequisites: Java 8, MXNet (단, MXNet 사용 시에만)\n",
    "- MMS는 프레임워크에 구애받지 않도록 설계되었기 때문에, 모든 프레임워크의 백엔드 엔진 역할을 할 수 있는 충분한 유연성을 제공합니다.\n",
    "- SageMaker MXNet 추론 컨테이너와 PyTorch 추론 컨테이너는 SageMaker inference toolkit으로 MMS를 래핑하여 사용합니다.\n",
    "    - 2020년 4월 말 PyTorch용 배포 웹 서비스인 torchserve가 출시되면서, 향후 PyTorch 추론 컨테이너는 MMS 기반에서 torchserve 기반으로 마이그레이션될 예정입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/inference.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "import boto3\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import copy\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch import topk\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "\n",
    "# Loads the model into memory from storage and return the model.\n",
    "def model_fn(model_dir):\n",
    "    logger.info(\"==> model_dir : {}\".format(model_dir))\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    last_hidden_units = model.fc.in_features\n",
    "    model.fc = torch.nn.Linear(last_hidden_units, 186)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pt')))\n",
    "    return model\n",
    "\n",
    "# Deserialize the request body\n",
    "def input_fn(request_body, request_content_type='application/x-image'):\n",
    "    print('An input_fn that loads a image tensor')\n",
    "    print(request_content_type)\n",
    "    if request_content_type == 'application/x-image':             \n",
    "        img = np.array(Image.open(io.BytesIO(request_body)))\n",
    "    elif request_content_type == 'application/x-npy':    \n",
    "        img = np.frombuffer(request_body, dtype='uint8').reshape(137, 236)   \n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Requested unsupported ContentType in content_type : ' + request_content_type)\n",
    "\n",
    "    img = 255 - img\n",
    "    img = img[:,:,np.newaxis]\n",
    "    img = np.repeat(img, 3, axis=2)    \n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img_tensor = test_transforms(img)\n",
    "\n",
    "    return img_tensor         \n",
    "        \n",
    "\n",
    "# Predicts on the deserialized object with the model from model_fn()\n",
    "def predict_fn(input_data, model):\n",
    "    logger.info('Entering the predict_fn function')\n",
    "    start_time = time.time()\n",
    "    input_data = input_data.unsqueeze(0)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_data = input_data.to(device)\n",
    "                          \n",
    "    result = {}\n",
    "                                                 \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_data)\n",
    "        pred_probs = F.softmax(logits, dim=1).data.squeeze()   \n",
    "        outputs = topk(pred_probs, 5)                  \n",
    "        result['score'] = outputs[0].detach().cpu().numpy()\n",
    "        result['class'] = outputs[1].detach().cpu().numpy()\n",
    "    \n",
    "    print(\"--- Elapsed time: %s secs ---\" % (time.time() - start_time))    \n",
    "    return result        \n",
    "\n",
    "# Serialize the prediction result into the response content type\n",
    "def output_fn(pred_output, accept=JSON_CONTENT_TYPE):\n",
    "    return json.dumps({'score': pred_output['score'].tolist(), \n",
    "                       'class': pred_output['class'].tolist()}), accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Local Endpoint Inference\n",
    "---\n",
    "\n",
    "충분한 검증 및 테스트 없이 훈련된 모델을 곧바로 실제 운영 환경에 배포하기에는 많은 위험 요소들이 있습니다. 따라서, 로컬 모드를 사용하여 실제 운영 환경에 배포하기 위한 추론 인스턴스를 시작하기 전에 노트북 인스턴스의 로컬 환경에서 모델을 배포하는 것을 권장합니다. 이를 로컬 모드 엔드포인트(Local Mode Endpoint)라고 합니다.\n",
    "\n",
    "먼저, 로컬 모드 엔드포인트의 컨테이너 배포 이전에 로컬 환경 상에서 직접 추론을 수행하여 결과를 확인하고, 곧바로 로컬 모드 엔드포인트를 배포해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content_type='application/x-image'` 일 경우 추론을 수행하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An input_fn that loads a image tensor\n",
      "application/x-image\n",
      "==> model_dir : ./model\n",
      "Entering the predict_fn function\n",
      "--- Elapsed time: 3.42805814743042 secs ---\n",
      "{'score': array([0.62198865, 0.2314413 , 0.04159949, 0.02067479, 0.0189735 ],\n",
      "      dtype=float32), 'class': array([  3,   2, 169, 168,  70])}\n"
     ]
    }
   ],
   "source": [
    "from src.inference import model_fn, input_fn, predict_fn, output_fn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "file_path = 'test_imgs/test_0.jpg'\n",
    "with open(file_path, mode='rb') as file:\n",
    "    img_byte = bytearray(file.read())\n",
    "data = input_fn(img_byte)\n",
    "model = model_fn('./model')\n",
    "result = predict_fn(data, model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content_type='application/x-npy'` 일 경우 추론을 수행하는 예시이며, numpy 행렬을 그대로 전송하게 됩니다. 속도는 `content_type='application/x-image'` 보다 더 빠르지만, `tobytes()`로 \n",
    "변환하여 전송할 경우 numpy 행렬의 `dtype`과 행렬 `shape`이 보존되지 않으므로 별도의 처리가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An input_fn that loads a image tensor\n",
      "application/x-npy\n",
      "==> model_dir : ./model\n",
      "Entering the predict_fn function\n",
      "--- Elapsed time: 0.01936507225036621 secs ---\n",
      "{'score': array([0.62198865, 0.2314413 , 0.04159949, 0.02067479, 0.0189735 ],\n",
      "      dtype=float32), 'class': array([  3,   2, 169, 168,  70])}\n"
     ]
    }
   ],
   "source": [
    "img_arr = np.array(Image.open(file_path))\n",
    "data = input_fn(img_arr.tobytes(), request_content_type='application/x-npy')\n",
    "model = model_fn('./model')\n",
    "result = predict_fn(data, model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "아래 코드 셀을 실행 후, 로그를 확인해 보세요. MMS에 대한 세팅값들을 확인하실 수 있습니다.\n",
    "\n",
    "```bash\n",
    "algo-1-cgw5k_1  | 2020-08-19 00:26:27,519 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
    "algo-1-cgw5k_1  | MMS Home: /opt/conda/lib/python3.6/site-packages\n",
    "algo-1-cgw5k_1  | Current directory: /\n",
    "algo-1-cgw5k_1  | Temp directory: /home/model-server/tmp\n",
    "algo-1-cgw5k_1  | Number of GPUs: 0\n",
    "algo-1-cgw5k_1  | Number of CPUs: 8\n",
    "algo-1-cgw5k_1  | Max heap size: 13646 M\n",
    "algo-1-cgw5k_1  | Python executable: /opt/conda/bin/python\n",
    "algo-1-cgw5k_1  | Config file: /etc/sagemaker-mms.properties\n",
    "algo-1-cgw5k_1  | Inference address: http://0.0.0.0:8080\n",
    "algo-1-cgw5k_1  | Management address: http://0.0.0.0:8080\n",
    "algo-1-cgw5k_1  | Model Store: /.sagemaker/mms/models\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmp2r4844_r_algo-1-vxhvg_1\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,279 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m MMS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Current directory: /\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Max heap size: 13646 M\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Python executable: /opt/conda/bin/python\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Model Store: /.sagemaker/mms/models\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Initial Models: ALL\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Log dir: /logs\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Netty threads: 0\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Netty client threads: 0\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Default workers per model: 8\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,346 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,365 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,545 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,546 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]47\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,546 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,547 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]52\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,553 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,554 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]51\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,554 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,554 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,555 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]50\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,555 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,554 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,555 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,556 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,557 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,557 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,557 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,559 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,560 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,560 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,560 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,562 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,562 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]54\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,563 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,563 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,564 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,564 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,565 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]49\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,565 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,566 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,566 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,566 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,567 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]48\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,567 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,567 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,568 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,587 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,587 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]53\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,587 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,587 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,587 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.6\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,615 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,616 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9003.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,616 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,616 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9002.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,617 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9007.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,617 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9005.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,618 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9004.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,618 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9006.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:56,619 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9001.\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m Model server started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:57,082 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 420\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:57,089 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 411\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:57,093 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 443\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:57,095 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 445\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:57,099 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 437\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:57,111 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 465\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:57,114 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 464\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:57,126 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 460\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:58,432 [INFO ] pool-1-thread-9 ACCESS_LOG - /172.18.0.1:57212 \"GET /ping HTTP/1.1\" 200 12\n",
      "!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.pytorch.model.PyTorchPredictor at 0x7f63ad36a9b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model_path = f'file://{os.getcwd()}/model/model.tar.gz'\n",
    "endpoint_name = \"local-endpoint-bangali-classifier-{}\".format(int(time.time()))\n",
    "\n",
    "local_pytorch_model = PyTorchModel(model_data=local_model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='./src/inference.py',\n",
    "                                   framework_version='1.3.1',\n",
    "                                   py_version='py3')\n",
    "\n",
    "local_pytorch_model.deploy(instance_type='local', \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로컬에서 컨테이너를 배포했기 때문에 컨테이너가 현재 실행 중임을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE                                                                          COMMAND                  CREATED             STATUS              PORTS                              NAMES\r\n",
      "85b681e61ed8        763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.3.1-cpu-py3   \"python /usr/local/b…\"   5 seconds ago       Up 3 seconds        0.0.0.0:8080->8080/tcp, 8081/tcp   tmp2r4844_r_algo-1-vxhvg_1\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker SDK `predict()` 메서드로 추론을 수행할 수도 있지만, 이번에는 boto3의 `invoke_endpoint()` 메서드로 추론을 수행해 보겠습니다.<br>\n",
    "Boto3는 서비스 레벨의 low-level SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 high-level SDK인 SageMaker SDK와 달리\n",
    "SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다.\n",
    "\n",
    "참고로 `invoke_endpoint()` 호출을 위한 런타임 클라이언트 인스턴스 생성 시, 로컬 배포 모드에서는 `sagemaker.local.LocalSagemakerRuntimeClient()`를 호출해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:58,917 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /opt/ml/model\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:58,917 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /opt/ml/model\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,200 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,200 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,300 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,400 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  33%|███▎      | 14.7M/44.7M [00:00<00:00, 154MB/s]\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,492 [WARN ] W-9007-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -  66%|██████▌   | 29.6M/44.7M [00:00<00:00, 154MB/s]\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,638 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - An input_fn that loads a image tensor\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,638 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - application/x-npy\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,638 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Entering the predict_fn function\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,639 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Entering the predict_fn function\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,710 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - --- Elapsed time: 0.07123494148254395 secs ---\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,710 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 993\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,711 [INFO ] W-9007-model ACCESS_LOG - /172.18.0.1:57216 \"POST /invocations HTTP/1.1\" 200 998\n",
      "{\"score\": [0.6219883561134338, 0.23144130408763885, 0.04159952700138092, 0.02067478932440281, 0.018973516300320625], \"class\": [3, 2, 169, 168, 70]}\n"
     ]
    }
   ],
   "source": [
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = local_pytorch_model.endpoint_name\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-npy',\n",
    "    Accept='application/json',\n",
    "    Body=img_arr.tobytes()\n",
    "    )\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,911 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /opt/ml/model\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:17:59,911 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - ==> model_dir : /opt/ml/model\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:18:00,258 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - An input_fn that loads a image tensor\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:18:00,258 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - application/x-image\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:18:00,258 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Entering the predict_fn function\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:18:00,258 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Entering the predict_fn function\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:18:00,330 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 611\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:18:00,330 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - --- Elapsed time: 0.07162833213806152 secs ---\n",
      "\u001b[36malgo-1-vxhvg_1  |\u001b[0m 2020-08-19 10:18:00,330 [INFO ] W-9002-model ACCESS_LOG - /172.18.0.1:57216 \"POST /invocations HTTP/1.1\" 200 612\n",
      "{'score': [0.6219883561134338, 0.23144130408763885, 0.04159952700138092, 0.02067478932440281, 0.018973516300320625], 'class': [3, 2, 169, 168, 70]}\n"
     ]
    }
   ],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-image',\n",
    "    Accept='application/json',\n",
    "    Body=img_byte\n",
    "    )\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    response = client.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "\n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    client.delete_endpoint_config(EndpointConfigName=endpoint_name)    \n",
    "    \n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')\n",
    "    print(f'--- Deleted endpoint_config: {endpoint_name}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "--- Deleted model: pytorch-inference-2020-08-19-10-17-52-283\n",
      "--- Deleted endpoint: local-endpoint-bangali-classifier-1597832268\n",
      "--- Deleted endpoint_config: local-endpoint-bangali-classifier-1597832268\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint(client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컨테이너가 삭제된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. SageMaker Hosted Endpoint Inference\n",
    "---\n",
    "\n",
    "이제 실제 운영 환경에 엔드포인트 배포를 수행해 보겠습니다. 로컬 모드 엔드포인트와 대부분의 코드가 동일하며, 모델 아티팩트 경로(`model_data`)와 인스턴스 유형(`instance_type`)만 변경해 주시면 됩니다. SageMaker가 관리하는 배포 클러스터를 프로비저닝하는 시간이 소요되기 때문에 추론 서비스를 시작하는 데에는 약 5~10분 정도 소요됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(sm_client, max_results=1, name_contains='pytorch'):\n",
    "    training_job = sm_client.list_training_jobs(MaxResults=max_results,\n",
    "                                         NameContains=name_contains,\n",
    "                                         SortBy='CreationTime', \n",
    "                                         SortOrder='Descending')\n",
    "    training_job_name = training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "    training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "    model_path = training_job_description['ModelArtifacts']['S3ModelArtifacts']  \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = get_model_path(client, max_results=3)\n",
    "endpoint_name = \"endpoint-bangali-classifier-{}\".format(int(time.time()))\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='./src/inference.py',\n",
    "                                   framework_version='1.3.1',\n",
    "                                   py_version='py3')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m5.xlarge', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name=endpoint_name,\n",
    "                                 wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'endpoint-bangali-classifier-1597722095',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:143656149352:endpoint/endpoint-bangali-classifier-1597722095',\n",
       " 'EndpointConfigName': 'endpoint-bangali-classifier-1597722095',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.3.1-cpu-py3',\n",
       "     'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference@sha256:a1b552fe76355d9f921c896c03ea985b00d7829f6f95c3ffc1d8bae9f29d8626',\n",
       "     'ResolutionTime': datetime.datetime(2020, 8, 18, 3, 41, 43, 604000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2020, 8, 18, 3, 41, 41, 692000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 8, 18, 3, 47, 58, 503000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'b5f2a32d-a9a4-40ff-8447-9b1d2669e9dc',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b5f2a32d-a9a4-40ff-8447-9b1d2669e9dc',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '767',\n",
       "   'date': 'Tue, 18 Aug 2020 04:10:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "endpoint_name = pytorch_model.endpoint_name\n",
    "client.describe_endpoint(EndpointName = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론을 수행합니다. 로컬 모드의 코드와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': [0.6219883561134338, 0.23144130408763885, 0.04159952700138092, 0.02067478932440281, 0.01897350326180458], 'class': [3, 2, 169, 168, 70]}\n"
     ]
    }
   ],
   "source": [
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/x-image',\n",
    "    Accept='application/json',\n",
    "    Body=img_byte\n",
    "    )\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hosted Endpoint Clean-up\n",
    "\n",
    "엔드포인트를 계속 사용하지 않는다면, 불필요한 과금을 피하기 위해 엔드포인트를 삭제해야 합니다. \n",
    "SageMaker SDK에서는 `delete_endpoint()` 메소드로 간단히 삭제할 수 있으며, UI에서도 쉽게 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Deleted model pytorch-inference-2020-08-18-03-41-40-100\n",
      "--- Deleted endpoint endpoint-bangali-classifier-1597722095\n",
      "--- Deleted endpoint_config endpoint-bangali-classifier-1597722095\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint(client, endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
