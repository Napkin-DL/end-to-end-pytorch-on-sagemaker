{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modlue 3. Distributed-Mutigpu Training with CutMix-ScriptMode\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amzaon SageMaker API을 효과적으로 이용하기 위해 distributed-multigpu 학습을 위한 PyTorch 프레임워크 자체 구현만으로 모델 훈련을 수행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time, datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 버전을 최신 버전으로 업데이트합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.4.6)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (1.18.1)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (4.2.0.32)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (1.14.0)\n",
      "Requirement already satisfied: Shapely in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (1.7.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (7.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (3.1.3)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (0.16.2)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (2.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.1.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (2.4)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations) (46.1.3.post20200330)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (4.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U torch \n",
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import boto3\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch, torchvision\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "job_bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "data_bucket = 'sagemaker-{}-{}'.format(sess.region_name, account_id)\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_paths = glob.glob('./input/train_image_data_*.parquet')\n",
    "# s3_data_parquet_path = 's3://{}/{}'.format(data_bucket, 'BangaliDataset/list/parquet/')\n",
    "s3_data_path = 's3://{}/{}'.format(data_bucket, 'bangali/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = 'bangali/train'\n",
    "# s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}'.format(data_bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local_data_path in local_data_paths:\n",
    "#     !aws s3 cp $local_data_path $s3_data_parquet_path\n",
    "    \n",
    "# !aws s3 cp ./input/train_folds.csv $s3_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_data_path = './input/train_images/'\n",
    "# s3_data_path = 's3://{}/{}'.format(data_bucket, 'BangaliDataset/bantrain_images/')\n",
    "\n",
    "# !aws s3 cp --recursive $local_data_path $s3_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write main_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "albumentations\n",
    "pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/main_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/main_trainer.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "import logging.handlers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import dis_util\n",
    "import sagemaker_containers\n",
    "import util\n",
    "\n",
    "## Apex import package\n",
    "try:\n",
    "    from apex.parallel import DistributedDataParallel as DDP\n",
    "    from apex.fp16_utils import *\n",
    "    from apex import amp, optimizers\n",
    "    from apex.multi_tensor_apply import multi_tensor_applier\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
    "\n",
    "\n",
    "## augmentation for setting\n",
    "from albumentations import (\n",
    "    Rotate,HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    ")\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "\n",
    "import dis_util\n",
    "import sagemaker_containers\n",
    "import util\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def parser_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Default Setting\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default='nccl',\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "    parser.add_argument('--channels-last', type=bool, default=True)\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('-p', '--print-freq', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "\n",
    "    # Hyperparameter Setting\n",
    "    parser.add_argument('--model_name', type=str, default='resnet50')\n",
    "    parser.add_argument('--height', type=int, default=128)\n",
    "    parser.add_argument('--width', type=int, default=128)\n",
    "    parser.add_argument('--num_folds', type=int, default=5)\n",
    "    parser.add_argument('--vld_fold_idx', type=int, default=4)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--num_epochs', type=int, default=3)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument('--test-batch-size', type=int, default=200, metavar='N',\n",
    "                        help='input batch size for testing (default: 200)')\n",
    "\n",
    "    # APEX Setting for Distributed Training\n",
    "    parser.add_argument('--apex', type=bool, default=False)\n",
    "    parser.add_argument('--opt-level', type=str, default='O0')\n",
    "    parser.add_argument('--keep-batchnorm-fp32', type=str, default=None)\n",
    "    parser.add_argument('--loss-scale', type=str, default=None)\n",
    "    parser.add_argument('--sync_bn', action='store_true',\n",
    "                        help='enabling apex sync BN.')\n",
    "    parser.add_argument('--prof', default=-1, type=int,\n",
    "                        help='Only run 10 iterations for profiling.')\n",
    "\n",
    "    # SageMaker Container environment\n",
    "    parser.add_argument('--hosts', type=list,\n",
    "                        default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current-host', type=str,\n",
    "                        default=os.environ['SM_CURRENT_HOST'])\n",
    "    parser.add_argument('--model-dir', type=str,\n",
    "                        default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--data-dir', type=str,\n",
    "                        default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--num-gpus', type=int,\n",
    "                        default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--output_data_dir', type=str,\n",
    "                        default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        last_hidden_units = self.model.fc.out_features\n",
    "        self.classifer_model = nn.Linear(last_hidden_units, 186)\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.model(x)\n",
    "        x = self.classifer_model(features)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class BangaliDataset(Dataset):\n",
    "    def __init__(self, imgs, label_df=None, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.label_df = label_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_idx = self.label_df.iloc[idx].id\n",
    "        img = (self.imgs[img_idx]).astype(np.uint8)\n",
    "        img = 255 - img\n",
    "    \n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']        \n",
    "        \n",
    "        if self.label_df is not None:\n",
    "            label_1 = self.label_df.iloc[idx].grapheme_root\n",
    "            label_2 = self.label_df.iloc[idx].vowel_diacritic\n",
    "            label_3 = self.label_df.iloc[idx].consonant_diacritic           \n",
    "            return img, np.array([label_1, label_2, label_3])        \n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "        \n",
    "\n",
    "def _rand_bbox(size, lam):\n",
    "    '''\n",
    "    CutMix Helper function.\n",
    "    Retrieved from https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "    '''\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    # 폭과 높이는 주어진 이미지의 폭과 높이의 beta distribution에서 뽑은 lambda로 얻는다\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    \n",
    "    # patch size 의 w, h 는 original image 의 w,h 에 np.sqrt(1-lambda) 를 곱해준 값입니다.\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # patch의 중심점은 uniform하게 뽑힘\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def _set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    mx.random.seed(seed)\n",
    "\n",
    "def _get_images(args, data_type='train'):\n",
    "\n",
    "    logger.info(\"=== Getting Labels ===\")\n",
    "    logger.info(args.data_dir)\n",
    "    \n",
    "    label_df = pd.read_csv(os.path.join(args.data_dir, 'train_folds.csv'))\n",
    "    #label_df = pd.read_csv(f'{train_dir}/train_folds.csv')\n",
    "     \n",
    "    trn_fold = [i for i in range(args.num_folds) if i not in [args.vld_fold_idx]]\n",
    "    vld_fold = [args.vld_fold_idx]\n",
    "\n",
    "    trn_idx = label_df.loc[label_df['fold'].isin(trn_fold)].index\n",
    "    vld_idx = label_df.loc[label_df['fold'].isin(vld_fold)].index\n",
    "\n",
    "    logger.info(\"=== Getting Images ===\")    \n",
    "    #files = sorted(glob2.glob(f'{train_dir}/{data_type}_*.parquet'))\n",
    "    files = [f'{args.data_dir}/{data_type}_image_data_{i}.parquet' for i in range(4)]\n",
    "    logger.info(files)\n",
    "    \n",
    "    image_df_list = [pd.read_parquet(f) for f in files]\n",
    "    imgs = [df.iloc[:, 1:].values.reshape(-1, args.height, args.width) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    args.imgs = np.concatenate(imgs, axis=0)\n",
    "    \n",
    "    args.trn_df = label_df.loc[trn_idx]\n",
    "    args.vld_df = label_df.loc[vld_idx]\n",
    "    \n",
    "    return args \n",
    "\n",
    "\n",
    "def _get_train_data_loader(args, **kwargs):\n",
    "    logger.info(\"Get train data loader\")\n",
    "    train_transforms = Compose([\n",
    "        Rotate(20),\n",
    "            OneOf([\n",
    "                IAAAdditiveGaussianNoise(),\n",
    "                GaussNoise(),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                MotionBlur(p=.2),\n",
    "                MedianBlur(blur_limit=3, p=0.1),\n",
    "                Blur(blur_limit=3, p=0.1),\n",
    "            ], p=0.2),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "            OneOf([\n",
    "                OpticalDistortion(p=0.3),\n",
    "                GridDistortion(p=.1),\n",
    "                IAAPiecewiseAffine(p=0.3),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                CLAHE(clip_limit=2),\n",
    "                IAASharpen(),\n",
    "                IAAEmboss(),\n",
    "                RandomBrightnessContrast(),            \n",
    "            ], p=0.3),\n",
    "            HueSaturationValue(p=0.3),\n",
    "        ToTensor()\n",
    "        ], p=1.0)\n",
    "    \n",
    "    dataset = BangaliDataset(imgs=args.imgs, label_df=args.trn_df, transform=train_transforms)\n",
    "    train_sampler = data.distributed.DistributedSampler(\n",
    "        dataset, num_replicas=int(args.world_size), rank=int(args.rank)) if args.multigpus_distributed else None\n",
    "    return data.DataLoader(dataset, batch_size=args.batch_size, shuffle=train_sampler is None,\n",
    "                                       sampler=train_sampler, collate_fn=dis_util.fast_collate, **kwargs), train_sampler\n",
    "\n",
    "\n",
    "def _get_test_data_loader(args, **kwargs):\n",
    "    logger.info(\"Get test data loader\")   \n",
    "\n",
    "    dataset = BangaliDataset(imgs=args.imgs, label_df=args.vld_df)\n",
    "    val_sampler = data.distributed.DistributedSampler(dataset) if args.multigpus_distributed else None\n",
    "    return data.DataLoader(dataset, batch_size=args.test_batch_size, shuffle=False, \n",
    "                           sampler=val_sampler, collate_fn=dis_util.fast_collate, **kwargs)\n",
    "\n",
    "\n",
    "def train(current_gpu, args):\n",
    "    best_acc1 = -1\n",
    "    model_history = {}\n",
    "    model_history = util.init_modelhistory(model_history)\n",
    "    train_start = time.time()\n",
    "\n",
    "    ## choose model from pytorch model_zoo\n",
    "    model = util.torch_model(args.model_name, pretrained=True)\n",
    "    loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    ## distributed_setting \n",
    "    model, args = dis_util.dist_setting(current_gpu, model, loss_fn, args)\n",
    "\n",
    "    ## CuDNN library will benchmark several algorithms and pick that which it found to be fastest\n",
    "    cudnn.benchmark = False if args.seed else True\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    if args.apex:\n",
    "        model, optimizer = dis_util.apex_init(model, optimizer, args)\n",
    "    \n",
    "    \n",
    "#     args.collate_fn = partial(dis_util.fast_collate, memory_format=args.memory_format)\n",
    "   \n",
    "    args = _get_images(args, data_type='train')\n",
    "    train_loader, train_sampler = _get_train_data_loader(args, **args.kwargs)\n",
    "    test_loader = _get_test_data_loader(args, **args.kwargs)\n",
    "\n",
    "    logger.info(\"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "        len(train_loader.sampler), len(train_loader.dataset),\n",
    "        100. * len(train_loader.sampler) / len(train_loader.dataset)\n",
    "    ))\n",
    "\n",
    "    logger.info(\"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "        len(test_loader.sampler), len(test_loader.dataset),\n",
    "        100. * len(test_loader.sampler) / len(test_loader.dataset)\n",
    "    ))\n",
    "\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        ## \n",
    "        batch_time = util.AverageMeter('Time', ':6.3f')\n",
    "        data_time = util.AverageMeter('Data', ':6.3f')\n",
    "        losses = util.AverageMeter('Loss', ':.4e')\n",
    "        top1 = util.AverageMeter('Acc@1', ':6.2f')\n",
    "        top5 = util.AverageMeter('Acc@5', ':6.2f')\n",
    "        progress = util.ProgressMeter(\n",
    "            len(train_loader),\n",
    "            [batch_time, data_time, losses, top1, top5],\n",
    "            prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "        model.train()\n",
    "        end = time.time()\n",
    "        \n",
    "        ## Set epoch count for DistributedSampler\n",
    "        if args.multigpus_distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        \n",
    "        \n",
    "        prefetcher = util.data_prefetcher(train_loader)\n",
    "        input, target = prefetcher.next()\n",
    "        batch_idx = 0\n",
    "        while input is not None:\n",
    "\n",
    "            batch_idx += 1\n",
    "            \n",
    "            if args.prof >= 0 and batch_idx == args.prof:\n",
    "                print(\"Profiling begun at iteration {}\".format(batch_idx))\n",
    "                torch.cuda.cudart().cudaProfilerStart()\n",
    "                \n",
    "            if args.prof >= 0: torch.cuda.nvtx.range_push(\"Body of iteration {}\".format(batch_idx))\n",
    "\n",
    "            util.adjust_learning_rate(optimizer, epoch, batch_idx, len(train_loader), args)\n",
    "            \n",
    "            ##### DATA Processing #####\n",
    "            targets_gra = targets[:, 0]\n",
    "            targets_vow = targets[:, 1]\n",
    "            targets_con = targets[:, 2]\n",
    "\n",
    "            # 50%의 확률로 원본 데이터 그대로 사용    \n",
    "            if np.random.rand() < 0.5:\n",
    "                logits = model(input)\n",
    "                grapheme = logits[:, :168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra)\n",
    "                loss2 = loss_fn(vowel, targets_vow)\n",
    "                loss3 = loss_fn(cons, targets_con) \n",
    "\n",
    "            else:\n",
    "\n",
    "                lam = np.random.beta(1.0, 1.0) \n",
    "                rand_index = torch.randperm(input.size()[0])\n",
    "                shuffled_targets_gra = targets_gra[rand_index]\n",
    "                shuffled_targets_vow = targets_vow[rand_index]\n",
    "                shuffled_targets_con = targets_con[rand_index]\n",
    "\n",
    "                bbx1, bby1, bbx2, bby2 = _rand_bbox(input.size(), lam)\n",
    "                input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # 픽셀 비율과 정확히 일치하도록 lambda 파라메터 조정  \n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "                \n",
    "                logits = model(input)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "                loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "                loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "\n",
    "            loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3    \n",
    "            trn_loss.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #########################################################\n",
    "            \n",
    "            \n",
    "#             # compute output\n",
    "#             if args.prof >= 0: torch.cuda.nvtx.range_push(\"forward\")\n",
    "#             output = model(input)\n",
    "#             if args.prof >= 0: torch.cuda.nvtx.range_pop()\n",
    "#             loss = criterion(output, target)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if args.prof >= 0: torch.cuda.nvtx.range_push(\"backward\")\n",
    "            if args.apex:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            if args.prof >= 0: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "            if args.prof >= 0: torch.cuda.nvtx.range_push(\"optimizer.step()\")\n",
    "            optimizer.step()\n",
    "            if args.prof >= 0: torch.cuda.nvtx.range_pop()\n",
    "            # Printing vital information\n",
    "            if (batch_idx + 1) % (args.log_interval) == 0:\n",
    "                s = f'[Epoch {epoch} Batch {batch_idx+1}/{len(train_loader)}] ' \\\n",
    "                f'loss: {running_loss / args.log_interval:.4f}'\n",
    "                print(s)\n",
    "                running_loss = 0\n",
    "                \n",
    "                \n",
    "#             if True or batch_idx % args.log_interval == 0:\n",
    "#                 # Every print_freq iterations, check the loss, accuracy, and speed.\n",
    "#                 # For best performance, it doesn't make sense to print these metrics every\n",
    "#                 # iteration, since they incur an allreduce and some host<->device syncs.\n",
    "\n",
    "#                 # Measure accuracy\n",
    "#                 prec1, prec5 = util.accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "#                 # Average loss and accuracy across processes for logging\n",
    "#                 if args.multigpus_distributed:\n",
    "#                     reduced_loss = dis_util.reduce_tensor(loss.data, args)\n",
    "#                     prec1 = dis_util.reduce_tensor(prec1, args)\n",
    "#                     prec5 = dis_util.reduce_tensor(prec5, args)\n",
    "#                 else:\n",
    "#                     reduced_loss = loss.data\n",
    "\n",
    "#                 # to_python_float incurs a host<->device sync\n",
    "#                 losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "#                 top1.update(to_python_float(prec1), input.size(0))\n",
    "#                 top5.update(to_python_float(prec5), input.size(0))\n",
    "                \n",
    "#                 ## Waiting until finishing operations on GPU (Pytorch default: async)\n",
    "#                 torch.cuda.synchronize()\n",
    "#                 batch_time.update((time.time() - end)/args.log_interval)\n",
    "#                 end = time.time()\n",
    "\n",
    "#                 if current_gpu == 0:\n",
    "#                     print('Epoch: [{0}][{1}/{2}]  '\n",
    "#                           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "#                           'Speed {3:.3f} ({4:.3f})  '\n",
    "#                           'Loss {loss.val:.10f} ({loss.avg:.4f})  '\n",
    "#                           'Prec@1 {top1.val:.3f} ({top1.avg:.3f})  '\n",
    "#                           'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                               epoch, batch_idx, len(train_loader),\n",
    "#                               args.world_size*args.batch_size/batch_time.val,\n",
    "#                               args.world_size*args.batch_size/batch_time.avg,\n",
    "#                               batch_time=batch_time,\n",
    "#                               loss=losses, top1=top1, top5=top5))\n",
    "#                     model_history['epoch'].append(epoch)\n",
    "#                     model_history['batch_idx'].append(batch_idx)\n",
    "#                     model_history['batch_time'].append(batch_time.val)\n",
    "#                     model_history['losses'].append(losses.val)\n",
    "#                     model_history['top1'].append(top1.val)\n",
    "#                     model_history['top5'].append(top5.val)\n",
    "                    \n",
    "#                 if args.prof >= 0: torch.cuda.nvtx.range_push(\"prefetcher.next()\")\n",
    "#                 input, target = prefetcher.next()\n",
    "#                 if args.prof >= 0: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "#                 # Pop range \"Body of iteration {}\".format(i)\n",
    "#                 if args.prof >= 0: torch.cuda.nvtx.range_pop()\n",
    "                    \n",
    "\n",
    "#                 if args.prof >= 0 and batch_idx == args.prof + 10:\n",
    "#                     print(\"Profiling ended at iteration {}\".format(batch_idx))\n",
    "#                     torch.cuda.cudart().cudaProfilerStop()\n",
    "#                     quit()\n",
    "               \n",
    "        acc1 = validate(test_loader, model, loss_fn, epoch, model_history, args)\n",
    "        \n",
    "        print(\" ****  acc1 :{}\".format(acc1))\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "        if not args.multigpus_distributed or (args.multigpus_distributed and args.rank % args.num_gpus == 0):\n",
    "            util.save_history(os.path.join(args.output_data_dir,\n",
    "                          'model_history.p'), model_history)\n",
    "\n",
    "            util.save_model({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_name': args.model_name,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'class_to_idx' : train_loader.dataset.class_to_idx,\n",
    "            }, is_best, args.model_dir)\n",
    "\n",
    "\n",
    "def validate(val_loader, model, loss_fn, epoch, model_history, args):\n",
    "    batch_time = util.AverageMeter('Time', ':6.3f')\n",
    "    losses = util.AverageMeter('Loss', ':.4e')\n",
    "    top1 = util.AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = util.AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = util.ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    \n",
    "    val_loss = []\n",
    "    val_true = []\n",
    "    val_pred = []    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    prefetcher = util.data_prefetcher(val_loader)\n",
    "    input, target = prefetcher.next()\n",
    "    batch_idx = 0\n",
    "    while input is not None:\n",
    "        batch_idx += 1\n",
    "    \n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "#             data = data.contiguous(memory_format=args.memory_format)\n",
    "#             target = target.contiguous()\n",
    "#             data = data.cuda(non_blocking=True)\n",
    "#             target = target.cuda(non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "                logits = model(input)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss= 0.5* loss_fn(grapheme, targets[:,0]) + 0.25*loss_fn(vowel, targets[:,1]) + \\\n",
    "                0.25*loss_fn(vowel, targets[:,2])\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                grapheme = grapheme.cpu().argmax(dim=1).data.numpy()\n",
    "                vowel = vowel.cpu().argmax(dim=1).data.numpy()\n",
    "                cons = cons.cpu().argmax(dim=1).data.numpy()\n",
    "\n",
    "                val_true.append(targets.cpu().numpy())\n",
    "                val_pred.append(np.stack([grapheme, vowel, cons], axis=1))                \n",
    "\n",
    "        val_true = np.concatenate(val_true)\n",
    "        val_pred = np.concatenate(val_pred)\n",
    "        val_loss = np.mean(val_loss)\n",
    "        trn_loss = np.mean(trn_loss)\n",
    "\n",
    "        score_g = recall_score(val_true[:,0], val_pred[:,0], average='macro')\n",
    "        score_v = recall_score(val_true[:,1], val_pred[:,1], average='macro')\n",
    "        score_c = recall_score(val_true[:,2], val_pred[:,2], average='macro')\n",
    "        final_score = np.average([score_g, score_v, score_c], weights=[2,1,1])\n",
    "\n",
    "        # Printing vital information\n",
    "        s = f'[Epoch {epoch}] ' \\\n",
    "        f'trn_loss: {trn_loss:.4f}, vld_loss: {val_loss:.4f}, score: {final_score:.4f}, ' \\\n",
    "        f'score_each: [{score_g:.4f}, {score_v:.4f}, {score_c:.4f}]'          \n",
    "        print(s)\n",
    "\n",
    "        ################################################################################\n",
    "        # ==> Save checkpoint and training stats\n",
    "        ################################################################################        \n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            state_dict = model.cpu().state_dict()\n",
    "            model = model.cuda()\n",
    "            torch.save(state_dict, os.path.join(args.model_output_dir, 'model.pt'))\n",
    "\n",
    "        # Record all statistics from this epoch\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'trn_loss': trn_loss,\n",
    "                'trn_time': trn_time,            \n",
    "                'val_loss': val_loss,\n",
    "                'score': final_score,\n",
    "                'score_g': score_g,\n",
    "                'score_v': score_v,\n",
    "                'score_c': score_c            \n",
    "            }\n",
    "        )      \n",
    "        \n",
    "        # === Save Model Parameters ===\n",
    "        logger.info(\"Model successfully saved at: {}\".format(args.model_output_dir))      \n",
    "        \n",
    "        \n",
    "#         # measure accuracy and record loss\n",
    "#         prec1, prec5 = util.accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "#         if args.multigpus_distributed:\n",
    "#             reduced_loss = dis_util.reduce_tensor(loss.data, args)\n",
    "#             prec1 = dis_util.reduce_tensor(prec1, args)\n",
    "#             prec5 = dis_util.reduce_tensor(prec5, args)\n",
    "#         else:\n",
    "#             reduced_loss = loss.data\n",
    "\n",
    "#         losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "#         top1.update(to_python_float(prec1), input.size(0))\n",
    "#         top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "#         # TODO:  Change timings to mirror train().\n",
    "#         if args.current_gpu == 0 and batch_idx % args.log_interval == 0:\n",
    "#             print('Test: [{0}/{1}]  '\n",
    "#                   'Time {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "#                   'Speed {2:.3f} ({3:.3f})  '\n",
    "#                   'Loss {loss.val:.4f} ({loss.avg:.4f})  '\n",
    "#                   'Prec@1 {top1.val:.3f} ({top1.avg:.3f})  '\n",
    "#                   'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                       batch_idx, len(val_loader),\n",
    "#                       args.world_size * args.batch_size / batch_time.val,\n",
    "#                       args.world_size * args.batch_size / batch_time.avg,\n",
    "#                       batch_time=batch_time, loss=losses,\n",
    "#                       top1=top1, top5=top5))\n",
    "#             model_history['val_epoch'].append(epoch)\n",
    "#             model_history['val_batch_idx'].append(batch_idx)\n",
    "#             model_history['val_batch_time'].append(batch_time.val)\n",
    "#             model_history['val_losses'].append(losses.val)\n",
    "#             model_history['val_top1'].append(top1.val)\n",
    "#             model_history['val_top5'].append(top5.val)\n",
    "#         input, target = prefetcher.next()\n",
    "\n",
    "#     print('  Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "#           .format(top1=top1, top5=top5))\n",
    "#     model_history['val_avg_epoch'].append(epoch)\n",
    "#     model_history['val_avg_batch_time'].append(batch_time.avg)\n",
    "#     model_history['val_avg_losses'].append(losses.avg)\n",
    "#     model_history['val_avg_top1'].append(top1.avg)\n",
    "#     model_history['val_avg_top5'].append(top5.avg)\n",
    "#     return top1.avg\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parser_args()\n",
    "    args.use_cuda = args.num_gpus > 0\n",
    "    print(\"args.use_cuda : {} , args.num_gpus : {}\".format(\n",
    "        args.use_cuda, args.num_gpus))\n",
    "    args.kwargs = {'num_workers': 4,\n",
    "                   'pin_memory': True} if args.use_cuda else {}\n",
    "    args.device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "    dis_util.dist_init(train, args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/dis_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/dis_util.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import sagemaker_containers\n",
    "\n",
    "try:\n",
    "    from apex.parallel import DistributedDataParallel as DDP\n",
    "    from apex.fp16_utils import *\n",
    "    from apex import amp, optimizers\n",
    "    from apex.multi_tensor_apply import multi_tensor_applier\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def dist_init(fn, args):\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    args.is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "    args.is_multigpus = args.num_gpus > 1\n",
    "    args.multigpus_distributed = (args.is_distributed or args.is_multigpus)\n",
    "\n",
    "    logger.debug(\"Distributed training - {}\".format(args.is_distributed))\n",
    "    logger.debug(\"Number of gpus available - {}\".format(args.num_gpus))\n",
    "\n",
    "    args.world_size = 1\n",
    "    if args.multigpus_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        args.apex = True\n",
    "        args.world_size = len(args.hosts) * args.num_gpus\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        args.host_num = args.hosts.index(args.current_host)\n",
    "        mp.spawn(fn, nprocs=args.num_gpus, args=(args,))\n",
    "    else:\n",
    "        current_gpu = 0\n",
    "        fn(current_gpu, args)\n",
    "\n",
    "\n",
    "def dist_setting(current_gpu, model, loss_fn, args):\n",
    "    print(\"channels_last : {}\".format(args.channels_last))\n",
    "    if args.channels_last:\n",
    "        args.memory_format = torch.channels_last\n",
    "    else:\n",
    "        args.memory_format = torch.contiguous_format\n",
    "\n",
    "    if args.apex:\n",
    "        args.lr = args.lr*float(args.batch_size*args.world_size)/256.\n",
    "    args.current_gpu = current_gpu\n",
    "    if args.current_gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.current_gpu))\n",
    "\n",
    "    if args.multigpus_distributed:\n",
    "        args.rank = args.num_gpus * args.host_num + args.current_gpu\n",
    "        dist.init_process_group(backend=args.backend,\n",
    "                                rank=args.rank, world_size=args.world_size)\n",
    "        logger.info('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\n",
    "            args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n",
    "            dist.get_rank(), args.num_gpus))\n",
    "\n",
    "    if args.sync_bn:\n",
    "        import apex\n",
    "        print(\"using apex synced BN\")\n",
    "        model = apex.parallel.convert_syncbn_model(model)\n",
    "\n",
    "    if args.multigpus_distributed:\n",
    "        if args.current_gpu is not None:\n",
    "            torch.cuda.set_device(args.current_gpu)\n",
    "            args.batch_size = int(args.batch_size / args.num_gpus)\n",
    "            if not args.apex:\n",
    "                model.cuda(args.current_gpu)\n",
    "                model = torch.nn.parallel.DistributedDataParallel(\n",
    "                    model, device_ids=[args.current_gpu])\n",
    "        else:\n",
    "            if not args.apex:\n",
    "                model.cuda()\n",
    "                model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif args.current_gpu is not None:\n",
    "        torch.cuda.set_device(args.current_gpu)\n",
    "        if not args.apex:\n",
    "            model = model.cuda(args.current_gpu)\n",
    "    else:\n",
    "        if not args.apex:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    return model, args\n",
    "\n",
    "\n",
    "def apex_init(model, optimizer, args):\n",
    "    model = model.cuda().to(memory_format=args.memory_format)\n",
    "    model, optimizer = amp.initialize(model, optimizer,\n",
    "                                      opt_level=args.opt_level,\n",
    "                                      keep_batchnorm_fp32=args.keep_batchnorm_fp32,\n",
    "                                      loss_scale=args.loss_scale\n",
    "                                      )\n",
    "    if args.multigpus_distributed:\n",
    "        model = DDP(model, delay_allreduce=True)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, args):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= args.world_size\n",
    "    return rt\n",
    "\n",
    "\n",
    "def fast_collate(batch, memory_format=torch.channels_last):\n",
    "    imgs = [img[0] for img in batch]\n",
    "    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n",
    "    w = imgs[0].size[0]\n",
    "    h = imgs[0].size[1]\n",
    "    tensor = torch.zeros((len(imgs), 3, h, w), dtype=torch.uint8).contiguous(\n",
    "        memory_format=memory_format)\n",
    "    for i, img in enumerate(imgs):\n",
    "        nump_array = np.array(img, dtype=np.uint8)\n",
    "        if(nump_array.ndim < 3):\n",
    "            nump_array = np.expand_dims(nump_array, axis=-1)\n",
    "        nump_array = np.rollaxis(nump_array, 2)\n",
    "        tensor[i] += torch.from_numpy(nump_array)\n",
    "    return tensor, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/util.py\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import models\n",
    "\n",
    "import sagemaker_containers\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def torch_model(model_name, pretrained=True):\n",
    "    model_names = sorted(name for name in models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(models.__dict__[name]))\n",
    "\n",
    "    if(model_name == \"inception_v3\"):\n",
    "        raise RuntimeError(\n",
    "            \"Currently, inception_v3 is not supported by this example.\")\n",
    "\n",
    "    # create model\n",
    "    if pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(model_name))\n",
    "        model = models.__dict__[model_name](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(model_name))\n",
    "        model = models.__dict__[model_name]()\n",
    "    return model\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def save_model(state, is_best, model_dir):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    filename = os.path.join(model_dir, 'checkpoint.pth')\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(model_dir, 'model_best.pth'))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "        \n",
    "def adjust_learning_rate(optimizer, epoch, step, len_epoch, args):\n",
    "    \"\"\"LR schedule that should yield 76% converged accuracy with batch size 256\"\"\"\n",
    "    factor = epoch // 30\n",
    "\n",
    "    if epoch >= 80:\n",
    "        factor = factor + 1\n",
    "\n",
    "    lr = args.lr*(0.1**factor)\n",
    "\n",
    "    \"\"\"Warmup\"\"\"\n",
    "    if epoch < 5:\n",
    "        lr = lr*float(1 + step + epoch*len_epoch)/(5.*len_epoch)\n",
    "\n",
    "    if(args.current_gpu == 0):\n",
    "        print(\"epoch = {}, step = {}, lr = {}\".format(epoch, step, lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "        \n",
    "class data_prefetcher():\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.mean = torch.tensor([0.5 * 255, 0.5 * 255, 0.5 * 255]).cuda().view(1,3,1,1)\n",
    "        self.std = torch.tensor([0.5 * 255, 0.5 * 255, 0.5 * 255]).cuda().view(1,3,1,1)\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(non_blocking=True)\n",
    "            self.next_target = self.next_target.cuda(non_blocking=True)\n",
    "            self.next_input = self.next_input.float()\n",
    "            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        target = self.next_target\n",
    "        if input is not None:\n",
    "            input.record_stream(torch.cuda.current_stream())\n",
    "        if target is not None:\n",
    "            target.record_stream(torch.cuda.current_stream())\n",
    "        self.preload()\n",
    "        return input, target\n",
    "    \n",
    "\n",
    "\n",
    "def save_history(path, history):\n",
    "\n",
    "    history_for_json = {}\n",
    "    # transform float values that aren't json-serializable\n",
    "    for key in history.keys():\n",
    "        history_for_json[key] = list(map(float, history[key]))\n",
    "\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(history_for_json, f, separators=(\n",
    "            ',', ':'), sort_keys=True, indent=4)\n",
    "        \n",
    "        \n",
    "\n",
    "def init_modelhistory(model_history):\n",
    "    model_history['epoch'] = []\n",
    "    model_history['batch_idx'] = []\n",
    "    model_history['batch_time'] = []\n",
    "    model_history['losses'] = []\n",
    "    model_history['top1'] = []\n",
    "    model_history['top5'] = []\n",
    "    model_history['val_epoch'] = []\n",
    "    model_history['val_batch_idx'] = []\n",
    "    model_history['val_batch_time'] = []\n",
    "    model_history['val_losses'] = []\n",
    "    model_history['val_top1'] = []\n",
    "    model_history['val_top5'] = []\n",
    "    model_history['val_avg_epoch'] = []\n",
    "    model_history['val_avg_batch_time'] = []\n",
    "    model_history['val_avg_losses'] = []\n",
    "    model_history['val_avg_top1'] = []\n",
    "    model_history['val_avg_top5'] = []\n",
    "    return model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'height' : 137,\n",
    "        'width' : 236,\n",
    "        'num_epochs': 1,\n",
    "        'batch-size' : 128,\n",
    "        'backend': 'nccl',\n",
    "        'lr': 0.001,\n",
    "        'apex' : True,\n",
    "        'opt-level' : 'O1'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.1 ms, sys: 4.14 ms, total: 38.3 ms\n",
      "Wall time: 39.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# all input configurations, parameters, and metrics specified in estimator \n",
    "# definition are automatically tracked\n",
    "estimator = PyTorch(\n",
    "    entry_point='./main_trainer.py',\n",
    "    source_dir='./src',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n",
    "    framework_version='1.5.0',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.8xlarge',\n",
    "    train_volume_size=400,\n",
    "    hyperparameters=hyperparameters,\n",
    "#     train_use_spot_instances=True,  # spot instance 활용\n",
    "#     train_max_run=12*60*60,\n",
    "#     train_max_wait=12*60*60,\n",
    "#     checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "#     tensorboard_output_config=TensorBoardOutputConfig(tensorboard_output),\n",
    "    metric_definitions=[\n",
    "        {'Name':'train:loss', 'Regex':'Train Loss: (.*?);'},\n",
    "        {'Name':'test:loss', 'Regex':'Test Average loss: (.*?),'},\n",
    "        {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}\n",
    "    ],\n",
    "    enable_sagemaker_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "training_job_name = \"training-job-{}\".format(int(time.time()))\n",
    "\n",
    "# Now associate the estimator with the Experiment and Trial\n",
    "estimator.fit(\n",
    "    inputs={'training': s3_data_path}, \n",
    "    job_name=training_job_name,\n",
    "    logs='All',\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-04 12:52:55 Starting - Starting the training job...\n",
      "2020-08-04 12:52:57 Starting - Launching requested ML instances......\n",
      "2020-08-04 12:54:00 Starting - Preparing the instances for training......\n",
      "2020-08-04 12:55:12 Downloading - Downloading input data...\n",
      "2020-08-04 12:55:44 Training - Downloading the training image......\n",
      "2020-08-04 12:56:50 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-08-04 12:56:51,259 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-08-04 12:56:51,302 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-08-04 12:56:54,328 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-04 12:56:54,622 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-04 12:56:54,622 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-04 12:56:54,622 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-04 12:56:54,622 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpo936cw5s/module_dir\u001b[0m\n",
      "\u001b[34mCollecting albumentations\n",
      "  Downloading albumentations-0.4.6.tar.gz (117 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow\n",
      "  Downloading pyarrow-1.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.2.2)\u001b[0m\n",
      "\u001b[34mCollecting imgaug>=0.4.0\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (5.3.1)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.3.0.36-cp36-cp36m-manylinux2014_x86_64.whl (36.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-image>=0.14.2\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting imageio\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting Shapely\n",
      "  Downloading Shapely-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (7.1.0)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python\n",
      "  Downloading opencv_python-4.3.0.36-cp36-cp36m-manylinux2014_x86_64.whl (43.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.14.0)\u001b[0m\n",
      "\u001b[34mCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.7.24-py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (4.4.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: albumentations, default-user-module-name\n",
      "  Building wheel for albumentations (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for albumentations (setup.py): finished with status 'done'\n",
      "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65167 sha256=fa0538aabcac18947321caabc19abe96eed4eda25fcb30548a17936e79eab3c4\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/db/df/d6cb0be184075a7799c1fd79240c389c16f51dfe18dc3332fa\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=34986 sha256=9ecbf55543f1b1e32617f84bbc2606a1834a8b9ac8cd7fb597266ffec268f920\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1dc598ib/wheels/3f/49/8f/b08b11a09f1375beeda79e949e14024016f517678b15385ad9\u001b[0m\n",
      "\u001b[34mSuccessfully built albumentations default-user-module-name\u001b[0m\n",
      "\u001b[34mERROR: scikit-image 0.17.2 has requirement pillow!=7.1.0,!=7.1.1,>=4.3.0, but you'll have pillow 7.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: imageio, PyWavelets, tifffile, scikit-image, Shapely, opencv-python, imgaug, opencv-python-headless, albumentations, pyarrow, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed PyWavelets-1.1.1 Shapely-1.7.0 albumentations-0.4.6 default-user-module-name-1.0.0 imageio-2.9.0 imgaug-0.4.0 opencv-python-4.3.0.36 opencv-python-headless-4.3.0.36 pyarrow-1.0.0 scikit-image-0.17.2 tifffile-2020.7.24\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1; however, version 20.2.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-08-04 12:57:09,607 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"opt-level\": \"O1\",\n",
      "        \"lr\": 0.001,\n",
      "        \"apex\": true,\n",
      "        \"batch-size\": 128,\n",
      "        \"width\": 236,\n",
      "        \"backend\": \"nccl\",\n",
      "        \"num_epochs\": 1,\n",
      "        \"height\": 137\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"training-job-1596545574\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-322537213286/training-job-1596545574/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"./main_trainer\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"./main_trainer.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"apex\":true,\"backend\":\"nccl\",\"batch-size\":128,\"height\":137,\"lr\":0.001,\"num_epochs\":1,\"opt-level\":\"O1\",\"width\":236}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=./main_trainer.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=./main_trainer\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-322537213286/training-job-1596545574/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"apex\":true,\"backend\":\"nccl\",\"batch-size\":128,\"height\":137,\"lr\":0.001,\"num_epochs\":1,\"opt-level\":\"O1\",\"width\":236},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"training-job-1596545574\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-322537213286/training-job-1596545574/source/sourcedir.tar.gz\",\"module_name\":\"./main_trainer\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"./main_trainer.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--apex\",\"True\",\"--backend\",\"nccl\",\"--batch-size\",\"128\",\"--height\",\"137\",\"--lr\",\"0.001\",\"--num_epochs\",\"1\",\"--opt-level\",\"O1\",\"--width\",\"236\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_OPT-LEVEL=O1\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_APEX=true\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_WIDTH=236\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_HEIGHT=137\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python ./main_trainer.py --apex True --backend nccl --batch-size 128 --height 137 --lr 0.001 --num_epochs 1 --opt-level O1 --width 236\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34margs.use_cuda : True , args.num_gpus : 4\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 1 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 0 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 2 for training\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 4 nodes. Current host rank is 2. Number of gpus: 4\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 3 for training\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 4 nodes. Current host rank is 3. Number of gpus: 4\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 4 nodes. Current host rank is 1. Number of gpus: 4\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 4 nodes. Current host rank is 0. Number of gpus: 4\u001b[0m\n",
      "\u001b[34mSelected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\u001b[0m\n",
      "\u001b[34mDefaults for this optimization level are:\u001b[0m\n",
      "\u001b[34menabled                : True\u001b[0m\n",
      "\u001b[34mopt_level              : O1\u001b[0m\n",
      "\u001b[34mcast_model_type        : None\u001b[0m\n",
      "\u001b[34mpatch_torch_functions  : True\u001b[0m\n",
      "\u001b[34mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[34mmaster_weights         : None\u001b[0m\n",
      "\u001b[34mloss_scale             : dynamic\u001b[0m\n",
      "\u001b[34mProcessing user overrides (additional kwargs that are not None)...\u001b[0m\n",
      "\u001b[34mAfter processing overrides, optimization options are:\u001b[0m\n",
      "\u001b[34menabled                : True\u001b[0m\n",
      "\u001b[34mopt_level              : O1\u001b[0m\n",
      "\u001b[34mcast_model_type        : None\u001b[0m\n",
      "\u001b[34mpatch_torch_functions  : True\u001b[0m\n",
      "\u001b[34mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[34mmaster_weights         : None\u001b[0m\n",
      "\u001b[34mloss_scale             : dynamic\u001b[0m\n",
      "\u001b[34mNCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 40168/160672 (25%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10042/40168 (25%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 40168/160672 (25%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10042/40168 (25%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 40168/160672 (25%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10042/40168 (25%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 40168/160672 (25%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10042/40168 (25%) of test data\u001b[0m\n",
      "\u001b[34m2020-08-04 13:05:13,063 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python ./main_trainer.py --apex True --backend nccl --batch-size 128 --height 137 --lr 0.001 --num_epochs 1 --opt-level O1 --width 236\"\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0.00/97.8M [00:00<?, ?B/s]#015  0%|          | 0.00/97.8M [00:00<?, ?B/s]#015  0%|          | 0.00/97.8M [00:00<?, ?B/s]#015  0%|          | 0.00/97.8M [00:00<?, ?B/s]#015  1%|          | 952k/97.8M [00:00<00:10, 9.52MB/s]#015  1%|          | 600k/97.8M [00:00<00:17, 5.99MB/s]#015  1%|          | 952k/97.8M [00:00<00:10, 9.47MB/s]#015  1%|          | 584k/97.8M [00:00<00:17, 5.84MB/s]#015  3%|â         | 2.50M/97.8M [00:00<00:09, 10.9MB/s]#015  3%|â         | 2.51M/97.8M [00:00<00:09, 10.9MB/s]#015  1%|â         | 1.45M/97.8M [00:00<00:15, 6.65MB/s]#015  1%|â         | 1.40M/97.8M [00:00<00:15, 6.46MB/s]#015  5%|â         | 4.55M/97.8M [00:00<00:07, 12.8MB/s]#015  3%|â         | 2.53M/97.8M [00:00<00:13, 7.57MB/s]#015  5%|â         | 4.56M/97.8M [00:00<00:07, 12.7MB/s]#015  3%|â         | 2.46M/97.8M [00:00<00:13, 7.38MB/s]#015  7%|â         | 7.14M/97.8M [00:00<00:06, 15.2MB/s]#015  7%|â         | 7.20M/97.8M [00:00<00:06, 15.1MB/s]#015  4%|â         | 3.98M/97.8M [00:00<00:11, 8.89MB/s]#015  4%|â         | 3.80M/97.8M [00:00<00:11, 8.61MB/s]#015 11%|â         | 10.6M/97.8M [00:00<00:04, 18.3MB/s]#015 11%|â         | 10.6M/97.8M [00:00<00:04, 18.3MB/s]#015  6%|â         | 5.80M/97.8M [00:00<00:09, 10.6MB/s]#015  6%|â         | 5.54M/97.8M [00:00<00:09, 10.2MB/s]#015 15%|ââ        | 14.9M/97.8M [00:00<00:03, 22.3MB/s]#015 15%|ââ        | 15.0M/97.8M [00:00<00:03, 22.3MB/s]#015  8%|â         | 8.21M/97.8M [00:00<00:07, 12.8MB/s]#015  8%|â         | 7.74M/97.8M [00:00<00:07, 12.3MB/s]#015 21%|ââ        | 20.4M/97.8M [00:00<00:02, 27.3MB/s]#015 21%|ââ        | 20.7M/97.8M [00:00<00:02, 27.5MB/s]#015 12%|ââ        | 11.3M/97.8M [00:00<00:05, 15.6MB/s]#015 11%|â         | 10.5M/97.8M [00:00<00:06, 14.8MB/s]#015 28%|âââ       | 26.9M/97.8M [00:00<00:02, 33.3MB/s]#015 29%|âââ       | 28.0M/97.8M [00:00<00:02, 34.0MB/s]#015 16%|ââ        | 15.3M/97.8M [00:00<00:04, 19.2MB/s]#015 14%|ââ        | 14.0M/97.8M [00:00<00:04, 18.0MB/s]#015 36%|ââââ      | 34.9M/97.8M [00:00<00:01, 40.6MB/s]#015 37%|ââââ      | 36.0M/97.8M [00:00<00:01, 41.4MB/s]#015 21%|ââ        | 20.3M/97.8M [00:00<00:03, 23.7MB/s]#015 19%|ââ        | 18.6M/97.8M [00:00<00:03, 22.1MB/s]#015 44%|âââââ     | 43.0M/97.8M [00:01<00:01, 48.2MB/s]#015 45%|âââââ     | 44.1M/97.8M [00:01<00:01, 49.0MB/s]#015 27%|âââ       | 26.6M/97.8M [00:01<00:02, 29.4MB/s]#015 25%|âââ       | 24.3M/97.8M [00:01<00:02, 27.3MB/s]#015 52%|ââââââ    | 51.0M/97.8M [00:01<00:00, 55.3MB/s]#015 53%|ââââââ    | 52.2M/97.8M [00:01<00:00, 56.0MB/s]#015 35%|ââââ      | 34.5M/97.8M [00:01<00:01, 36.5MB/s]#015 32%|ââââ      | 31.4M/97.8M [00:01<00:02, 33.7MB/s]#015 60%|ââââââ    | 59.1M/97.8M [00:01<00:00, 61.8MB/s]#015 62%|âââââââ   | 60.2M/97.8M [00:01<00:00, 62.4MB/s]#015 44%|âââââ     | 42.7M/97.8M [00:01<00:01, 44.0MB/s]#015 40%|ââââ      | 39.2M/97.8M [00:01<00:01, 40.9MB/s]#015 69%|âââââââ   | 67.2M/97.8M [00:01<00:00, 67.2MB/s]#015 70%|âââââââ   | 68.4M/97.8M [00:01<00:00, 67.7MB/s]#015 52%|ââââââ    | 50.6M/97.8M [00:01<00:00, 51.2MB/s]#015 48%|âââââ     | 46.9M/97.8M [00:01<00:01, 47.9MB/s]#015 77%|ââââââââ  | 75.2M/97.8M [00:01<00:00, 71.6MB/s]#015 78%|ââââââââ  | 76.5M/97.8M [00:01<00:00, 72.0MB/s]#015 59%|ââââââ    | 58.0M/97.8M [00:01<00:00, 57.1MB/s]#015 56%|ââââââ    | 54.4M/97.8M [00:01<00:00, 54.4MB/s]#015 85%|âââââââââ | 83.2M/97.8M [00:01<00:00, 74.8MB/s]#015 87%|âââââââââ | 84.6M/97.8M [00:01<00:00, 75.4MB/s]#015 68%|âââââââ   | 66.2M/97.8M [00:01<00:00, 63.4MB/s]#015 64%|âââââââ   | 62.1M/97.8M [00:01<00:00, 60.3MB/s]#015 93%|ââââââââââ| 91.2M/97.8M [00:01<00:00, 77.2MB/s]#015 95%|ââââââââââ| 92.6M/97.8M [00:01<00:00, 77.8MB/s]#015 76%|ââââââââ  | 74.1M/97.8M [00:01<00:00, 68.3MB/s]#015 71%|ââââââââ  | 69.8M/97.8M [00:01<00:00, 65.3MB/s]#015100%|ââââââââââ| 97.8M/97.8M [00:01<00:00, 61.2MB/s]\u001b[0m\n",
      "\u001b[34m#015100%|ââââââââââ| 97.8M/97.8M [00:01<00:00, 60.7MB/s]\u001b[0m\n",
      "\u001b[34m#015 84%|âââââââââ | 82.2M/97.8M [00:01<00:00, 72.4MB/s]#015 79%|ââââââââ  | 77.6M/97.8M [00:01<00:00, 69.2MB/s]#015 92%|ââââââââââ| 90.3M/97.8M [00:01<00:00, 75.6MB/s]#015 87%|âââââââââ | 85.3M/97.8M [00:01<00:00, 72.4MB/s]#015100%|ââââââââââ| 97.8M/97.8M [00:01<00:00, 53.8MB/s]\u001b[0m\n",
      "\u001b[34m#015 95%|ââââââââââ| 92.9M/97.8M [00:01<00:00, 74.4MB/s]#015100%|ââââââââââ| 97.8M/97.8M [00:01<00:00, 51.9MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 12 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 12 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 12 leaked semaphores to clean up at shutdown\n",
      "  len(cache))\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"./main_trainer.py\", line 657, in <module>\n",
      "    main()\n",
      "  File \"./main_trainer.py\", line 653, in main\n",
      "    dis_util.dist_init(train, args)\n",
      "  File \"/opt/ml/code/dis_util.py\", line 62, in dist_init\n",
      "    mp.spawn(fn, nprocs=args.num_gpus, args=(args,))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\n",
      "    raise Exception(msg)\u001b[0m\n",
      "\u001b[34mException: \n",
      "\u001b[0m\n",
      "\u001b[34m-- Process 3 terminated with the following error:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/opt/ml/code/main_trainer.py\", line 334, in train\n",
      "    prefetcher = util.data_prefetcher(train_loader)\n",
      "  File \"/opt/ml/code/util.py\", line 138, in __init__\n",
      "    self.loader = iter(loader)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 279, in __iter__\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 719, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\n",
      "    reduction.dump(process_obj, fp)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\u001b[0m\n",
      "\u001b[34mOverflowError: cannot serialize a bytes object larger than 4 GiB\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-08-04 13:05:25 Uploading - Uploading generated training model\n",
      "2020-08-04 13:05:25 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job training-job-1596545574: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python ./main_trainer.py --apex True --backend nccl --batch-size 128 --height 137 --lr 0.001 --num_epochs 1 --opt-level O1 --width 236\"\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r  1%|          | 952k/97.8M [00:00<00:10, 9.52MB/s]\r  1%|          | 600k/97.8M [00:00<00:17, 5.99MB/s]\r  1%|          | 952k/97.8M",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-0cc621c6fbe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3070\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3071\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3072\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2662\u001b[0m                 ),\n\u001b[1;32m   2663\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2664\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2665\u001b[0m             )\n\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job training-job-1596545574: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python ./main_trainer.py --apex True --backend nccl --batch-size 128 --height 137 --lr 0.001 --num_epochs 1 --opt-level O1 --width 236\"\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r  1%|          | 952k/97.8M [00:00<00:10, 9.52MB/s]\r  1%|          | 600k/97.8M [00:00<00:17, 5.99MB/s]\r  1%|          | 952k/97.8M"
     ]
    }
   ],
   "source": [
    "sagemaker_session.logs_for_job(estimator.latest_training_job.name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "display(df_stats)\n",
    "\n",
    "#model.load_state_dict(torch.load('./model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
