{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modlue 3. Distributed-Mutigpu Training with CutMix-ScriptMode\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amzaon SageMaker API을 효과적으로 이용하기 위해 distributed-multigpu 학습을 위한 PyTorch 프레임워크 자체 구현만으로 모델 훈련을 수행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time, datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 버전을 최신 버전으로 업데이트합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.4.6)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (1.18.1)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (4.2.0.32)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (1.14.0)\n",
      "Requirement already satisfied: Shapely in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (1.7.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (7.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (3.1.3)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (0.16.2)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (2.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.1.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (2.4)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations) (46.1.3.post20200330)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (4.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U torch \n",
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import boto3\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch, torchvision\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "job_bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "data_bucket = 'sagemaker-{}-{}'.format(sess.region_name, account_id)\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "local_data_paths = glob.glob('./input/train_image_data_*.parquet')\n",
    "# s3_data_parquet_path = 's3://{}/{}'.format(data_bucket, 'BangaliDataset/list/parquet/')\n",
    "s3_data_path = sagemaker.s3_input(s3_data='s3://{}/{}'.format(data_bucket, 'bangali/train'), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = 'bangali/train'\n",
    "# s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}'.format(data_bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local_data_path in local_data_paths:\n",
    "#     !aws s3 cp $local_data_path $s3_data_parquet_path\n",
    "    \n",
    "# !aws s3 cp ./input/train_folds.csv $s3_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_data_path = './input/train_images/'\n",
    "# s3_data_path = 's3://{}/{}'.format(data_bucket, 'BangaliDataset/bantrain_images/')\n",
    "\n",
    "# !aws s3 cp --recursive $local_data_path $s3_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write main_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "albumentations\n",
    "pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Rotate,HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    ")\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data_set’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data = 's3://{}/{}'.format(data_bucket, 'bangali/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-322537213286/bangali/train'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-2-322537213286/bangali/train/train_folds.csv to data_set/train_folds.csv\n",
      "download: s3://sagemaker-us-east-2-322537213286/bangali/train/train_image_data_2.parquet to data_set/train_image_data_2.parquet\n",
      "download: s3://sagemaker-us-east-2-322537213286/bangali/train/train_image_data_1.parquet to data_set/train_image_data_1.parquet\n",
      "download: s3://sagemaker-us-east-2-322537213286/bangali/train/train_image_data_3.parquet to data_set/train_image_data_3.parquet\n",
      "download: s3://sagemaker-us-east-2-322537213286/bangali/train/train_image_data_0.parquet to data_set/train_image_data_0.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp $s3_data/ ./data_set --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data_set'\n",
    "vld_fold_idx = 4\n",
    "num_folds=5\n",
    "height=137\n",
    "width=236\n",
    "data_type='train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(os.path.join(data_dir, 'train_folds.csv'))\n",
    "#label_df = pd.read_csv(f'{train_dir}/train_folds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_fold = [i for i in range(num_folds) if i not in [vld_fold_idx]]\n",
    "vld_fold = [vld_fold_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idx = label_df.loc[label_df['fold'].isin(trn_fold)].index\n",
    "vld_idx = label_df.loc[label_df['fold'].isin(vld_fold)].index\n",
    "\n",
    "#files = sorted(glob2.glob(f'{train_dir}/{data_type}_*.parquet'))\n",
    "files = [f'{data_dir}/{data_type}_image_data_{i}.parquet' for i in range(4)]\n",
    "\n",
    "\n",
    "image_df_list = [pd.read_parquet(f) for f in files]\n",
    "imgs = [df.iloc[:, 1:].values.reshape(-1, height, width) for df in image_df_list]\n",
    "del image_df_list\n",
    "gc.collect()\n",
    "imgs = np.concatenate(imgs, axis=0)\n",
    "\n",
    "trn_df = label_df.loc[trn_idx]\n",
    "vld_df = label_df.loc[vld_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200840, 137, 236)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BangaliDataset(Dataset):\n",
    "    def __init__(self, imgs, label_df=None, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.label_df = label_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_idx = self.label_df.iloc[idx].id\n",
    "        img = (self.imgs[img_idx]).astype(np.uint8)\n",
    "        img = 255 - img\n",
    "    \n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']        \n",
    "        \n",
    "        if self.label_df is not None:\n",
    "            label_1 = self.label_df.iloc[idx].grapheme_root\n",
    "            label_2 = self.label_df.iloc[idx].vowel_diacritic\n",
    "            label_3 = self.label_df.iloc[idx].consonant_diacritic           \n",
    "            return img, np.array([label_1, label_2, label_3])        \n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_collate(batch, memory_format=torch.channels_last):\n",
    "    imgs = [img[0] for img in batch]\n",
    "    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n",
    "    print(\"imgs[0] : {}\".format(imgs[0].shape))\n",
    "    print(\"imgs[0] : {}\".format(imgs[0].shape[0]))\n",
    "    w = imgs[0].size[0]\n",
    "    h = imgs[0].size[1]\n",
    "    tensor = torch.zeros((len(imgs), 3, w, h), dtype=torch.uint8).contiguous(\n",
    "        memory_format=memory_format)\n",
    "    for i, img in enumerate(imgs):\n",
    "        nump_array = np.array(img, dtype=np.uint8)\n",
    "        if(nump_array.ndim < 3):\n",
    "            nump_array = np.expand_dims(nump_array, axis=-1)\n",
    "        nump_array = np.rollaxis(nump_array, 2)\n",
    "        tensor[i] += torch.from_numpy(nump_array)\n",
    "    return tensor, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_data_loader(imgs, trn_df, kwargs):\n",
    "    train_transforms = Compose([\n",
    "        Rotate(20),\n",
    "            OneOf([\n",
    "                IAAAdditiveGaussianNoise(),\n",
    "                GaussNoise(),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                MotionBlur(p=.2),\n",
    "                MedianBlur(blur_limit=3, p=0.1),\n",
    "                Blur(blur_limit=3, p=0.1),\n",
    "            ], p=0.2),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "            OneOf([\n",
    "                OpticalDistortion(p=0.3),\n",
    "                GridDistortion(p=.1),\n",
    "                IAAPiecewiseAffine(p=0.3),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                CLAHE(clip_limit=2),\n",
    "                IAASharpen(),\n",
    "                IAAEmboss(),\n",
    "                RandomBrightnessContrast(),            \n",
    "            ], p=0.3),\n",
    "            HueSaturationValue(p=0.3)\n",
    "        ], p=1.0)\n",
    "    \n",
    "    dataset = BangaliDataset(imgs=imgs, label_df=trn_df, transform=train_transforms)\n",
    "    train_sampler = None\n",
    "    return data.DataLoader(dataset, batch_size=128, shuffle=train_sampler is None,\n",
    "                                       sampler=train_sampler, collate_fn=fast_collate,num_workers=0,pin_memory=True), train_sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, train_sampler = _get_train_data_loader(imgs, trn_df, kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_prefetcher():\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.mean = torch.tensor([0.5 * 255, 0.5 * 255, 0.5 * 255]).cuda().view(1,3,1,1)\n",
    "        self.std = torch.tensor([0.5 * 255, 0.5 * 255, 0.5 * 255]).cuda().view(1,3,1,1)\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(non_blocking=True)\n",
    "            self.next_target = self.next_target.cuda(non_blocking=True)\n",
    "            self.next_input = self.next_input.float()\n",
    "            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        target = self.next_target\n",
    "        if input is not None:\n",
    "            input.record_stream(torch.cuda.current_stream())\n",
    "        if target is not None:\n",
    "            target.record_stream(torch.cuda.current_stream())\n",
    "        self.preload()\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs[0] : (137, 236, 3)\n",
      "imgs[0] : 137\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-a0f522dceac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-243-096af33368f1>\u001b[0m in \u001b[0;36mfast_collate\u001b[0;34m(batch, memory_format)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imgs[0] : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imgs[0] : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     tensor = torch.zeros((len(imgs), 3, w, h), dtype=torch.uint8).contiguous(\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for test in train_loader:\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/main_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/main_trainer.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "import logging.handlers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import dis_util\n",
    "import sagemaker_containers\n",
    "import util\n",
    "\n",
    "## Apex import package\n",
    "try:\n",
    "    from apex.parallel import DistributedDataParallel as DDP\n",
    "    from apex.fp16_utils import *\n",
    "    from apex import amp, optimizers\n",
    "    from apex.multi_tensor_apply import multi_tensor_applier\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
    "\n",
    "\n",
    "## augmentation for setting\n",
    "from albumentations import (\n",
    "    Rotate,HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    ")\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "\n",
    "import dis_util\n",
    "import sagemaker_containers\n",
    "import util\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def parser_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Default Setting\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default='nccl',\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "    parser.add_argument('--channels-last', type=bool, default=True)\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('-p', '--print-freq', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "\n",
    "    # Hyperparameter Setting\n",
    "    parser.add_argument('--model_name', type=str, default='resnet50')\n",
    "    parser.add_argument('--height', type=int, default=128)\n",
    "    parser.add_argument('--width', type=int, default=128)\n",
    "    parser.add_argument('--num_folds', type=int, default=5)\n",
    "    parser.add_argument('--vld_fold_idx', type=int, default=4)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--num_epochs', type=int, default=3)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument('--test-batch-size', type=int, default=200, metavar='N',\n",
    "                        help='input batch size for testing (default: 200)')\n",
    "\n",
    "    # APEX Setting for Distributed Training\n",
    "    parser.add_argument('--apex', type=bool, default=False)\n",
    "    parser.add_argument('--opt-level', type=str, default='O0')\n",
    "    parser.add_argument('--keep-batchnorm-fp32', type=str, default=None)\n",
    "    parser.add_argument('--loss-scale', type=str, default=None)\n",
    "    parser.add_argument('--sync_bn', action='store_true',\n",
    "                        help='enabling apex sync BN.')\n",
    "    parser.add_argument('--prof', default=-1, type=int,\n",
    "                        help='Only run 10 iterations for profiling.')\n",
    "\n",
    "    # SageMaker Container environment\n",
    "    parser.add_argument('--hosts', type=list,\n",
    "                        default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current-host', type=str,\n",
    "                        default=os.environ['SM_CURRENT_HOST'])\n",
    "    parser.add_argument('--model-dir', type=str,\n",
    "                        default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--data-dir', type=str,\n",
    "                        default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--num-gpus', type=int,\n",
    "                        default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--output_data_dir', type=str,\n",
    "                        default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        last_hidden_units = self.model.fc.out_features\n",
    "        self.classifer_model = nn.Linear(last_hidden_units, 186)\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.model(x)\n",
    "        x = self.classifer_model(features)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class BangaliDataset(Dataset):\n",
    "    def __init__(self, imgs, label_df=None, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.label_df = label_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_idx = self.label_df.iloc[idx].id\n",
    "        img = (self.imgs[img_idx]).astype(np.uint8)\n",
    "        img = 255 - img\n",
    "    \n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']        \n",
    "        \n",
    "        if self.label_df is not None:\n",
    "            label_1 = self.label_df.iloc[idx].grapheme_root\n",
    "            label_2 = self.label_df.iloc[idx].vowel_diacritic\n",
    "            label_3 = self.label_df.iloc[idx].consonant_diacritic           \n",
    "            return img, np.array([label_1, label_2, label_3])        \n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "        \n",
    "\n",
    "def _rand_bbox(size, lam):\n",
    "    '''\n",
    "    CutMix Helper function.\n",
    "    Retrieved from https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "    '''\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    # 폭과 높이는 주어진 이미지의 폭과 높이의 beta distribution에서 뽑은 lambda로 얻는다\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    \n",
    "    # patch size 의 w, h 는 original image 의 w,h 에 np.sqrt(1-lambda) 를 곱해준 값입니다.\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # patch의 중심점은 uniform하게 뽑힘\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def _set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    mx.random.seed(seed)\n",
    "\n",
    "def _get_images(args, data_type='train'):\n",
    "\n",
    "    logger.info(\"=== Getting Labels ===\")\n",
    "    logger.info(args.data_dir)\n",
    "    \n",
    "    label_df = pd.read_csv(os.path.join(args.data_dir, 'train_folds.csv'))\n",
    "    #label_df = pd.read_csv(f'{train_dir}/train_folds.csv')\n",
    "     \n",
    "    trn_fold = [i for i in range(args.num_folds) if i not in [args.vld_fold_idx]]\n",
    "    vld_fold = [args.vld_fold_idx]\n",
    "\n",
    "    trn_idx = label_df.loc[label_df['fold'].isin(trn_fold)].index\n",
    "    vld_idx = label_df.loc[label_df['fold'].isin(vld_fold)].index\n",
    "\n",
    "    logger.info(\"=== Getting Images ===\")    \n",
    "    #files = sorted(glob2.glob(f'{train_dir}/{data_type}_*.parquet'))\n",
    "    files = [f'{args.data_dir}/{data_type}_image_data_{i}.parquet' for i in range(4)]\n",
    "    logger.info(files)\n",
    "    \n",
    "    image_df_list = [pd.read_parquet(f) for f in files]\n",
    "    imgs = [df.iloc[:, 1:].values.reshape(-1, args.height, args.width) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    args.imgs = np.concatenate(imgs, axis=0)\n",
    "    \n",
    "    args.trn_df = label_df.loc[trn_idx]\n",
    "    args.vld_df = label_df.loc[vld_idx]\n",
    "    \n",
    "    return args \n",
    "\n",
    "\n",
    "def _get_train_data_loader(args, **kwargs):\n",
    "    logger.info(\"Get train data loader\")\n",
    "    train_transforms = Compose([\n",
    "        Rotate(20),\n",
    "            OneOf([\n",
    "                IAAAdditiveGaussianNoise(),\n",
    "                GaussNoise(),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                MotionBlur(p=.2),\n",
    "                MedianBlur(blur_limit=3, p=0.1),\n",
    "                Blur(blur_limit=3, p=0.1),\n",
    "            ], p=0.2),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "            OneOf([\n",
    "                OpticalDistortion(p=0.3),\n",
    "                GridDistortion(p=.1),\n",
    "                IAAPiecewiseAffine(p=0.3),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                CLAHE(clip_limit=2),\n",
    "                IAASharpen(),\n",
    "                IAAEmboss(),\n",
    "                RandomBrightnessContrast(),            \n",
    "            ], p=0.3),\n",
    "            HueSaturationValue(p=0.3),\n",
    "#         ToTensor()\n",
    "        ], p=1.0)\n",
    "    \n",
    "    dataset = BangaliDataset(imgs=args.imgs, label_df=args.trn_df, transform=train_transforms)\n",
    "    train_sampler = data.distributed.DistributedSampler(\n",
    "        dataset, num_replicas=int(args.world_size), rank=int(args.rank)) if args.multigpus_distributed else None\n",
    "    return data.DataLoader(dataset, batch_size=args.batch_size, shuffle=train_sampler is None,\n",
    "                                       sampler=train_sampler, collate_fn=dis_util.fast_collate, **kwargs), train_sampler\n",
    "\n",
    "\n",
    "def _get_test_data_loader(args, **kwargs):\n",
    "    logger.info(\"Get test data loader\")   \n",
    "\n",
    "    dataset = BangaliDataset(imgs=args.imgs, label_df=args.vld_df)\n",
    "    val_sampler = data.distributed.DistributedSampler(dataset) if args.multigpus_distributed else None\n",
    "    return data.DataLoader(dataset, batch_size=args.test_batch_size, shuffle=False, \n",
    "                           sampler=val_sampler, collate_fn=dis_util.fast_collate, **kwargs)\n",
    "\n",
    "\n",
    "def train(current_gpu, args):\n",
    "    best_acc1 = -1\n",
    "    model_history = {}\n",
    "    model_history = util.init_modelhistory(model_history)\n",
    "    train_start = time.time()\n",
    "\n",
    "    ## choose model from pytorch model_zoo\n",
    "    model = util.torch_model(args.model_name, pretrained=True)\n",
    "    loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    ## distributed_setting \n",
    "    model, args = dis_util.dist_setting(current_gpu, model, loss_fn, args)\n",
    "\n",
    "    ## CuDNN library will benchmark several algorithms and pick that which it found to be fastest\n",
    "    cudnn.benchmark = False if args.seed else True\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    if args.apex:\n",
    "        model, optimizer = dis_util.apex_init(model, optimizer, args)\n",
    "    \n",
    "    \n",
    "#     args.collate_fn = partial(dis_util.fast_collate, memory_format=args.memory_format)\n",
    "   \n",
    "    args = _get_images(args, data_type='train')\n",
    "    train_loader, train_sampler = _get_train_data_loader(args, **args.kwargs)\n",
    "    test_loader = _get_test_data_loader(args, **args.kwargs)\n",
    "\n",
    "    logger.info(\"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "        len(train_loader.sampler), len(train_loader.dataset),\n",
    "        100. * len(train_loader.sampler) / len(train_loader.dataset)\n",
    "    ))\n",
    "\n",
    "    logger.info(\"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "        len(test_loader.sampler), len(test_loader.dataset),\n",
    "        100. * len(test_loader.sampler) / len(test_loader.dataset)\n",
    "    ))\n",
    "\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        ## \n",
    "        batch_time = util.AverageMeter('Time', ':6.3f')\n",
    "        data_time = util.AverageMeter('Data', ':6.3f')\n",
    "        losses = util.AverageMeter('Loss', ':.4e')\n",
    "        top1 = util.AverageMeter('Acc@1', ':6.2f')\n",
    "        top5 = util.AverageMeter('Acc@5', ':6.2f')\n",
    "        progress = util.ProgressMeter(\n",
    "            len(train_loader),\n",
    "            [batch_time, data_time, losses, top1, top5],\n",
    "            prefix=\"Epoch: [{}]\".format(epoch))\n",
    "        \n",
    "        trn_loss = []\n",
    "        model.train()\n",
    "        end = time.time()\n",
    "        running_loss = 0.0\n",
    "        ## Set epoch count for DistributedSampler\n",
    "        if args.multigpus_distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        \n",
    "        \n",
    "        prefetcher = util.data_prefetcher(train_loader)\n",
    "        input, target = prefetcher.next()\n",
    "        batch_idx = 0\n",
    "        while input is not None:\n",
    "\n",
    "            batch_idx += 1\n",
    "            \n",
    "            if args.prof >= 0 and batch_idx == args.prof:\n",
    "                print(\"Profiling begun at iteration {}\".format(batch_idx))\n",
    "                torch.cuda.cudart().cudaProfilerStart()\n",
    "                \n",
    "            if args.prof >= 0: torch.cuda.nvtx.range_push(\"Body of iteration {}\".format(batch_idx))\n",
    "\n",
    "            util.adjust_learning_rate(optimizer, epoch, batch_idx, len(train_loader), args)\n",
    "            \n",
    "            ##### DATA Processing #####\n",
    "            targets_gra = target[:, 0]\n",
    "            targets_vow = target[:, 1]\n",
    "            targets_con = target[:, 2]\n",
    "\n",
    "            # 50%의 확률로 원본 데이터 그대로 사용    \n",
    "            if np.random.rand() < 0.5:\n",
    "                logits = model(input)\n",
    "                grapheme = logits[:, :168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra)\n",
    "                loss2 = loss_fn(vowel, targets_vow)\n",
    "                loss3 = loss_fn(cons, targets_con) \n",
    "\n",
    "            else:\n",
    "\n",
    "                lam = np.random.beta(1.0, 1.0) \n",
    "                rand_index = torch.randperm(input.size()[0])\n",
    "                shuffled_targets_gra = targets_gra[rand_index]\n",
    "                shuffled_targets_vow = targets_vow[rand_index]\n",
    "                shuffled_targets_con = targets_con[rand_index]\n",
    "\n",
    "                bbx1, bby1, bbx2, bby2 = _rand_bbox(input.size(), lam)\n",
    "                input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # 픽셀 비율과 정확히 일치하도록 lambda 파라메터 조정  \n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "                \n",
    "                logits = model(input)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "                loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "                loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "\n",
    "            loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3    \n",
    "            trn_loss.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #########################################################\n",
    "            \n",
    "            \n",
    "#             # compute output\n",
    "#             output = model(input)\n",
    "#             loss = criterion(output, target)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if args.apex:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            # Printing vital information\n",
    "            if (batch_idx + 1) % (args.log_interval) == 0:\n",
    "                s = f'[Epoch {epoch} Batch {batch_idx+1}/{len(train_loader)}] ' \\\n",
    "                f'loss: {running_loss / args.log_interval:.4f}'\n",
    "                print(s)\n",
    "                running_loss = 0\n",
    "                \n",
    "                \n",
    "#             if True or batch_idx % args.log_interval == 0:\n",
    "#                 # Every print_freq iterations, check the loss, accuracy, and speed.\n",
    "#                 # For best performance, it doesn't make sense to print these metrics every\n",
    "#                 # iteration, since they incur an allreduce and some host<->device syncs.\n",
    "\n",
    "#                 # Measure accuracy\n",
    "#                 prec1, prec5 = util.accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "#                 # Average loss and accuracy across processes for logging\n",
    "#                 if args.multigpus_distributed:\n",
    "#                     reduced_loss = dis_util.reduce_tensor(loss.data, args)\n",
    "#                     prec1 = dis_util.reduce_tensor(prec1, args)\n",
    "#                     prec5 = dis_util.reduce_tensor(prec5, args)\n",
    "#                 else:\n",
    "#                     reduced_loss = loss.data\n",
    "\n",
    "#                 # to_python_float incurs a host<->device sync\n",
    "#                 losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "#                 top1.update(to_python_float(prec1), input.size(0))\n",
    "#                 top5.update(to_python_float(prec5), input.size(0))\n",
    "                \n",
    "#                 ## Waiting until finishing operations on GPU (Pytorch default: async)\n",
    "#                 torch.cuda.synchronize()\n",
    "#                 batch_time.update((time.time() - end)/args.log_interval)\n",
    "#                 end = time.time()\n",
    "\n",
    "#                 if current_gpu == 0:\n",
    "#                     print('Epoch: [{0}][{1}/{2}]  '\n",
    "#                           'Time {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "#                           'Speed {3:.3f} ({4:.3f})  '\n",
    "#                           'Loss {loss.val:.10f} ({loss.avg:.4f})  '\n",
    "#                           'Prec@1 {top1.val:.3f} ({top1.avg:.3f})  '\n",
    "#                           'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                               epoch, batch_idx, len(train_loader),\n",
    "#                               args.world_size*args.batch_size/batch_time.val,\n",
    "#                               args.world_size*args.batch_size/batch_time.avg,\n",
    "#                               batch_time=batch_time,\n",
    "#                               loss=losses, top1=top1, top5=top5))\n",
    "#                     model_history['epoch'].append(epoch)\n",
    "#                     model_history['batch_idx'].append(batch_idx)\n",
    "#                     model_history['batch_time'].append(batch_time.val)\n",
    "#                     model_history['losses'].append(losses.val)\n",
    "#                     model_history['top1'].append(top1.val)\n",
    "#                     model_history['top5'].append(top5.val)\n",
    "                    \n",
    "\n",
    "            input, target = prefetcher.next()\n",
    "               \n",
    "        acc1 = validate(test_loader, model, loss_fn, epoch, model_history, args)\n",
    "        \n",
    "        print(\" ****  acc1 :{}\".format(acc1))\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "        if not args.multigpus_distributed or (args.multigpus_distributed and args.rank % args.num_gpus == 0):\n",
    "            util.save_history(os.path.join(args.output_data_dir,\n",
    "                          'model_history.p'), model_history)\n",
    "\n",
    "            util.save_model({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_name': args.model_name,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'class_to_idx' : train_loader.dataset.class_to_idx,\n",
    "            }, is_best, args.model_dir)\n",
    "\n",
    "\n",
    "def validate(val_loader, model, loss_fn, epoch, model_history, args):\n",
    "    batch_time = util.AverageMeter('Time', ':6.3f')\n",
    "    losses = util.AverageMeter('Loss', ':.4e')\n",
    "    top1 = util.AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = util.AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = util.ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    \n",
    "    val_loss = []\n",
    "    val_true = []\n",
    "    val_pred = []    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    prefetcher = util.data_prefetcher(val_loader)\n",
    "    input, target = prefetcher.next()\n",
    "    batch_idx = 0\n",
    "    while input is not None:\n",
    "        batch_idx += 1\n",
    "    \n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "#             data = data.contiguous(memory_format=args.memory_format)\n",
    "#             target = target.contiguous()\n",
    "#             data = data.cuda(non_blocking=True)\n",
    "#             target = target.cuda(non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "                logits = model(input)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss= 0.5* loss_fn(grapheme, target[:,0]) + 0.25*loss_fn(vowel, target[:,1]) + \\\n",
    "                0.25*loss_fn(vowel, target[:,2])\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                grapheme = grapheme.cpu().argmax(dim=1).data.numpy()\n",
    "                vowel = vowel.cpu().argmax(dim=1).data.numpy()\n",
    "                cons = cons.cpu().argmax(dim=1).data.numpy()\n",
    "\n",
    "                val_true.append(target.cpu().numpy())\n",
    "                val_pred.append(np.stack([grapheme, vowel, cons], axis=1))                \n",
    "\n",
    "        val_true = np.concatenate(val_true)\n",
    "        val_pred = np.concatenate(val_pred)\n",
    "        val_loss = np.mean(val_loss)\n",
    "        trn_loss = np.mean(trn_loss)\n",
    "\n",
    "        score_g = recall_score(val_true[:,0], val_pred[:,0], average='macro')\n",
    "        score_v = recall_score(val_true[:,1], val_pred[:,1], average='macro')\n",
    "        score_c = recall_score(val_true[:,2], val_pred[:,2], average='macro')\n",
    "        final_score = np.average([score_g, score_v, score_c], weights=[2,1,1])\n",
    "\n",
    "        # Printing vital information\n",
    "        s = f'[Epoch {epoch}] ' \\\n",
    "        f'trn_loss: {trn_loss:.4f}, vld_loss: {val_loss:.4f}, score: {final_score:.4f}, ' \\\n",
    "        f'score_each: [{score_g:.4f}, {score_v:.4f}, {score_c:.4f}]'          \n",
    "        print(s)\n",
    "\n",
    "        ################################################################################\n",
    "        # ==> Save checkpoint and training stats\n",
    "        ################################################################################        \n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            state_dict = model.cpu().state_dict()\n",
    "            model = model.cuda()\n",
    "            torch.save(state_dict, os.path.join(args.model_output_dir, 'model.pt'))\n",
    "\n",
    "        # Record all statistics from this epoch\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'trn_loss': trn_loss,\n",
    "                'trn_time': trn_time,            \n",
    "                'val_loss': val_loss,\n",
    "                'score': final_score,\n",
    "                'score_g': score_g,\n",
    "                'score_v': score_v,\n",
    "                'score_c': score_c            \n",
    "            }\n",
    "        )      \n",
    "        \n",
    "        # === Save Model Parameters ===\n",
    "        logger.info(\"Model successfully saved at: {}\".format(args.model_output_dir))      \n",
    "        \n",
    "        \n",
    "#         # measure accuracy and record loss\n",
    "#         prec1, prec5 = util.accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "#         if args.multigpus_distributed:\n",
    "#             reduced_loss = dis_util.reduce_tensor(loss.data, args)\n",
    "#             prec1 = dis_util.reduce_tensor(prec1, args)\n",
    "#             prec5 = dis_util.reduce_tensor(prec5, args)\n",
    "#         else:\n",
    "#             reduced_loss = loss.data\n",
    "\n",
    "#         losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "#         top1.update(to_python_float(prec1), input.size(0))\n",
    "#         top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "#         # TODO:  Change timings to mirror train().\n",
    "#         if args.current_gpu == 0 and batch_idx % args.log_interval == 0:\n",
    "#             print('Test: [{0}/{1}]  '\n",
    "#                   'Time {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "#                   'Speed {2:.3f} ({3:.3f})  '\n",
    "#                   'Loss {loss.val:.4f} ({loss.avg:.4f})  '\n",
    "#                   'Prec@1 {top1.val:.3f} ({top1.avg:.3f})  '\n",
    "#                   'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "#                       batch_idx, len(val_loader),\n",
    "#                       args.world_size * args.batch_size / batch_time.val,\n",
    "#                       args.world_size * args.batch_size / batch_time.avg,\n",
    "#                       batch_time=batch_time, loss=losses,\n",
    "#                       top1=top1, top5=top5))\n",
    "#             model_history['val_epoch'].append(epoch)\n",
    "#             model_history['val_batch_idx'].append(batch_idx)\n",
    "#             model_history['val_batch_time'].append(batch_time.val)\n",
    "#             model_history['val_losses'].append(losses.val)\n",
    "#             model_history['val_top1'].append(top1.val)\n",
    "#             model_history['val_top5'].append(top5.val)\n",
    "#         input, target = prefetcher.next()\n",
    "\n",
    "#     print('  Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "#           .format(top1=top1, top5=top5))\n",
    "#     model_history['val_avg_epoch'].append(epoch)\n",
    "#     model_history['val_avg_batch_time'].append(batch_time.avg)\n",
    "#     model_history['val_avg_losses'].append(losses.avg)\n",
    "#     model_history['val_avg_top1'].append(top1.avg)\n",
    "#     model_history['val_avg_top5'].append(top5.avg)\n",
    "#     return top1.avg\n",
    "\n",
    "## iter() overflowerror: cannot serialize a bytes object larger than 4 gib --> num_worker=0 resolved\n",
    "def main():\n",
    "    args = parser_args()\n",
    "    args.use_cuda = args.num_gpus > 0\n",
    "    print(\"args.use_cuda : {} , args.num_gpus : {}\".format(\n",
    "        args.use_cuda, args.num_gpus))\n",
    "    args.kwargs = {'num_workers': 0,\n",
    "                   'pin_memory': True} if args.use_cuda else {}\n",
    "    args.device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "    dis_util.dist_init(train, args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/dis_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/dis_util.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import sagemaker_containers\n",
    "\n",
    "try:\n",
    "    from apex.parallel import DistributedDataParallel as DDP\n",
    "    from apex.fp16_utils import *\n",
    "    from apex import amp, optimizers\n",
    "    from apex.multi_tensor_apply import multi_tensor_applier\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def dist_init(fn, args):\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    args.is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "    args.is_multigpus = args.num_gpus > 1\n",
    "    args.multigpus_distributed = (args.is_distributed or args.is_multigpus)\n",
    "\n",
    "    logger.debug(\"Distributed training - {}\".format(args.is_distributed))\n",
    "    logger.debug(\"Number of gpus available - {}\".format(args.num_gpus))\n",
    "\n",
    "    args.world_size = 1\n",
    "    if args.multigpus_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        args.apex = True\n",
    "        args.world_size = len(args.hosts) * args.num_gpus\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        args.host_num = args.hosts.index(args.current_host)\n",
    "        mp.spawn(fn, nprocs=args.num_gpus, args=(args,))\n",
    "    else:\n",
    "        current_gpu = 0\n",
    "        fn(current_gpu, args)\n",
    "\n",
    "\n",
    "def dist_setting(current_gpu, model, loss_fn, args):\n",
    "    print(\"channels_last : {}\".format(args.channels_last))\n",
    "    if args.channels_last:\n",
    "        args.memory_format = torch.channels_last\n",
    "    else:\n",
    "        args.memory_format = torch.contiguous_format\n",
    "\n",
    "    if args.apex:\n",
    "        args.lr = args.lr*float(args.batch_size*args.world_size)/256.\n",
    "    args.current_gpu = current_gpu\n",
    "    if args.current_gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.current_gpu))\n",
    "\n",
    "    if args.multigpus_distributed:\n",
    "        args.rank = args.num_gpus * args.host_num + args.current_gpu\n",
    "        dist.init_process_group(backend=args.backend,\n",
    "                                rank=args.rank, world_size=args.world_size)\n",
    "        logger.info('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\n",
    "            args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n",
    "            dist.get_rank(), args.num_gpus))\n",
    "\n",
    "    if args.sync_bn:\n",
    "        import apex\n",
    "        print(\"using apex synced BN\")\n",
    "        model = apex.parallel.convert_syncbn_model(model)\n",
    "\n",
    "    if args.multigpus_distributed:\n",
    "        if args.current_gpu is not None:\n",
    "            torch.cuda.set_device(args.current_gpu)\n",
    "            args.batch_size = int(args.batch_size / args.num_gpus)\n",
    "            if not args.apex:\n",
    "                model.cuda(args.current_gpu)\n",
    "                model = torch.nn.parallel.DistributedDataParallel(\n",
    "                    model, device_ids=[args.current_gpu])\n",
    "        else:\n",
    "            if not args.apex:\n",
    "                model.cuda()\n",
    "                model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif args.current_gpu is not None:\n",
    "        torch.cuda.set_device(args.current_gpu)\n",
    "        if not args.apex:\n",
    "            model = model.cuda(args.current_gpu)\n",
    "    else:\n",
    "        if not args.apex:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    return model, args\n",
    "\n",
    "\n",
    "def apex_init(model, optimizer, args):\n",
    "    model = model.cuda().to(memory_format=args.memory_format)\n",
    "    model, optimizer = amp.initialize(model, optimizer,\n",
    "                                      opt_level=args.opt_level,\n",
    "                                      keep_batchnorm_fp32=args.keep_batchnorm_fp32,\n",
    "                                      loss_scale=args.loss_scale\n",
    "                                      )\n",
    "    if args.multigpus_distributed:\n",
    "        model = DDP(model, delay_allreduce=True)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, args):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= args.world_size\n",
    "    return rt\n",
    "\n",
    "\n",
    "def fast_collate(batch, memory_format=torch.channels_last):\n",
    "    imgs = [img[0] for img in batch]\n",
    "    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n",
    "    w = imgs[0].shape[0]\n",
    "    h = imgs[0].shape[1]\n",
    "    tensor = torch.zeros((len(imgs), 3, w, h), dtype=torch.uint8).contiguous(\n",
    "        memory_format=memory_format)\n",
    "    for i, img in enumerate(imgs):\n",
    "        nump_array = np.array(img, dtype=np.uint8)\n",
    "        if(nump_array.ndim < 3):\n",
    "            nump_array = np.expand_dims(nump_array, axis=-1)\n",
    "        nump_array = np.rollaxis(nump_array, 2)\n",
    "        tensor[i] += torch.from_numpy(nump_array)\n",
    "    return tensor, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/util.py\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import models\n",
    "\n",
    "import sagemaker_containers\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def torch_model(model_name, pretrained=True):\n",
    "    model_names = sorted(name for name in models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(models.__dict__[name]))\n",
    "\n",
    "    if(model_name == \"inception_v3\"):\n",
    "        raise RuntimeError(\n",
    "            \"Currently, inception_v3 is not supported by this example.\")\n",
    "\n",
    "    # create model\n",
    "    if pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(model_name))\n",
    "        model = models.__dict__[model_name](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(model_name))\n",
    "        model = models.__dict__[model_name]()\n",
    "    return model\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def save_model(state, is_best, model_dir):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    filename = os.path.join(model_dir, 'checkpoint.pth')\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(model_dir, 'model_best.pth'))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "        \n",
    "def adjust_learning_rate(optimizer, epoch, step, len_epoch, args):\n",
    "    \"\"\"LR schedule that should yield 76% converged accuracy with batch size 256\"\"\"\n",
    "    factor = epoch // 30\n",
    "\n",
    "    if epoch >= 80:\n",
    "        factor = factor + 1\n",
    "\n",
    "    lr = args.lr*(0.1**factor)\n",
    "\n",
    "    \"\"\"Warmup\"\"\"\n",
    "    if epoch < 5:\n",
    "        lr = lr*float(1 + step + epoch*len_epoch)/(5.*len_epoch)\n",
    "\n",
    "    if(args.current_gpu == 0):\n",
    "        print(\"epoch = {}, step = {}, lr = {}\".format(epoch, step, lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "        \n",
    "class data_prefetcher():\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.mean = torch.tensor([0.5 * 255, 0.5 * 255, 0.5 * 255]).cuda().view(1,3,1,1)\n",
    "        self.std = torch.tensor([0.5 * 255, 0.5 * 255, 0.5 * 255]).cuda().view(1,3,1,1)\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(non_blocking=True)\n",
    "            self.next_target = self.next_target.cuda(non_blocking=True)\n",
    "            self.next_input = self.next_input.float()\n",
    "            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        target = self.next_target\n",
    "        if input is not None:\n",
    "            input.record_stream(torch.cuda.current_stream())\n",
    "        if target is not None:\n",
    "            target.record_stream(torch.cuda.current_stream())\n",
    "        self.preload()\n",
    "        return input, target\n",
    "    \n",
    "\n",
    "\n",
    "def save_history(path, history):\n",
    "\n",
    "    history_for_json = {}\n",
    "    # transform float values that aren't json-serializable\n",
    "    for key in history.keys():\n",
    "        history_for_json[key] = list(map(float, history[key]))\n",
    "\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(history_for_json, f, separators=(\n",
    "            ',', ':'), sort_keys=True, indent=4)\n",
    "        \n",
    "        \n",
    "\n",
    "def init_modelhistory(model_history):\n",
    "    model_history['epoch'] = []\n",
    "    model_history['batch_idx'] = []\n",
    "    model_history['batch_time'] = []\n",
    "    model_history['losses'] = []\n",
    "    model_history['top1'] = []\n",
    "    model_history['top5'] = []\n",
    "    model_history['val_epoch'] = []\n",
    "    model_history['val_batch_idx'] = []\n",
    "    model_history['val_batch_time'] = []\n",
    "    model_history['val_losses'] = []\n",
    "    model_history['val_top1'] = []\n",
    "    model_history['val_top5'] = []\n",
    "    model_history['val_avg_epoch'] = []\n",
    "    model_history['val_avg_batch_time'] = []\n",
    "    model_history['val_avg_losses'] = []\n",
    "    model_history['val_avg_top1'] = []\n",
    "    model_history['val_avg_top5'] = []\n",
    "    return model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'height' : 137,\n",
    "        'width' : 236,\n",
    "        'num_epochs': 1,\n",
    "        'batch-size' : 256,\n",
    "        'backend': 'nccl',\n",
    "        'lr': 0.001,\n",
    "        'apex' : True,\n",
    "        'opt-level' : 'O0'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 ms, sys: 3.61 ms, total: 37.2 ms\n",
      "Wall time: 43.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# all input configurations, parameters, and metrics specified in estimator \n",
    "# definition are automatically tracked\n",
    "estimator = PyTorch(\n",
    "    entry_point='./main_trainer.py',\n",
    "    source_dir='./src',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n",
    "    framework_version='1.5.0',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.8xlarge',\n",
    "    train_volume_size=400,\n",
    "    hyperparameters=hyperparameters,\n",
    "#     train_use_spot_instances=True,  # spot instance 활용\n",
    "#     train_max_run=12*60*60,\n",
    "#     train_max_wait=12*60*60,\n",
    "#     checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "#     tensorboard_output_config=TensorBoardOutputConfig(tensorboard_output),\n",
    "    metric_definitions=[\n",
    "        {'Name':'train:loss', 'Regex':'Train Loss: (.*?);'},\n",
    "        {'Name':'test:loss', 'Regex':'Test Average loss: (.*?),'},\n",
    "        {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}\n",
    "    ],\n",
    "    enable_sagemaker_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "training_job_name = \"training-job-{}\".format(int(time.time()))\n",
    "\n",
    "# Now associate the estimator with the Experiment and Trial\n",
    "estimator.fit(\n",
    "    inputs=s3_data_path, \n",
    "    job_name=training_job_name,\n",
    "    logs='All',\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-07 11:35:20 Starting - Starting the training job...\n",
      "2020-08-07 11:35:22 Starting - Launching requested ML instances.........\n",
      "2020-08-07 11:36:53 Starting - Preparing the instances for training...\n",
      "2020-08-07 11:37:40 Downloading - Downloading input data......\n",
      "2020-08-07 11:38:23 Training - Downloading the training image...\n",
      "2020-08-07 11:39:10 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-08-07 11:39:11,913 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-08-07 11:39:11,956 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-08-07 11:39:14,991 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-07 11:39:15,276 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-07 11:39:15,276 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-07 11:39:15,277 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-07 11:39:15,277 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpwd04ou1u/module_dir\u001b[0m\n",
      "\u001b[34mCollecting albumentations\n",
      "  Downloading albumentations-0.4.6.tar.gz (117 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow\n",
      "  Downloading pyarrow-1.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.2.2)\u001b[0m\n",
      "\u001b[34mCollecting imgaug>=0.4.0\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (5.3.1)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.3.0.36-cp36-cp36m-manylinux2014_x86_64.whl (36.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.14.0)\u001b[0m\n",
      "\u001b[34mCollecting imageio\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python\n",
      "  Downloading opencv_python-4.3.0.36-cp36-cp36m-manylinux2014_x86_64.whl (43.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (7.1.0)\u001b[0m\n",
      "\u001b[34mCollecting Shapely\n",
      "  Downloading Shapely-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-image>=0.14.2\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[34mCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4)\u001b[0m\n",
      "\u001b[34mCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.7.24-py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (4.4.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: albumentations, default-user-module-name\n",
      "  Building wheel for albumentations (setup.py): started\n",
      "  Building wheel for albumentations (setup.py): finished with status 'done'\n",
      "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65167 sha256=f31b15995348f2f39216a1b3f2213d150d81b8aaa760fd3b076b2707bb4332eb\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/db/df/d6cb0be184075a7799c1fd79240c389c16f51dfe18dc3332fa\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=34926 sha256=2bd0905997b22c56534ea4a952a15bdcc23cb8c62d7ba60547b296b30d4df3bd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p8uoj7rn/wheels/8b/50/9b/d5570796b0f0ea889719021dd839c75b359010f1e09e3096ee\u001b[0m\n",
      "\u001b[34mSuccessfully built albumentations default-user-module-name\u001b[0m\n",
      "\u001b[34mERROR: scikit-image 0.17.2 has requirement pillow!=7.1.0,!=7.1.1,>=4.3.0, but you'll have pillow 7.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: imageio, opencv-python, Shapely, PyWavelets, tifffile, scikit-image, imgaug, opencv-python-headless, albumentations, pyarrow, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed PyWavelets-1.1.1 Shapely-1.7.0 albumentations-0.4.6 default-user-module-name-1.0.0 imageio-2.9.0 imgaug-0.4.0 opencv-python-4.3.0.36 opencv-python-headless-4.3.0.36 pyarrow-1.0.0 scikit-image-0.17.2 tifffile-2020.7.24\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1; however, version 20.2.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-08-07 11:39:30,279 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"opt-level\": \"O0\",\n",
      "        \"lr\": 0.001,\n",
      "        \"apex\": true,\n",
      "        \"batch-size\": 256,\n",
      "        \"width\": 236,\n",
      "        \"backend\": \"nccl\",\n",
      "        \"num_epochs\": 1,\n",
      "        \"height\": 137\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"training-job-1596800119\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-322537213286/training-job-1596800119/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"./main_trainer\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"./main_trainer.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"apex\":true,\"backend\":\"nccl\",\"batch-size\":256,\"height\":137,\"lr\":0.001,\"num_epochs\":1,\"opt-level\":\"O0\",\"width\":236}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=./main_trainer.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=./main_trainer\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-322537213286/training-job-1596800119/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"apex\":true,\"backend\":\"nccl\",\"batch-size\":256,\"height\":137,\"lr\":0.001,\"num_epochs\":1,\"opt-level\":\"O0\",\"width\":236},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"training-job-1596800119\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-322537213286/training-job-1596800119/source/sourcedir.tar.gz\",\"module_name\":\"./main_trainer\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"./main_trainer.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--apex\",\"True\",\"--backend\",\"nccl\",\"--batch-size\",\"256\",\"--height\",\"137\",\"--lr\",\"0.001\",\"--num_epochs\",\"1\",\"--opt-level\",\"O0\",\"--width\",\"236\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_OPT-LEVEL=O0\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_APEX=true\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_WIDTH=236\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_HEIGHT=137\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python ./main_trainer.py --apex True --backend nccl --batch-size 256 --height 137 --lr 0.001 --num_epochs 1 --opt-level O0 --width 236\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34margs.use_cuda : True , args.num_gpus : 4\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 4\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 3 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 0 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 1 for training\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 4 nodes. Current host rank is 1. Number of gpus: 4\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 2 for training\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 4 nodes. Current host rank is 2. Number of gpus: 4\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 4 nodes. Current host rank is 3. Number of gpus: 4\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 4 nodes. Current host rank is 0. Number of gpus: 4\u001b[0m\n",
      "\u001b[34mSelected optimization level O0:  Pure FP32 training.\n",
      "\u001b[0m\n",
      "\u001b[34mDefaults for this optimization level are:\u001b[0m\n",
      "\u001b[34menabled                : True\u001b[0m\n",
      "\u001b[34mopt_level              : O0\u001b[0m\n",
      "\u001b[34mcast_model_type        : torch.float32\u001b[0m\n",
      "\u001b[34mpatch_torch_functions  : False\u001b[0m\n",
      "\u001b[34mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[34mmaster_weights         : False\u001b[0m\n",
      "\u001b[34mloss_scale             : 1.0\u001b[0m\n",
      "\u001b[34mProcessing user overrides (additional kwargs that are not None)...\u001b[0m\n",
      "\u001b[34mAfter processing overrides, optimization options are:\u001b[0m\n",
      "\u001b[34menabled                : True\u001b[0m\n",
      "\u001b[34mopt_level              : O0\u001b[0m\n",
      "\u001b[34mcast_model_type        : torch.float32\u001b[0m\n",
      "\u001b[34mpatch_torch_functions  : False\u001b[0m\n",
      "\u001b[34mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[34mmaster_weights         : False\u001b[0m\n",
      "\u001b[34mloss_scale             : 1.0\u001b[0m\n",
      "\u001b[34mNCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 40168/160672 (25%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10042/40168 (25%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 40168/160672 (25%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10042/40168 (25%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 40168/160672 (25%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10042/40168 (25%) of test data\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 1, lr = 0.0008025477707006369\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:21.176 algo-1:146 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:21.176 algo-1:147 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:21.176 algo-1:147 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:21.176 algo-1:147 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:21.176 algo-1:146 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:21.176 algo-1:146 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:21.179 algo-1:147 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:21.179 algo-1:146 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:22.173 algo-1:148 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:22.174 algo-1:148 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:22.174 algo-1:148 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:22.177 algo-1:148 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 40168/160672 (25%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10042/40168 (25%) of test data\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:27.451 algo-1:149 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:27.452 algo-1:149 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:27.452 algo-1:149 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-07 11:45:27.454 algo-1:149 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 2, lr = 0.0008038216560509555\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 3, lr = 0.0008050955414012739\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 4, lr = 0.0008063694267515923\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 5, lr = 0.0008076433121019109\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 6, lr = 0.0008089171974522293\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 7, lr = 0.0008101910828025477\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 8, lr = 0.0008114649681528663\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 9, lr = 0.0008127388535031847\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/628] loss: 3.4125\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/628] loss: 3.4012\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/628] loss: 3.4205\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/628] loss: 3.4183\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 10, lr = 0.0008140127388535033\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 11, lr = 0.0008152866242038217\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 12, lr = 0.0008165605095541401\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 13, lr = 0.0008178343949044587\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 14, lr = 0.0008191082802547771\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 15, lr = 0.0008203821656050955\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 16, lr = 0.0008216560509554141\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 17, lr = 0.0008229299363057325\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 18, lr = 0.000824203821656051\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 19, lr = 0.0008254777070063695\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/628] loss: 2.9762\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/628] loss: 3.0077\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/628] loss: 3.0069\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/628] loss: 2.9689\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 20, lr = 0.0008267515923566879\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 21, lr = 0.0008280254777070064\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 22, lr = 0.0008292993630573249\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 23, lr = 0.0008305732484076433\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 24, lr = 0.0008318471337579618\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 25, lr = 0.0008331210191082803\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 26, lr = 0.0008343949044585987\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 27, lr = 0.0008356687898089172\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 28, lr = 0.0008369426751592357\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 29, lr = 0.0008382165605095542\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/628] loss: 2.7817\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/628] loss: 2.9621\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/628] loss: 2.7095\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/628] loss: 2.5840\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 30, lr = 0.0008394904458598726\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 31, lr = 0.0008407643312101911\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 32, lr = 0.0008420382165605096\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 33, lr = 0.0008433121019108281\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 34, lr = 0.0008445859872611466\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 35, lr = 0.000845859872611465\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 36, lr = 0.0008471337579617835\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 37, lr = 0.000848407643312102\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 38, lr = 0.0008496815286624204\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 39, lr = 0.0008509554140127389\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/628] loss: 2.6701\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/628] loss: 2.5629\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/628] loss: 2.7111\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/628] loss: 2.3314\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 40, lr = 0.0008522292993630574\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch = 1, step = 41, lr = 0.0008535031847133758\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 42, lr = 0.0008547770700636944\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 43, lr = 0.0008560509554140128\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 44, lr = 0.0008573248407643312\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 45, lr = 0.0008585987261146498\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 46, lr = 0.0008598726114649682\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 47, lr = 0.0008611464968152866\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 48, lr = 0.0008624203821656052\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 49, lr = 0.0008636942675159236\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/628] loss: 2.3856\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/628] loss: 2.2749\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/628] loss: 2.1058\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/628] loss: 2.5695\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 50, lr = 0.000864968152866242\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 51, lr = 0.0008662420382165606\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 52, lr = 0.000867515923566879\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 53, lr = 0.0008687898089171976\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 54, lr = 0.000870063694267516\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 55, lr = 0.0008713375796178344\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 56, lr = 0.000872611464968153\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 57, lr = 0.0008738853503184714\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 58, lr = 0.0008751592356687898\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 59, lr = 0.0008764331210191084\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/628] loss: 2.2038\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/628] loss: 2.1372\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/628] loss: 2.5245\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/628] loss: 2.6619\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 60, lr = 0.0008777070063694268\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 61, lr = 0.0008789808917197453\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 62, lr = 0.0008802547770700638\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 63, lr = 0.0008815286624203822\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 64, lr = 0.0008828025477707007\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 65, lr = 0.0008840764331210192\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 66, lr = 0.0008853503184713377\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 67, lr = 0.0008866242038216561\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 68, lr = 0.0008878980891719746\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 69, lr = 0.0008891719745222931\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/628] loss: 2.2705\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/628] loss: 1.8194\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/628] loss: 1.9890\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/628] loss: 1.9989\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 70, lr = 0.0008904458598726115\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 71, lr = 0.00089171974522293\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 72, lr = 0.0008929936305732485\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 73, lr = 0.000894267515923567\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 74, lr = 0.0008955414012738855\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 75, lr = 0.0008968152866242038\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 76, lr = 0.0008980891719745222\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 77, lr = 0.0008993630573248408\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 78, lr = 0.0009006369426751592\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 79, lr = 0.0009019108280254776\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/628] loss: 2.3397\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/628] loss: 1.9436\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/628] loss: 1.8373\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/628] loss: 2.2778\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 80, lr = 0.0009031847133757962\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 81, lr = 0.0009044585987261146\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 82, lr = 0.000905732484076433\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 83, lr = 0.0009070063694267516\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 84, lr = 0.00090828025477707\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 85, lr = 0.0009095541401273885\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 86, lr = 0.000910828025477707\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 87, lr = 0.0009121019108280254\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 88, lr = 0.0009133757961783439\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 89, lr = 0.0009146496815286624\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/628] loss: 1.9377\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/628] loss: 2.0224\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/628] loss: 2.0380\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/628] loss: 2.0777\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 90, lr = 0.0009159235668789808\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 91, lr = 0.0009171974522292993\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 92, lr = 0.0009184713375796178\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 93, lr = 0.0009197452229299363\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 94, lr = 0.0009210191082802548\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 95, lr = 0.0009222929936305732\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 96, lr = 0.0009235668789808917\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 97, lr = 0.0009248407643312102\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 98, lr = 0.0009261146496815286\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 99, lr = 0.0009273885350318471\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/628] loss: 2.2295\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/628] loss: 1.6920\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/628] loss: 1.9805\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/628] loss: 1.4508\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 100, lr = 0.0009286624203821656\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 101, lr = 0.0009299363057324841\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 102, lr = 0.0009312101910828025\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 103, lr = 0.000932484076433121\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 104, lr = 0.0009337579617834395\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 105, lr = 0.0009350318471337579\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 106, lr = 0.0009363057324840764\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 107, lr = 0.0009375796178343949\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 108, lr = 0.0009388535031847133\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 109, lr = 0.0009401273885350319\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 110/628] loss: 1.7947\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 110/628] loss: 1.7753\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 110/628] loss: 1.7324\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 110/628] loss: 1.9263\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 110, lr = 0.0009414012738853503\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 111, lr = 0.0009426751592356687\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 112, lr = 0.0009439490445859873\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 113, lr = 0.0009452229299363057\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 114, lr = 0.0009464968152866242\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 115, lr = 0.0009477707006369427\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 116, lr = 0.0009490445859872611\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 117, lr = 0.0009503184713375797\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 118, lr = 0.0009515923566878981\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 119, lr = 0.0009528662420382165\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 120/628] loss: 1.6561\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 120/628] loss: 1.8102\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 120/628] loss: 2.0171\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 120/628] loss: 1.9458\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 120, lr = 0.0009541401273885351\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 121, lr = 0.0009554140127388535\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 122, lr = 0.0009566878980891719\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 123, lr = 0.0009579617834394905\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 124, lr = 0.0009592356687898089\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 125, lr = 0.0009605095541401274\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 126, lr = 0.0009617834394904459\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 127, lr = 0.0009630573248407643\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 128, lr = 0.0009643312101910828\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 129, lr = 0.0009656050955414013\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 130/628] loss: 1.8755\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 130/628] loss: 1.3431\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 130/628] loss: 1.8067\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 130/628] loss: 1.2689\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 130, lr = 0.0009668789808917197\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 131, lr = 0.0009681528662420382\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 132, lr = 0.0009694267515923567\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 133, lr = 0.0009707006369426752\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 134, lr = 0.0009719745222929936\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 135, lr = 0.0009732484076433121\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 136, lr = 0.0009745222929936306\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 137, lr = 0.0009757961783439491\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 138, lr = 0.0009770700636942675\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 139, lr = 0.000978343949044586\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 140/628] loss: 1.7456\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 140/628] loss: 1.6823\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 140/628] loss: 1.4919\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 140/628] loss: 1.7091\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 140, lr = 0.0009796178343949044\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 141, lr = 0.0009808917197452229\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 142, lr = 0.0009821656050955415\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 143, lr = 0.00098343949044586\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 144, lr = 0.0009847133757961784\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 145, lr = 0.0009859872611464968\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 146, lr = 0.0009872611464968152\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 147, lr = 0.000988535031847134\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 148, lr = 0.0009898089171974523\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 149, lr = 0.0009910828025477708\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 150/628] loss: 1.2405\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 150/628] loss: 1.3983\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 150/628] loss: 1.8472\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 150/628] loss: 1.8422\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 150, lr = 0.0009923566878980892\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch = 1, step = 151, lr = 0.0009936305732484076\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 152, lr = 0.000994904458598726\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 153, lr = 0.0009961783439490447\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 154, lr = 0.0009974522292993632\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 155, lr = 0.0009987261146496816\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 156, lr = 0.001\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 157, lr = 0.0010012738853503185\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 158, lr = 0.0010025477707006369\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 159, lr = 0.0010038216560509555\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 160/628] loss: 1.6784\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 160/628] loss: 1.6718\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 160/628] loss: 2.0846\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 160/628] loss: 1.7365\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 160, lr = 0.001005095541401274\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 161, lr = 0.0010063694267515924\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 162, lr = 0.0010076433121019108\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 163, lr = 0.0010089171974522293\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 164, lr = 0.0010101910828025477\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 165, lr = 0.0010114649681528664\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 166, lr = 0.0010127388535031848\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 167, lr = 0.0010140127388535032\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 168, lr = 0.0010152866242038217\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 169, lr = 0.00101656050955414\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 170/628] loss: 1.2606\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 170/628] loss: 1.6647\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 170/628] loss: 1.5829\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 170/628] loss: 1.6770\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 170, lr = 0.0010178343949044588\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 171, lr = 0.0010191082802547772\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 172, lr = 0.0010203821656050956\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 173, lr = 0.001021656050955414\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 174, lr = 0.0010229299363057325\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 175, lr = 0.001024203821656051\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 176, lr = 0.0010254777070063696\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 177, lr = 0.001026751592356688\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 178, lr = 0.0010280254777070064\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 179, lr = 0.0010292993630573249\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 180/628] loss: 1.2260\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 180/628] loss: 1.2836\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 180/628] loss: 1.3463\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 180/628] loss: 1.7023\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 180, lr = 0.0010305732484076433\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 181, lr = 0.0010318471337579618\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 182, lr = 0.0010331210191082804\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 183, lr = 0.0010343949044585988\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 184, lr = 0.0010356687898089173\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 185, lr = 0.0010369426751592357\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 186, lr = 0.0010382165605095541\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 187, lr = 0.0010394904458598726\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 188, lr = 0.0010407643312101912\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 189, lr = 0.0010420382165605097\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 190/628] loss: 1.0412\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 190/628] loss: 1.5811\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 190/628] loss: 1.8660\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 190/628] loss: 2.3459\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 190, lr = 0.001043312101910828\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 191, lr = 0.0010445859872611465\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 192, lr = 0.001045859872611465\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 193, lr = 0.0010471337579617836\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 194, lr = 0.001048407643312102\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 195, lr = 0.0010496815286624205\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 196, lr = 0.001050955414012739\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 197, lr = 0.0010522292993630574\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 198, lr = 0.0010535031847133758\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 199, lr = 0.0010547770700636944\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 200/628] loss: 1.1534\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 200/628] loss: 1.4665\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 200/628] loss: 1.5966\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 200/628] loss: 1.5353\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 200, lr = 0.0010560509554140129\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 201, lr = 0.0010573248407643313\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 202, lr = 0.0010585987261146497\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 203, lr = 0.0010598726114649682\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 204, lr = 0.0010611464968152866\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 205, lr = 0.001062420382165605\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 206, lr = 0.0010636942675159235\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 207, lr = 0.001064968152866242\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 208, lr = 0.0010662420382165606\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 209, lr = 0.001067515923566879\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 210/628] loss: 2.3015\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 210/628] loss: 1.5115\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 210/628] loss: 1.3187\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 210/628] loss: 1.6006\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 210, lr = 0.0010687898089171974\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 211, lr = 0.0010700636942675159\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 212, lr = 0.0010713375796178343\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 213, lr = 0.0010726114649681527\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 214, lr = 0.0010738853503184714\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 215, lr = 0.0010751592356687898\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 216, lr = 0.0010764331210191083\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 217, lr = 0.0010777070063694267\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 218, lr = 0.0010789808917197451\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 219, lr = 0.0010802547770700636\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 220/628] loss: 2.0997\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 220/628] loss: 1.6255\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 220/628] loss: 1.2923\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 220/628] loss: 1.6725\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 220, lr = 0.0010815286624203822\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 221, lr = 0.0010828025477707007\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 222, lr = 0.001084076433121019\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 223, lr = 0.0010853503184713375\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 224, lr = 0.001086624203821656\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 225, lr = 0.0010878980891719744\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 226, lr = 0.001089171974522293\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 227, lr = 0.0010904458598726115\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 228, lr = 0.00109171974522293\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 229, lr = 0.0010929936305732484\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 230/628] loss: 1.4784\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 230/628] loss: 2.3553\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 230/628] loss: 1.5216\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 230/628] loss: 1.3440\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 230, lr = 0.0010942675159235668\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 231, lr = 0.0010955414012738854\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 232, lr = 0.0010968152866242039\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 233, lr = 0.0010980891719745223\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 234, lr = 0.0010993630573248407\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 235, lr = 0.0011006369426751592\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 236, lr = 0.0011019108280254776\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 237, lr = 0.0011031847133757963\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 238, lr = 0.0011044585987261147\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 239, lr = 0.0011057324840764331\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 240/628] loss: 1.3065\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 240/628] loss: 1.3136\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 240/628] loss: 1.5869\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 240/628] loss: 1.3011\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 240, lr = 0.0011070063694267516\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 241, lr = 0.00110828025477707\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 242, lr = 0.0011095541401273884\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 243, lr = 0.001110828025477707\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 244, lr = 0.0011121019108280255\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 245, lr = 0.001113375796178344\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 246, lr = 0.0011146496815286624\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 247, lr = 0.0011159235668789808\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 248, lr = 0.0011171974522292993\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 249, lr = 0.001118471337579618\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 250/628] loss: 1.2119\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 250/628] loss: 1.2863\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 250/628] loss: 1.5534\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 250/628] loss: 1.4667\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 250, lr = 0.0011197452229299363\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.logs_for_job(estimator.latest_training_job.name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "display(df_stats)\n",
    "\n",
    "#model.load_state_dict(torch.load('./model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
