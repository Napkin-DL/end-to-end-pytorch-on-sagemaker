{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modlue 3. Distributed-Mutigpu Training with CutMix-ScriptMode\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amzaon SageMaker API을 효과적으로 이용하기 위해 distributed-multigpu 학습을 위한 PyTorch 프레임워크 자체 구현만으로 모델 훈련을 수행해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time, datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 버전을 최신 버전으로 업데이트합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import boto3\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch, torchvision\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "# create a s3 bucket to hold data, note that your account might already created a bucket with the same name\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "job_bucket = 'sagemaker-experiments-{}-{}'.format(sess.region_name, account_id)\n",
    "data_bucket = 'sagemaker-{}-{}'.format(sess.region_name, account_id)\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=data_bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "local_data_paths = glob.glob('./input/train_image_data_*.parquet')\n",
    "s3_data_path = sagemaker.s3_input(s3_data='s3://{}/{}'.format(data_bucket, 'bangali/train'), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write main_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "albumentations\n",
    "pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/main_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/main_trainer.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import glob\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "import logging.handlers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import dis_util\n",
    "import sagemaker_containers\n",
    "import util\n",
    "\n",
    "## Apex import package\n",
    "try:\n",
    "    from apex.parallel import DistributedDataParallel as DDP\n",
    "    from apex.fp16_utils import *\n",
    "    from apex import amp, optimizers\n",
    "    from apex.multi_tensor_apply import multi_tensor_applier\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
    "\n",
    "\n",
    "## augmentation for setting\n",
    "from albumentations import (\n",
    "    Rotate,HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    ")\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "\n",
    "import dis_util\n",
    "import sagemaker_containers\n",
    "import util\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def parser_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Default Setting\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default='nccl',\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "    parser.add_argument('--channels-last', type=bool, default=True)\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('-p', '--print-freq', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "\n",
    "    # Hyperparameter Setting\n",
    "    parser.add_argument('--model_name', type=str, default='resnet50')\n",
    "    parser.add_argument('--height', type=int, default=128)\n",
    "    parser.add_argument('--width', type=int, default=128)\n",
    "    parser.add_argument('--num_folds', type=int, default=5)\n",
    "    parser.add_argument('--vld_fold_idx', type=int, default=4)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--num_epochs', type=int, default=3)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument('--test-batch-size', type=int, default=200, metavar='N',\n",
    "                        help='input batch size for testing (default: 200)')\n",
    "\n",
    "    # APEX Setting for Distributed Training\n",
    "    parser.add_argument('--apex', type=bool, default=False)\n",
    "    parser.add_argument('--opt-level', type=str, default='O0')\n",
    "    parser.add_argument('--keep-batchnorm-fp32', type=str, default=None)\n",
    "    parser.add_argument('--loss-scale', type=str, default=None)\n",
    "    parser.add_argument('--sync_bn', action='store_true',\n",
    "                        help='enabling apex sync BN.')\n",
    "    parser.add_argument('--prof', default=-1, type=int,\n",
    "                        help='Only run 10 iterations for profiling.')\n",
    "\n",
    "    # SageMaker Container environment\n",
    "    parser.add_argument('--hosts', type=list,\n",
    "                        default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current-host', type=str,\n",
    "                        default=os.environ['SM_CURRENT_HOST'])\n",
    "    parser.add_argument('--model-dir', type=str,\n",
    "                        default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--data-dir', type=str,\n",
    "                        default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--num-gpus', type=int,\n",
    "                        default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--output_data_dir', type=str,\n",
    "                        default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        last_hidden_units = self.model.fc.out_features\n",
    "        self.classifer_model = nn.Linear(last_hidden_units, 186)\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.model(x)\n",
    "        x = self.classifer_model(features)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class BangaliDataset(Dataset):\n",
    "    def __init__(self, imgs, label_df=None, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.label_df = label_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_idx = self.label_df.iloc[idx].id\n",
    "        img = (self.imgs[img_idx]).astype(np.uint8)\n",
    "        img = 255 - img\n",
    "    \n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']        \n",
    "        \n",
    "        if self.label_df is not None:\n",
    "            label_1 = self.label_df.iloc[idx].grapheme_root\n",
    "            label_2 = self.label_df.iloc[idx].vowel_diacritic\n",
    "            label_3 = self.label_df.iloc[idx].consonant_diacritic           \n",
    "            return img, np.array([label_1, label_2, label_3])        \n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "        \n",
    "\n",
    "def _rand_bbox(size, lam):\n",
    "    '''\n",
    "    CutMix Helper function.\n",
    "    Retrieved from https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "    '''\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    # 폭과 높이는 주어진 이미지의 폭과 높이의 beta distribution에서 뽑은 lambda로 얻는다\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    \n",
    "    # patch size 의 w, h 는 original image 의 w,h 에 np.sqrt(1-lambda) 를 곱해준 값입니다.\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # patch의 중심점은 uniform하게 뽑힘\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def _set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    mx.random.seed(seed)\n",
    "\n",
    "def _get_images(args, data_type='train'):\n",
    "\n",
    "    logger.info(\"=== Getting Labels ===\")\n",
    "    logger.info(args.data_dir)\n",
    "    \n",
    "    label_df = pd.read_csv(os.path.join(args.data_dir, 'train_folds.csv'))\n",
    "     \n",
    "    trn_fold = [i for i in range(args.num_folds) if i not in [args.vld_fold_idx]]\n",
    "    vld_fold = [args.vld_fold_idx]\n",
    "\n",
    "    trn_idx = label_df.loc[label_df['fold'].isin(trn_fold)].index\n",
    "    vld_idx = label_df.loc[label_df['fold'].isin(vld_fold)].index\n",
    "\n",
    "    logger.info(\"=== Getting Images ===\")\n",
    "    files = [file for file in glob.glob(os.path.join(args.data_dir,'*')) if file.split('.')[-1] == 'parquet']\n",
    "#     files = [f'{args.data_dir}/{data_type}_image_data_{i}.parquet' for i in range(4)]\n",
    "    logger.info(files)\n",
    "    \n",
    "    image_df_list = [pd.read_parquet(f) for f in files]\n",
    "    imgs = [df.iloc[:, 1:].values.reshape(-1, args.height, args.width) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    args.imgs = np.concatenate(imgs, axis=0)\n",
    "    \n",
    "    args.trn_df = label_df.loc[trn_idx]\n",
    "    args.vld_df = label_df.loc[vld_idx]\n",
    "    \n",
    "\n",
    "    \n",
    "    return args \n",
    "\n",
    "\n",
    "def _get_train_data_loader(args, **kwargs):\n",
    "    logger.info(\"Get train data loader\")\n",
    "    train_transforms = Compose([\n",
    "        Rotate(20),\n",
    "            OneOf([\n",
    "                IAAAdditiveGaussianNoise(),\n",
    "                GaussNoise(),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                MotionBlur(p=.2),\n",
    "                MedianBlur(blur_limit=3, p=0.1),\n",
    "                Blur(blur_limit=3, p=0.1),\n",
    "            ], p=0.2),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "            OneOf([\n",
    "                OpticalDistortion(p=0.3),\n",
    "                GridDistortion(p=.1),\n",
    "                IAAPiecewiseAffine(p=0.3),\n",
    "            ], p=0.2),\n",
    "            OneOf([\n",
    "                CLAHE(clip_limit=2),\n",
    "                IAASharpen(),\n",
    "                IAAEmboss(),\n",
    "                RandomBrightnessContrast(),            \n",
    "            ], p=0.3),\n",
    "            HueSaturationValue(p=0.3),\n",
    "#         ToTensor()\n",
    "        ], p=1.0)\n",
    "    \n",
    "    dataset = BangaliDataset(imgs=args.imgs, label_df=args.trn_df, transform=train_transforms)\n",
    "    train_sampler = data.distributed.DistributedSampler(\n",
    "        dataset, num_replicas=int(args.world_size), rank=int(args.rank)) if args.multigpus_distributed else None\n",
    "    return data.DataLoader(dataset, batch_size=args.batch_size, shuffle=train_sampler is None,\n",
    "                                       sampler=train_sampler, collate_fn=dis_util.fast_collate, **kwargs), train_sampler\n",
    "\n",
    "\n",
    "def _get_test_data_loader(args, **kwargs):\n",
    "    logger.info(\"Get test data loader\")   \n",
    "\n",
    "    dataset = BangaliDataset(imgs=args.imgs, label_df=args.vld_df)\n",
    "    val_sampler = data.distributed.DistributedSampler(dataset) if args.multigpus_distributed else None\n",
    "    return data.DataLoader(dataset, batch_size=args.test_batch_size, shuffle=False, \n",
    "                           sampler=val_sampler, collate_fn=dis_util.fast_collate, **kwargs)\n",
    "\n",
    "\n",
    "def train(current_gpu, args):\n",
    "    best_acc1 = -1\n",
    "    model_history = {}\n",
    "    model_history = util.init_modelhistory(model_history)\n",
    "    train_start = time.time()\n",
    "\n",
    "    ## choose model from pytorch model_zoo\n",
    "    model = util.torch_model(args.model_name, pretrained=True)\n",
    "    loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    ## distributed_setting \n",
    "    model, args = dis_util.dist_setting(current_gpu, model, loss_fn, args)\n",
    "\n",
    "    ## CuDNN library will benchmark several algorithms and pick that which it found to be fastest\n",
    "    cudnn.benchmark = False if args.seed else True\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    if args.apex:\n",
    "        model, optimizer = dis_util.apex_init(model, optimizer, args)\n",
    "    \n",
    "    \n",
    "#     args.collate_fn = partial(dis_util.fast_collate, memory_format=args.memory_format)\n",
    "   \n",
    "    args = _get_images(args, data_type='train')\n",
    "    train_loader, train_sampler = _get_train_data_loader(args, **args.kwargs)\n",
    "    test_loader = _get_test_data_loader(args, **args.kwargs)\n",
    "\n",
    "    logger.info(\"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "        len(train_loader.sampler), len(train_loader.dataset),\n",
    "        100. * len(train_loader.sampler) / len(train_loader.dataset)\n",
    "    ))\n",
    "\n",
    "    logger.info(\"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "        len(test_loader.sampler), len(test_loader.dataset),\n",
    "        100. * len(test_loader.sampler) / len(test_loader.dataset)\n",
    "    ))\n",
    "\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        ## \n",
    "        batch_time = util.AverageMeter('Time', ':6.3f')\n",
    "        data_time = util.AverageMeter('Data', ':6.3f')\n",
    "        losses = util.AverageMeter('Loss', ':.4e')\n",
    "        top1 = util.AverageMeter('Acc@1', ':6.2f')\n",
    "        top5 = util.AverageMeter('Acc@5', ':6.2f')\n",
    "        progress = util.ProgressMeter(\n",
    "            len(train_loader),\n",
    "            [batch_time, data_time, losses, top1, top5],\n",
    "            prefix=\"Epoch: [{}]\".format(epoch))\n",
    "        \n",
    "        trn_loss = []\n",
    "        model.train()\n",
    "        end = time.time()\n",
    "        running_loss = 0.0\n",
    "        ## Set epoch count for DistributedSampler\n",
    "        if args.multigpus_distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        \n",
    "        \n",
    "        prefetcher = util.data_prefetcher(train_loader)\n",
    "        input, target = prefetcher.next()\n",
    "        batch_idx = 0\n",
    "        while input is not None:\n",
    "\n",
    "            batch_idx += 1\n",
    "            \n",
    "            if args.prof >= 0 and batch_idx == args.prof:\n",
    "                print(\"Profiling begun at iteration {}\".format(batch_idx))\n",
    "                torch.cuda.cudart().cudaProfilerStart()\n",
    "                \n",
    "            if args.prof >= 0: torch.cuda.nvtx.range_push(\"Body of iteration {}\".format(batch_idx))\n",
    "\n",
    "            util.adjust_learning_rate(optimizer, epoch, batch_idx, len(train_loader), args)\n",
    "            \n",
    "            ##### DATA Processing #####\n",
    "            targets_gra = target[:, 0]\n",
    "            targets_vow = target[:, 1]\n",
    "            targets_con = target[:, 2]\n",
    "\n",
    "            # 50%의 확률로 원본 데이터 그대로 사용    \n",
    "            if np.random.rand() < 0.5:\n",
    "                logits = model(input)\n",
    "                grapheme = logits[:, :168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra)\n",
    "                loss2 = loss_fn(vowel, targets_vow)\n",
    "                loss3 = loss_fn(cons, targets_con) \n",
    "\n",
    "            else:\n",
    "\n",
    "                lam = np.random.beta(1.0, 1.0) \n",
    "                rand_index = torch.randperm(input.size()[0])\n",
    "                shuffled_targets_gra = targets_gra[rand_index]\n",
    "                shuffled_targets_vow = targets_vow[rand_index]\n",
    "                shuffled_targets_con = targets_con[rand_index]\n",
    "\n",
    "                bbx1, bby1, bbx2, bby2 = _rand_bbox(input.size(), lam)\n",
    "                input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # 픽셀 비율과 정확히 일치하도록 lambda 파라메터 조정  \n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[-1] * input.size()[-2]))\n",
    "                \n",
    "                logits = model(input)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "                loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "                loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "\n",
    "            loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3    \n",
    "            trn_loss.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #########################################################\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if args.apex:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            # Printing vital information\n",
    "            if (batch_idx + 1) % (args.log_interval) == 0:\n",
    "                s = f'[Epoch {epoch} Batch {batch_idx+1}/{len(train_loader)}] ' \\\n",
    "                f'loss: {running_loss / args.log_interval:.4f}'\n",
    "                print(s)\n",
    "                running_loss = 0\n",
    "                \n",
    "                \n",
    "            if True or batch_idx % args.log_interval == 0:\n",
    "                # Every log_interval iterations, check the loss, accuracy, and speed.\n",
    "                # For best performance, it doesn't make sense to print these metrics every\n",
    "                # iteration, since they incur an allreduce and some host<->device syncs.\n",
    "\n",
    "                # Measure accuracy\n",
    "                prec1, prec5 = util.accuracy(logits, target, topk=(1, 5))\n",
    "\n",
    "                # Average loss and accuracy across processes for logging\n",
    "                if args.multigpus_distributed:\n",
    "                    reduced_loss = dis_util.reduce_tensor(loss.data, args)\n",
    "                    prec1 = dis_util.reduce_tensor(prec1, args)\n",
    "                    prec5 = dis_util.reduce_tensor(prec5, args)\n",
    "                else:\n",
    "                    reduced_loss = loss.data\n",
    "                \n",
    "                # to_python_float incurs a host<->device sync\n",
    "                losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "                top1.update(to_python_float(prec1), input.size(0))\n",
    "                top5.update(to_python_float(prec5), input.size(0))\n",
    "                \n",
    "                ## Waiting until finishing operations on GPU (Pytorch default: async)\n",
    "                torch.cuda.synchronize()\n",
    "                batch_time.update((time.time() - end)/args.log_interval)\n",
    "                end = time.time()\n",
    "\n",
    "                if current_gpu == 0:\n",
    "                    print('Epoch: [{0}][{1}/{2}]  '\n",
    "                          'Time {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "                          'Speed {3:.3f} ({4:.3f})  '\n",
    "                          'Loss {loss.val:.10f} ({loss.avg:.4f})  '\n",
    "                          'Prec@1 {top1.val:.3f} ({top1.avg:.3f})  '\n",
    "                          'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                              epoch, batch_idx, len(train_loader),\n",
    "                              args.world_size*args.batch_size/batch_time.val,\n",
    "                              args.world_size*args.batch_size/batch_time.avg,\n",
    "                              batch_time=batch_time,\n",
    "                              loss=losses, top1=top1, top5=top5))\n",
    "                    model_history['epoch'].append(epoch)\n",
    "                    model_history['batch_idx'].append(batch_idx)\n",
    "                    model_history['batch_time'].append(batch_time.val)\n",
    "                    model_history['losses'].append(losses.val)\n",
    "                    model_history['top1'].append(top1.val)\n",
    "                    model_history['top5'].append(top5.val)\n",
    "                    \n",
    "\n",
    "            input, target = prefetcher.next()\n",
    "            \n",
    "        acc1 = validate(test_loader, model, loss_fn, epoch, model_history, trn_loss, args)            \n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "\n",
    "        if not args.multigpus_distributed or (args.multigpus_distributed and args.rank % args.num_gpus == 0):\n",
    "            util.save_history(os.path.join(args.output_data_dir,\n",
    "                          'model_history.p'), model_history)\n",
    "\n",
    "            util.save_model({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_name': args.model_name,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc1': best_acc1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "#                 'class_to_idx' : train_loader.dataset.class_to_idx,\n",
    "            }, is_best, args.model_dir)\n",
    "\n",
    "\n",
    "def validate(val_loader, model, loss_fn, epoch, model_history, trn_loss, args):\n",
    "    batch_time = util.AverageMeter('Time', ':6.3f')\n",
    "    losses = util.AverageMeter('Loss', ':.4e')\n",
    "    top1 = util.AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = util.AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = util.ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "    val_loss = []\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    prefetcher = util.data_prefetcher(val_loader)\n",
    "    input, target = prefetcher.next()\n",
    "    batch_idx = 0\n",
    "    while input is not None:\n",
    "        batch_idx += 1\n",
    "    \n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "                logits = model(input)\n",
    "                grapheme = logits[:,:168]\n",
    "                vowel = logits[:, 168:179]\n",
    "                cons = logits[:, 179:]\n",
    "\n",
    "                loss= 0.5* loss_fn(grapheme, target[:,0]) + 0.25*loss_fn(vowel, target[:,1]) + \\\n",
    "                0.25*loss_fn(vowel, target[:,2])\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                grapheme = grapheme.cpu().argmax(dim=1).data.numpy()\n",
    "                vowel = vowel.cpu().argmax(dim=1).data.numpy()\n",
    "                cons = cons.cpu().argmax(dim=1).data.numpy()\n",
    "\n",
    "                val_true.append(target.cpu().numpy())\n",
    "                val_pred.append(np.stack([grapheme, vowel, cons], axis=1))                \n",
    "  \n",
    "                \n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = util.accuracy(logits, target, topk=(1, 5))\n",
    "\n",
    "        if args.multigpus_distributed:\n",
    "            reduced_loss = dis_util.reduce_tensor(loss.data, args)\n",
    "            prec1 = dis_util.reduce_tensor(prec1, args)\n",
    "            prec5 = dis_util.reduce_tensor(prec5, args)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # TODO:  Change timings to mirror train().\n",
    "        if args.current_gpu == 0 and batch_idx % args.log_interval == 0:\n",
    "            print('Test: [{0}/{1}]  '\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "                  'Speed {2:.3f} ({3:.3f})  '\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})  '\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})  '\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      batch_idx, len(val_loader),\n",
    "                      args.world_size * args.batch_size / batch_time.val,\n",
    "                      args.world_size * args.batch_size / batch_time.avg,\n",
    "                      batch_time=batch_time, loss=losses,\n",
    "                      top1=top1, top5=top5))\n",
    "            model_history['val_epoch'].append(epoch)\n",
    "            model_history['val_batch_idx'].append(batch_idx)\n",
    "            model_history['val_batch_time'].append(batch_time.val)\n",
    "            model_history['val_losses'].append(losses.val)\n",
    "            model_history['val_top1'].append(top1.val)\n",
    "            model_history['val_top5'].append(top5.val)\n",
    "        input, target = prefetcher.next()\n",
    "\n",
    "\n",
    "        \n",
    "    val_true_concat = np.concatenate(val_true)\n",
    "    val_pred_concat = np.concatenate(val_pred)\n",
    "    val_loss_mean = np.mean(val_loss)\n",
    "    trn_loss_mean = np.mean(trn_loss)\n",
    "\n",
    "    score_g = recall_score(val_true_concat[:,0], val_pred_concat[:,0], average='macro')\n",
    "    score_v = recall_score(val_true_concat[:,1], val_pred_concat[:,1], average='macro')\n",
    "    score_c = recall_score(val_true_concat[:,2], val_pred_concat[:,2], average='macro')\n",
    "    final_score = np.average([score_g, score_v, score_c], weights=[2,1,1])\n",
    "\n",
    "    if args.current_gpu == 0:\n",
    "        # Printing vital information\n",
    "        s = f'[Epoch {epoch}] ' \\\n",
    "        f'trn_loss: {trn_loss_mean:.4f}, vld_loss: {val_loss_mean:.4f}, score: {final_score:.4f}, ' \\\n",
    "        f'score_each: [{score_g:.4f}, {score_v:.4f}, {score_c:.4f}]'          \n",
    "        print(s)  \n",
    "        \n",
    "        \n",
    "    print('  Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "    model_history['val_avg_epoch'].append(epoch)\n",
    "    model_history['val_avg_batch_time'].append(batch_time.avg)\n",
    "    model_history['val_avg_losses'].append(losses.avg)\n",
    "    model_history['val_avg_top1'].append(top1.avg)\n",
    "    model_history['val_avg_top5'].append(top5.avg)\n",
    "    return top1.avg\n",
    "\n",
    "## iter() overflowerror: cannot serialize a bytes object larger than 4 gib --> num_worker=0 resolved\n",
    "def main():\n",
    "    args = parser_args()\n",
    "    args.use_cuda = args.num_gpus > 0\n",
    "    print(\"args.use_cuda : {} , args.num_gpus : {}\".format(\n",
    "        args.use_cuda, args.num_gpus))\n",
    "    args.kwargs = {'num_workers': 0,\n",
    "                   'pin_memory': True} if args.use_cuda else {}\n",
    "    args.device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "    dis_util.dist_init(train, args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/dis_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/dis_util.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import sagemaker_containers\n",
    "\n",
    "try:\n",
    "    from apex.parallel import DistributedDataParallel as DDP\n",
    "    from apex.fp16_utils import *\n",
    "    from apex import amp, optimizers\n",
    "    from apex.multi_tensor_apply import multi_tensor_applier\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def dist_init(fn, args):\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    args.is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "    args.is_multigpus = args.num_gpus > 1\n",
    "    args.multigpus_distributed = (args.is_distributed or args.is_multigpus)\n",
    "\n",
    "    logger.debug(\"Distributed training - {}\".format(args.is_distributed))\n",
    "    logger.debug(\"Number of gpus available - {}\".format(args.num_gpus))\n",
    "\n",
    "    args.world_size = 1\n",
    "    if args.multigpus_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        args.apex = True\n",
    "        args.world_size = len(args.hosts) * args.num_gpus\n",
    "        os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "        args.host_num = args.hosts.index(args.current_host)\n",
    "        mp.spawn(fn, nprocs=args.num_gpus, args=(args,))\n",
    "    else:\n",
    "        current_gpu = 0\n",
    "        fn(current_gpu, args)\n",
    "\n",
    "\n",
    "def dist_setting(current_gpu, model, loss_fn, args):\n",
    "    print(\"channels_last : {}\".format(args.channels_last))\n",
    "    if args.channels_last:\n",
    "        args.memory_format = torch.channels_last\n",
    "    else:\n",
    "        args.memory_format = torch.contiguous_format\n",
    "\n",
    "    if args.apex:\n",
    "        args.lr = args.lr*float(args.batch_size*args.world_size)/256.\n",
    "    args.current_gpu = current_gpu\n",
    "    if args.current_gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.current_gpu))\n",
    "\n",
    "    if args.multigpus_distributed:\n",
    "        args.rank = args.num_gpus * args.host_num + args.current_gpu\n",
    "        dist.init_process_group(backend=args.backend,\n",
    "                                rank=args.rank, world_size=args.world_size)\n",
    "        logger.info('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\n",
    "            args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n",
    "            dist.get_rank(), args.num_gpus))\n",
    "\n",
    "    if args.sync_bn:\n",
    "        import apex\n",
    "        print(\"using apex synced BN\")\n",
    "        model = apex.parallel.convert_syncbn_model(model)\n",
    "\n",
    "    if args.multigpus_distributed:\n",
    "        if args.current_gpu is not None:\n",
    "            torch.cuda.set_device(args.current_gpu)\n",
    "            args.batch_size = int(args.batch_size / args.num_gpus)\n",
    "            if not args.apex:\n",
    "                model.cuda(args.current_gpu)\n",
    "                model = torch.nn.parallel.DistributedDataParallel(\n",
    "                    model, device_ids=[args.current_gpu])\n",
    "        else:\n",
    "            if not args.apex:\n",
    "                model.cuda()\n",
    "                model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif args.current_gpu is not None:\n",
    "        torch.cuda.set_device(args.current_gpu)\n",
    "        if not args.apex:\n",
    "            model = model.cuda(args.current_gpu)\n",
    "    else:\n",
    "        if not args.apex:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    return model, args\n",
    "\n",
    "\n",
    "def apex_init(model, optimizer, args):\n",
    "    model = model.cuda().to(memory_format=args.memory_format)\n",
    "    model, optimizer = amp.initialize(model, optimizer,\n",
    "                                      opt_level=args.opt_level,\n",
    "                                      keep_batchnorm_fp32=args.keep_batchnorm_fp32,\n",
    "                                      loss_scale=args.loss_scale\n",
    "                                      )\n",
    "    if args.multigpus_distributed:\n",
    "        model = DDP(model, delay_allreduce=True)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, args):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= args.world_size\n",
    "    return rt\n",
    "\n",
    "\n",
    "def fast_collate(batch, memory_format=torch.channels_last):\n",
    "    imgs = [img[0] for img in batch]\n",
    "    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n",
    "    w = imgs[0].shape[0]\n",
    "    h = imgs[0].shape[1]\n",
    "    tensor = torch.zeros((len(imgs), 3, w, h), dtype=torch.uint8).contiguous(\n",
    "        memory_format=memory_format)\n",
    "    for i, img in enumerate(imgs):\n",
    "        nump_array = np.array(img, dtype=np.uint8)\n",
    "        if(nump_array.ndim < 3):\n",
    "            nump_array = np.expand_dims(nump_array, axis=-1)\n",
    "        nump_array = np.rollaxis(nump_array, 2)\n",
    "        tensor[i] += torch.from_numpy(nump_array)\n",
    "    return tensor, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/util.py\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "from torchvision import models\n",
    "\n",
    "import sagemaker_containers\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def torch_model(model_name, pretrained=True):\n",
    "    model_names = sorted(name for name in models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(models.__dict__[name]))\n",
    "\n",
    "    if(model_name == \"inception_v3\"):\n",
    "        raise RuntimeError(\n",
    "            \"Currently, inception_v3 is not supported by this example.\")\n",
    "\n",
    "    # create model\n",
    "    if pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(model_name))\n",
    "        model = models.__dict__[model_name](pretrained=True)\n",
    "    else:\n",
    "        print(\"=> creating model '{}'\".format(model_name))\n",
    "        model = models.__dict__[model_name]()\n",
    "    return model\n",
    "\n",
    "\n",
    "def accuracy(logits, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "#         print(\"batch_size : {}\".format(batch_size))\n",
    "\n",
    "        _, pred1 = logits[:, :168].topk(maxk, 1, True, True)\n",
    "        _, pred2 = logits[:, 168:179].topk(maxk, 1, True, True)\n",
    "        _, pred3 = logits[:, 179:].topk(maxk, 1, True, True)\n",
    "\n",
    "#         print(\"pred1 : {}, pred2 : {}, pred3 : {}\".format(pred1 ,pred2, pred3))\n",
    "        pred1 = pred1.t()\n",
    "        pred2 = pred2.t()\n",
    "        pred3 = pred3.t()\n",
    "        correct1 = pred1.eq(target[:,0].view(1, -1).expand_as(pred1))\n",
    "        correct2 = pred2.eq(target[:,1].view(1, -1).expand_as(pred2))\n",
    "        correct3 = pred3.eq(target[:,2].view(1, -1).expand_as(pred3))\n",
    "#         print(\"correct1 : {}, correct2 : {}, correct3 : {}\".format(correct1 ,correct2, correct3))\n",
    "        \n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k1 = correct1[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            correct_k2 = correct2[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            correct_k3 = correct3[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            correct_k = 0.5 * correct_k1 + 0.25 * correct_k2 + 0.25 * correct_k3\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res    \n",
    "    \n",
    "\n",
    "def save_model(state, is_best, model_dir):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    filename = os.path.join(model_dir, 'checkpoint.pth')\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(model_dir, 'model_best.pth'))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "        \n",
    "def adjust_learning_rate(optimizer, epoch, step, len_epoch, args):\n",
    "    \"\"\"LR schedule that should yield 76% converged accuracy with batch size 256\"\"\"\n",
    "    factor = epoch // 30\n",
    "\n",
    "    if epoch >= 80:\n",
    "        factor = factor + 1\n",
    "\n",
    "    lr = args.lr*(0.1**factor)\n",
    "\n",
    "    \"\"\"Warmup\"\"\"\n",
    "    if epoch < 5:\n",
    "        lr = lr*float(1 + step + epoch*len_epoch)/(5.*len_epoch)\n",
    "\n",
    "    if(args.current_gpu == 0):\n",
    "        print(\"epoch = {}, step = {}, lr = {}\".format(epoch, step, lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "        \n",
    "class data_prefetcher():\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.mean = torch.tensor([0.5 * 255, 0.5 * 255, 0.5 * 255]).cuda().view(1,3,1,1)\n",
    "        self.std = torch.tensor([0.5 * 255, 0.5 * 255, 0.5 * 255]).cuda().view(1,3,1,1)\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(non_blocking=True)\n",
    "            self.next_target = self.next_target.cuda(non_blocking=True)\n",
    "            self.next_input = self.next_input.float()\n",
    "            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        target = self.next_target\n",
    "        if input is not None:\n",
    "            input.record_stream(torch.cuda.current_stream())\n",
    "        if target is not None:\n",
    "            target.record_stream(torch.cuda.current_stream())\n",
    "        self.preload()\n",
    "        return input, target\n",
    "    \n",
    "\n",
    "\n",
    "def save_history(path, history):\n",
    "\n",
    "    history_for_json = {}\n",
    "    # transform float values that aren't json-serializable\n",
    "    for key in history.keys():\n",
    "        history_for_json[key] = list(map(float, history[key]))\n",
    "\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(history_for_json, f, separators=(\n",
    "            ',', ':'), sort_keys=True, indent=4)\n",
    "        \n",
    "        \n",
    "\n",
    "def init_modelhistory(model_history):\n",
    "    model_history['epoch'] = []\n",
    "    model_history['batch_idx'] = []\n",
    "    model_history['batch_time'] = []\n",
    "    model_history['losses'] = []\n",
    "    model_history['top1'] = []\n",
    "    model_history['top5'] = []\n",
    "    model_history['val_epoch'] = []\n",
    "    model_history['val_batch_idx'] = []\n",
    "    model_history['val_batch_time'] = []\n",
    "    model_history['val_losses'] = []\n",
    "    model_history['val_top1'] = []\n",
    "    model_history['val_top5'] = []\n",
    "    model_history['val_avg_epoch'] = []\n",
    "    model_history['val_avg_batch_time'] = []\n",
    "    model_history['val_avg_losses'] = []\n",
    "    model_history['val_avg_top1'] = []\n",
    "    model_history['val_avg_top5'] = []\n",
    "    return model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]\n",
    "\n",
    "\n",
    "# [\n",
    "#         {'Name':'train:loss', 'Regex':'Train Loss: (.*?);'},\n",
    "#         {'Name':'test:loss', 'Regex':'Test Average loss: (.*?),'},\n",
    "#         {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        'height' : 137,\n",
    "        'width' : 236,\n",
    "        'num_epochs': 50,\n",
    "        'batch-size' : 192*8,   ## 192 for 8xlarge*2, 100 for 16xlarge*2, 64 for 16xlarge*3\n",
    "        'backend': 'nccl',\n",
    "        'lr': 0.001,\n",
    "        'apex' : True,\n",
    "        'opt-level' : 'O0'\n",
    "    }\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.p3.16xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all input configurations, parameters, and metrics specified in estimator \n",
    "# definition are automatically tracked\n",
    "estimator = PyTorch(\n",
    "    entry_point='./main_trainer.py',\n",
    "    source_dir='./src',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n",
    "    framework_version='1.5.0',\n",
    "    train_instance_count=train_instance_count,\n",
    "    train_instance_type=train_instance_type,\n",
    "    train_volume_size=400,\n",
    "    hyperparameters=hyperparameters,\n",
    "#     train_use_spot_instances=True,  # spot instance 활용\n",
    "#     train_max_run=12*60*60,\n",
    "#     train_max_wait=12*60*60,\n",
    "#     checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "#     tensorboard_output_config=TensorBoardOutputConfig(tensorboard_output),\n",
    "    metric_definitions=metrics_definitions,\n",
    "    enable_sagemaker_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "training_job_name = \"training-job-{}\".format(int(time.time()))\n",
    "\n",
    "# Now associate the estimator with the Experiment and Trial\n",
    "estimator.fit(\n",
    "    inputs=s3_data_path, \n",
    "    job_name=training_job_name,\n",
    "    logs='All',\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-08 07:34:12 Starting - Starting the training job...\n",
      "2020-08-08 07:34:14 Starting - Launching requested ML instances.........\n",
      "2020-08-08 07:35:49 Starting - Preparing the instances for training......\n",
      "2020-08-08 07:37:05 Downloading - Downloading input data...\n",
      "2020-08-08 07:37:42 Training - Downloading the training image......\n",
      "2020-08-08 07:38:43 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-08-08 07:38:44,547 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-08-08 07:38:44,624 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-08-08 07:38:44,628 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-08-08 07:38:44,902 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-08-08 07:38:44,903 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-08-08 07:38:44,903 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-08-08 07:38:44,903 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpkdt26380/module_dir\u001b[0m\n",
      "\u001b[34mCollecting albumentations\n",
      "  Downloading albumentations-0.4.6.tar.gz (117 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow\n",
      "  Downloading pyarrow-1.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (1.2.2)\u001b[0m\n",
      "\u001b[34mCollecting imgaug>=0.4.0\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from albumentations->-r requirements.txt (line 1)) (5.3.1)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.3.0.36-cp36-cp36m-manylinux2014_x86_64.whl (36.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-image>=0.14.2\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting imageio\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting Shapely\n",
      "  Downloading Shapely-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (7.1.0)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python\n",
      "  Downloading opencv_python-4.3.0.36-cp36-cp36m-manylinux2014_x86_64.whl (43.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[34mCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.7.24-py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4)\u001b[0m\n",
      "\u001b[34mCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations->-r requirements.txt (line 1)) (4.4.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: albumentations, default-user-module-name\n",
      "  Building wheel for albumentations (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for albumentations (setup.py): finished with status 'done'\n",
      "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65167 sha256=cf51b1faf00e5652a93bdf1bbeb9ce9cee0f12f5a6a8e0f0be8172ea2298ed9b\n",
      "  Stored in directory: /root/.cache/pip/wheels/38/db/df/d6cb0be184075a7799c1fd79240c389c16f51dfe18dc3332fa\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=34622 sha256=fce1ecf52b38254c3793d6aee758013c9dbe68b92205c8442be7844cba058228\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o39ccswh/wheels/21/09/91/2f31d2385f524e93d29275186afb655802dcb662eef1debb7e\u001b[0m\n",
      "\u001b[34mSuccessfully built albumentations default-user-module-name\u001b[0m\n",
      "\u001b[34mERROR: scikit-image 0.17.2 has requirement pillow!=7.1.0,!=7.1.1,>=4.3.0, but you'll have pillow 7.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tifffile, PyWavelets, imageio, scikit-image, Shapely, opencv-python, imgaug, opencv-python-headless, albumentations, pyarrow, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed PyWavelets-1.1.1 Shapely-1.7.0 albumentations-0.4.6 default-user-module-name-1.0.0 imageio-2.9.0 imgaug-0.4.0 opencv-python-4.3.0.36 opencv-python-headless-4.3.0.36 pyarrow-1.0.0 scikit-image-0.17.2 tifffile-2020.7.24\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1; however, version 20.2.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-08-08 07:39:00,499 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"opt-level\": \"O0\",\n",
      "        \"lr\": 0.001,\n",
      "        \"apex\": true,\n",
      "        \"batch-size\": 1536,\n",
      "        \"width\": 236,\n",
      "        \"backend\": \"nccl\",\n",
      "        \"num_epochs\": 50,\n",
      "        \"height\": 137\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"training-job-1596872052\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-322537213286/training-job-1596872052/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"./main_trainer\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"./main_trainer.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"apex\":true,\"backend\":\"nccl\",\"batch-size\":1536,\"height\":137,\"lr\":0.001,\"num_epochs\":50,\"opt-level\":\"O0\",\"width\":236}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=./main_trainer.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=./main_trainer\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-322537213286/training-job-1596872052/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"apex\":true,\"backend\":\"nccl\",\"batch-size\":1536,\"height\":137,\"lr\":0.001,\"num_epochs\":50,\"opt-level\":\"O0\",\"width\":236},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"training-job-1596872052\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-322537213286/training-job-1596872052/source/sourcedir.tar.gz\",\"module_name\":\"./main_trainer\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"./main_trainer.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--apex\",\"True\",\"--backend\",\"nccl\",\"--batch-size\",\"1536\",\"--height\",\"137\",\"--lr\",\"0.001\",\"--num_epochs\",\"50\",\"--opt-level\",\"O0\",\"--width\",\"236\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_OPT-LEVEL=O0\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_APEX=true\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=1536\u001b[0m\n",
      "\u001b[34mSM_HP_WIDTH=236\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=nccl\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_EPOCHS=50\u001b[0m\n",
      "\u001b[34mSM_HP_HEIGHT=137\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python ./main_trainer.py --apex True --backend nccl --batch-size 1536 --height 137 --lr 0.001 --num_epochs 50 --opt-level O0 --width 236\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34margs.use_cuda : True , args.num_gpus : 8\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 8\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n",
      "\u001b[34m=> using pre-trained model 'resnet50'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 6 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 7 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 2 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 1 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 3 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 4 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 0 for training\u001b[0m\n",
      "\u001b[34mchannels_last : True\u001b[0m\n",
      "\u001b[34mUse GPU: 5 for training\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 8 nodes. Current host rank is 5. Number of gpus: 8\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 8 nodes. Current host rank is 6. Number of gpus: 8\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 8 nodes. Current host rank is 7. Number of gpus: 8\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 8 nodes. Current host rank is 2. Number of gpus: 8\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 8 nodes. Current host rank is 1. Number of gpus: 8\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 8 nodes. Current host rank is 3. Number of gpus: 8\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 8 nodes. Current host rank is 4. Number of gpus: 8\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'nccl' backend on 8 nodes. Current host rank is 0. Number of gpus: 8\u001b[0m\n",
      "\u001b[34mSelected optimization level O0:  Pure FP32 training.\n",
      "\u001b[0m\n",
      "\u001b[34mDefaults for this optimization level are:\u001b[0m\n",
      "\u001b[34menabled                : True\u001b[0m\n",
      "\u001b[34mopt_level              : O0\u001b[0m\n",
      "\u001b[34mcast_model_type        : torch.float32\u001b[0m\n",
      "\u001b[34mpatch_torch_functions  : False\u001b[0m\n",
      "\u001b[34mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[34mmaster_weights         : False\u001b[0m\n",
      "\u001b[34mloss_scale             : 1.0\u001b[0m\n",
      "\u001b[34mProcessing user overrides (additional kwargs that are not None)...\u001b[0m\n",
      "\u001b[34mAfter processing overrides, optimization options are:\u001b[0m\n",
      "\u001b[34menabled                : True\u001b[0m\n",
      "\u001b[34mopt_level              : O0\u001b[0m\n",
      "\u001b[34mcast_model_type        : torch.float32\u001b[0m\n",
      "\u001b[34mpatch_torch_functions  : False\u001b[0m\n",
      "\u001b[34mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[34mmaster_weights         : False\u001b[0m\n",
      "\u001b[34mloss_scale             : 1.0\u001b[0m\n",
      "\u001b[34mNCCL version 2.4.8+cuda10.1\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Labels ===\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34m=== Getting Images ===\u001b[0m\n",
      "\u001b[34m['/opt/ml/input/data/training/train_image_data_2.parquet', '/opt/ml/input/data/training/train_image_data_0.parquet', '/opt/ml/input/data/training/train_image_data_1.parquet', '/opt/ml/input/data/training/train_image_data_3.parquet']\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20084/160672 (12%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5021/40168 (12%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20084/160672 (12%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5021/40168 (12%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20084/160672 (12%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5021/40168 (12%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20084/160672 (12%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5021/40168 (12%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20084/160672 (12%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5021/40168 (12%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20084/160672 (12%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5021/40168 (12%) of test data\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20084/160672 (12%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5021/40168 (12%) of test data\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.006 algo-1:249 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.007 algo-1:249 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.007 algo-1:249 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.012 algo-1:249 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.597 algo-1:247 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.597 algo-1:247 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.597 algo-1:247 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.600 algo-1:247 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.665 algo-1:245 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.665 algo-1:245 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.665 algo-1:245 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.668 algo-1:245 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.726 algo-1:244 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.726 algo-1:244 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.726 algo-1:244 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.730 algo-1:244 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.794 algo-1:248 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.794 algo-1:248 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.794 algo-1:248 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:05.798 algo-1:248 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 20084/160672 (12%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 5021/40168 (12%) of test data\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:06.692 algo-1:243 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:06.692 algo-1:243 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:06.692 algo-1:243 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:06.696 algo-1:243 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 1, lr = 0.009782857142857143\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:08.166 algo-1:242 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:08.166 algo-1:242 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:08.166 algo-1:242 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:08.169 algo-1:242 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:09.671 algo-1:246 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:09.671 algo-1:246 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:09.671 algo-1:246 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-08-08 07:45:09.674 algo-1:246 INFO hook.py:364] Monitoring the collections: losses\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch: [1][1/105]  Time 1.142 (1.142)  Speed 1345.132 (1345.132)  Loss 6.0509176254 (6.0509)  Prec@1 2.311 (2.311)  Prec@5 13.070 (13.070)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 2, lr = 0.009874285714285714\u001b[0m\n",
      "\u001b[34mEpoch: [1][2/105]  Time 0.993 (1.068)  Speed 1546.462 (1438.788)  Loss 5.2088403702 (5.6299)  Prec@1 17.074 (9.692)  Prec@5 45.622 (29.346)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 3, lr = 0.009965714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][3/105]  Time 0.945 (1.027)  Speed 1625.994 (1496.209)  Loss 4.8501343727 (5.3700)  Prec@1 21.305 (13.563)  Prec@5 48.096 (35.596)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 4, lr = 0.010057142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][4/105]  Time 1.088 (1.042)  Speed 1411.977 (1474.223)  Loss 4.6030130386 (5.1782)  Prec@1 22.201 (15.723)  Prec@5 50.016 (39.201)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 5, lr = 0.010148571428571429\u001b[0m\n",
      "\u001b[34mEpoch: [1][5/105]  Time 0.627 (0.959)  Speed 2451.105 (1601.910)  Loss 4.1541042328 (4.9734)  Prec@1 22.591 (17.096)  Prec@5 49.854 (41.331)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 6, lr = 0.01024\u001b[0m\n",
      "\u001b[34mEpoch: [1][6/105]  Time 1.066 (0.977)  Speed 1440.940 (1572.630)  Loss 3.8463180065 (4.7856)  Prec@1 21.338 (17.803)  Prec@5 51.188 (42.974)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 7, lr = 0.010331428571428573\u001b[0m\n",
      "\u001b[34mEpoch: [1][7/105]  Time 1.112 (0.996)  Speed 1381.894 (1542.221)  Loss 4.0787339211 (4.6846)  Prec@1 19.189 (18.001)  Prec@5 50.602 (44.064)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 8, lr = 0.010422857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][8/105]  Time 0.899 (0.984)  Speed 1709.058 (1561.272)  Loss 3.6622228622 (4.5568)  Prec@1 22.542 (18.569)  Prec@5 50.635 (44.885)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 9, lr = 0.010514285714285716\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/105] loss: 3.9834\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/105] loss: 3.9698\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/105] loss: 4.0978\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/105] loss: 4.0218\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/105] loss: 3.9927\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/105] loss: 4.0772\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/105] loss: 4.0020\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 10/105] loss: 3.9107\u001b[0m\n",
      "\u001b[34mEpoch: [1][9/105]  Time 1.077 (0.994)  Speed 1426.294 (1545.026)  Loss 3.6149146557 (4.4521)  Prec@1 21.273 (18.869)  Prec@5 49.788 (45.430)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 10, lr = 0.010605714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][10/105]  Time 1.037 (0.998)  Speed 1481.496 (1538.429)  Loss 3.5131959915 (4.3582)  Prec@1 21.924 (19.175)  Prec@5 49.382 (45.825)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 11, lr = 0.010697142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][11/105]  Time 1.101 (1.008)  Speed 1394.863 (1524.168)  Loss 3.4566185474 (4.2763)  Prec@1 21.273 (19.366)  Prec@5 48.958 (46.110)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 12, lr = 0.010788571428571429\u001b[0m\n",
      "\u001b[34mEpoch: [1][12/105]  Time 0.608 (0.974)  Speed 2525.545 (1576.249)  Loss 3.5826458931 (4.2185)  Prec@1 21.175 (19.516)  Prec@5 49.154 (46.364)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 13, lr = 0.010879999999999999\u001b[0m\n",
      "\u001b[34mEpoch: [1][13/105]  Time 1.017 (0.978)  Speed 1509.701 (1570.923)  Loss 3.5825796127 (4.1696)  Prec@1 22.038 (19.710)  Prec@5 48.193 (46.504)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 14, lr = 0.010971428571428571\u001b[0m\n",
      "\u001b[34mEpoch: [1][14/105]  Time 0.579 (0.949)  Speed 2651.236 (1618.016)  Loss 3.5593454838 (4.1260)  Prec@1 21.598 (19.845)  Prec@5 47.445 (46.572)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 15, lr = 0.011062857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][15/105]  Time 0.983 (0.952)  Speed 1562.302 (1614.178)  Loss 3.4620513916 (4.0817)  Prec@1 21.712 (19.970)  Prec@5 46.403 (46.560)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 16, lr = 0.011154285714285714\u001b[0m\n",
      "\u001b[34mEpoch: [1][16/105]  Time 1.079 (0.960)  Speed 1423.817 (1600.802)  Loss 3.5139665604 (4.0462)  Prec@1 21.680 (20.076)  Prec@5 46.826 (46.577)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 17, lr = 0.011245714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][17/105]  Time 1.007 (0.962)  Speed 1526.026 (1596.201)  Loss 3.4431805611 (4.0108)  Prec@1 21.989 (20.189)  Prec@5 49.040 (46.722)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 18, lr = 0.011337142857142856\u001b[0m\n",
      "\u001b[34mEpoch: [1][18/105]  Time 1.107 (0.970)  Speed 1387.372 (1582.964)  Loss 3.3501751423 (3.9741)  Prec@1 22.152 (20.298)  Prec@5 49.040 (46.851)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 19, lr = 0.011428571428571429\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/105] loss: 3.5031\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/105] loss: 3.4486\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/105] loss: 3.4473\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/105] loss: 3.4364\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/105] loss: 3.4951\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/105] loss: 3.5186\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/105] loss: 3.4723\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 20/105] loss: 3.5190\u001b[0m\n",
      "\u001b[34mEpoch: [1][19/105]  Time 0.600 (0.951)  Speed 2558.924 (1615.390)  Loss 3.3368411064 (3.9405)  Prec@1 21.517 (20.362)  Prec@5 49.544 (46.992)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 20, lr = 0.01152\u001b[0m\n",
      "\u001b[34mEpoch: [1][20/105]  Time 0.927 (0.950)  Speed 1656.932 (1617.418)  Loss 3.2865083218 (3.9078)  Prec@1 21.257 (20.407)  Prec@5 49.658 (47.126)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 21, lr = 0.011611428571428571\u001b[0m\n",
      "\u001b[34mEpoch: [1][21/105]  Time 1.075 (0.956)  Speed 1428.986 (1607.325)  Loss 3.2267386913 (3.8754)  Prec@1 21.598 (20.464)  Prec@5 51.286 (47.324)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 22, lr = 0.011702857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][22/105]  Time 1.092 (0.962)  Speed 1406.743 (1596.975)  Loss 3.2511167526 (3.8470)  Prec@1 21.582 (20.514)  Prec@5 50.423 (47.465)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 23, lr = 0.011794285714285714\u001b[0m\n",
      "\u001b[34mEpoch: [1][23/105]  Time 1.005 (0.964)  Speed 1528.629 (1593.876)  Loss 3.2492783070 (3.8210)  Prec@1 21.289 (20.548)  Prec@5 49.984 (47.574)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 24, lr = 0.011885714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][24/105]  Time 0.581 (0.948)  Speed 2643.019 (1620.681)  Loss 3.1942806244 (3.7949)  Prec@1 21.810 (20.601)  Prec@5 51.042 (47.719)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 25, lr = 0.011977142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][25/105]  Time 1.115 (0.954)  Speed 1377.739 (1609.330)  Loss 3.2102909088 (3.7715)  Prec@1 22.249 (20.667)  Prec@5 50.716 (47.839)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 26, lr = 0.012068571428571429\u001b[0m\n",
      "\u001b[34mEpoch: [1][26/105]  Time 1.005 (0.956)  Speed 1527.701 (1606.030)  Loss 3.2163176537 (3.7502)  Prec@1 22.835 (20.750)  Prec@5 50.895 (47.956)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 27, lr = 0.01216\u001b[0m\n",
      "\u001b[34mEpoch: [1][27/105]  Time 1.119 (0.962)  Speed 1373.186 (1596.007)  Loss 3.2066085339 (3.7300)  Prec@1 22.152 (20.802)  Prec@5 50.195 (48.039)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 28, lr = 0.012251428571428573\u001b[0m\n",
      "\u001b[34mEpoch: [1][28/105]  Time 0.601 (0.950)  Speed 2555.011 (1617.692)  Loss 3.2081100941 (3.7114)  Prec@1 21.761 (20.836)  Prec@5 50.358 (48.122)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 29, lr = 0.012342857142857143\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/105] loss: 3.2003\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/105] loss: 3.2218\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/105] loss: 3.2161\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/105] loss: 3.2261\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/105] loss: 3.2315\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/105] loss: 3.2257\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/105] loss: 3.2485\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 30/105] loss: 3.2250\u001b[0m\n",
      "\u001b[34mEpoch: [1][29/105]  Time 0.938 (0.949)  Speed 1637.608 (1618.371)  Loss 3.1945269108 (3.6936)  Prec@1 22.640 (20.898)  Prec@5 50.391 (48.200)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 30, lr = 0.012434285714285715\u001b[0m\n",
      "\u001b[34mEpoch: [1][30/105]  Time 0.608 (0.938)  Speed 2524.952 (1637.974)  Loss 3.2030019760 (3.6772)  Prec@1 22.705 (20.959)  Prec@5 50.716 (48.284)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 31, lr = 0.012525714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][31/105]  Time 1.104 (0.943)  Speed 1390.706 (1628.633)  Loss 3.2078647614 (3.6621)  Prec@1 22.607 (21.012)  Prec@5 49.674 (48.329)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 32, lr = 0.012617142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][32/105]  Time 0.970 (0.944)  Speed 1584.084 (1627.203)  Loss 3.1947693825 (3.6475)  Prec@1 22.412 (21.056)  Prec@5 49.837 (48.376)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 33, lr = 0.012708571428571428\u001b[0m\n",
      "\u001b[34mEpoch: [1][33/105]  Time 0.590 (0.933)  Speed 2604.072 (1645.913)  Loss 3.1990442276 (3.6339)  Prec@1 22.396 (21.096)  Prec@5 50.260 (48.433)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 34, lr = 0.012799999999999999\u001b[0m\n",
      "\u001b[34mEpoch: [1][34/105]  Time 0.817 (0.930)  Speed 1881.082 (1651.988)  Loss 3.1918647289 (3.6209)  Prec@1 21.891 (21.120)  Prec@5 50.081 (48.482)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 35, lr = 0.012891428571428571\u001b[0m\n",
      "\u001b[34mEpoch: [1][35/105]  Time 1.026 (0.933)  Speed 1496.858 (1647.110)  Loss 3.1853830814 (3.6084)  Prec@1 21.338 (21.126)  Prec@5 50.114 (48.528)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch = 1, step = 36, lr = 0.012982857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][36/105]  Time 1.127 (0.938)  Speed 1363.335 (1637.642)  Loss 3.1775479317 (3.5965)  Prec@1 22.135 (21.154)  Prec@5 49.805 (48.564)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 37, lr = 0.013074285714285714\u001b[0m\n",
      "\u001b[34mEpoch: [1][37/105]  Time 1.081 (0.942)  Speed 1421.464 (1630.938)  Loss 3.1811597347 (3.5852)  Prec@1 22.005 (21.177)  Prec@5 50.179 (48.607)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 38, lr = 0.013165714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][38/105]  Time 0.846 (0.939)  Speed 1816.482 (1635.334)  Loss 3.2027611732 (3.5752)  Prec@1 20.947 (21.171)  Prec@5 49.528 (48.632)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 39, lr = 0.013257142857142856\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/105] loss: 3.1899\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/105] loss: 3.1968\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/105] loss: 3.1960\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/105] loss: 3.1878\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/105] loss: 3.1776\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/105] loss: 3.2183\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/105] loss: 3.1951\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 40/105] loss: 3.1884\u001b[0m\n",
      "\u001b[34mEpoch: [1][39/105]  Time 1.051 (0.942)  Speed 1461.610 (1630.365)  Loss 3.1939043999 (3.5654)  Prec@1 21.842 (21.188)  Prec@5 49.805 (48.662)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 40, lr = 0.013348571428571428\u001b[0m\n",
      "\u001b[34mEpoch: [1][40/105]  Time 1.049 (0.945)  Speed 1463.598 (1625.734)  Loss 3.1965689659 (3.5562)  Prec@1 21.875 (21.205)  Prec@5 49.821 (48.691)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 41, lr = 0.01344\u001b[0m\n",
      "\u001b[34mEpoch: [1][41/105]  Time 1.083 (0.948)  Speed 1417.734 (1619.937)  Loss 3.1771674156 (3.5469)  Prec@1 22.168 (21.229)  Prec@5 49.723 (48.716)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 42, lr = 0.013531428571428571\u001b[0m\n",
      "\u001b[34mEpoch: [1][42/105]  Time 1.070 (0.951)  Speed 1436.065 (1615.014)  Loss 3.1966464520 (3.5386)  Prec@1 22.510 (21.259)  Prec@5 50.423 (48.756)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 43, lr = 0.013622857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][43/105]  Time 1.157 (0.956)  Speed 1327.430 (1606.918)  Loss 3.1899304390 (3.5305)  Prec@1 21.940 (21.275)  Prec@5 50.244 (48.791)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 44, lr = 0.013714285714285715\u001b[0m\n",
      "\u001b[34mEpoch: [1][44/105]  Time 0.984 (0.956)  Speed 1561.582 (1605.858)  Loss 3.1942846775 (3.5229)  Prec@1 21.908 (21.289)  Prec@5 50.260 (48.824)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 45, lr = 0.013805714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][45/105]  Time 0.843 (0.954)  Speed 1823.112 (1610.122)  Loss 3.1846160889 (3.5153)  Prec@1 22.738 (21.322)  Prec@5 50.798 (48.868)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 46, lr = 0.013897142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][46/105]  Time 1.106 (0.957)  Speed 1389.198 (1604.575)  Loss 3.1879591942 (3.5082)  Prec@1 22.526 (21.348)  Prec@5 50.667 (48.907)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 47, lr = 0.013988571428571428\u001b[0m\n",
      "\u001b[34mEpoch: [1][47/105]  Time 1.039 (0.959)  Speed 1478.985 (1601.681)  Loss 3.1951799393 (3.5016)  Prec@1 21.973 (21.361)  Prec@5 50.179 (48.934)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 48, lr = 0.01408\u001b[0m\n",
      "\u001b[34mEpoch: [1][48/105]  Time 0.599 (0.951)  Speed 2563.363 (1614.298)  Loss 3.1892051697 (3.4951)  Prec@1 22.249 (21.380)  Prec@5 49.658 (48.950)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 49, lr = 0.014171428571428573\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/105] loss: 3.1837\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/105] loss: 3.1874\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/105] loss: 3.1953\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/105] loss: 3.1867\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/105] loss: 3.1877\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/105] loss: 3.1892\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/105] loss: 3.1845\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 50/105] loss: 3.1942\u001b[0m\n",
      "\u001b[34mEpoch: [1][49/105]  Time 0.992 (0.952)  Speed 1548.030 (1612.889)  Loss 3.1742272377 (3.4885)  Prec@1 22.119 (21.395)  Prec@5 50.423 (48.980)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 50, lr = 0.014262857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][50/105]  Time 1.094 (0.955)  Speed 1403.947 (1608.103)  Loss 3.1831352711 (3.4824)  Prec@1 21.842 (21.404)  Prec@5 51.156 (49.023)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 51, lr = 0.014354285714285715\u001b[0m\n",
      "\u001b[34mEpoch: [1][51/105]  Time 1.070 (0.957)  Speed 1435.830 (1604.328)  Loss 3.1864545345 (3.4766)  Prec@1 21.647 (21.408)  Prec@5 50.212 (49.046)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 52, lr = 0.014445714285714287\u001b[0m\n",
      "\u001b[34mEpoch: [1][52/105]  Time 0.585 (0.950)  Speed 2626.108 (1616.423)  Loss 3.2060554028 (3.4714)  Prec@1 22.428 (21.428)  Prec@5 50.505 (49.074)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 53, lr = 0.014537142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][53/105]  Time 1.110 (0.953)  Speed 1383.935 (1611.316)  Loss 3.1917922497 (3.4661)  Prec@1 21.517 (21.430)  Prec@5 51.465 (49.120)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 54, lr = 0.014628571428571428\u001b[0m\n",
      "\u001b[34mEpoch: [1][54/105]  Time 0.882 (0.952)  Speed 1741.590 (1613.551)  Loss 3.1860923767 (3.4609)  Prec@1 22.135 (21.443)  Prec@5 50.065 (49.137)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 55, lr = 0.014719999999999999\u001b[0m\n",
      "\u001b[34mEpoch: [1][55/105]  Time 0.876 (0.951)  Speed 1752.552 (1615.881)  Loss 3.1815290451 (3.4558)  Prec@1 22.038 (21.454)  Prec@5 50.570 (49.163)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 56, lr = 0.014811428571428571\u001b[0m\n",
      "\u001b[34mEpoch: [1][56/105]  Time 0.850 (0.949)  Speed 1806.735 (1618.935)  Loss 3.1770205498 (3.4509)  Prec@1 22.542 (21.473)  Prec@5 50.407 (49.185)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 57, lr = 0.014902857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][57/105]  Time 0.957 (0.949)  Speed 1604.884 (1618.686)  Loss 3.1780006886 (3.4461)  Prec@1 21.305 (21.470)  Prec@5 49.951 (49.199)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 58, lr = 0.014994285714285713\u001b[0m\n",
      "\u001b[34mEpoch: [1][58/105]  Time 0.963 (0.949)  Speed 1595.356 (1618.278)  Loss 3.1857750416 (3.4416)  Prec@1 22.380 (21.486)  Prec@5 51.318 (49.235)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 59, lr = 0.015085714285714286\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/105] loss: 3.1897\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/105] loss: 3.1794\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/105] loss: 3.1928\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/105] loss: 3.1873\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/105] loss: 3.1930\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/105] loss: 3.1847\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/105] loss: 3.1863\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 60/105] loss: 3.1804\u001b[0m\n",
      "\u001b[34mEpoch: [1][59/105]  Time 1.091 (0.952)  Speed 1407.794 (1614.188)  Loss 3.1910531521 (3.4373)  Prec@1 22.656 (21.506)  Prec@5 50.586 (49.258)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 60, lr = 0.015177142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][60/105]  Time 0.957 (0.952)  Speed 1604.853 (1614.031)  Loss 3.1805911064 (3.4331)  Prec@1 22.298 (21.519)  Prec@5 50.602 (49.281)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 61, lr = 0.015268571428571428\u001b[0m\n",
      "\u001b[34mEpoch: [1][61/105]  Time 1.068 (0.954)  Speed 1437.882 (1610.796)  Loss 3.2109487057 (3.4294)  Prec@1 21.777 (21.523)  Prec@5 50.342 (49.298)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 62, lr = 0.01536\u001b[0m\n",
      "\u001b[34mEpoch: [1][62/105]  Time 0.975 (0.954)  Speed 1574.961 (1610.205)  Loss 3.1789951324 (3.4254)  Prec@1 22.135 (21.533)  Prec@5 50.618 (49.319)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 63, lr = 0.01545142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [1][63/105]  Time 0.797 (0.951)  Speed 1926.211 (1614.409)  Loss 3.2025923729 (3.4219)  Prec@1 22.054 (21.541)  Prec@5 49.984 (49.330)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 64, lr = 0.015542857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][64/105]  Time 1.066 (0.953)  Speed 1440.576 (1611.371)  Loss 3.1717398167 (3.4179)  Prec@1 21.712 (21.544)  Prec@5 49.561 (49.333)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 65, lr = 0.015634285714285715\u001b[0m\n",
      "\u001b[34mEpoch: [1][65/105]  Time 0.959 (0.953)  Speed 1601.220 (1611.214)  Loss 3.1756126881 (3.4142)  Prec@1 22.233 (21.554)  Prec@5 49.935 (49.343)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 66, lr = 0.015725714285714287\u001b[0m\n",
      "\u001b[34mEpoch: [1][66/105]  Time 1.106 (0.956)  Speed 1388.798 (1607.314)  Loss 3.1877236366 (3.4108)  Prec@1 22.331 (21.566)  Prec@5 50.244 (49.356)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 67, lr = 0.015817142857142856\u001b[0m\n",
      "\u001b[34mEpoch: [1][67/105]  Time 0.950 (0.956)  Speed 1616.550 (1607.451)  Loss 3.1869034767 (3.4074)  Prec@1 21.973 (21.572)  Prec@5 49.137 (49.353)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 68, lr = 0.015908571428571428\u001b[0m\n",
      "\u001b[34mEpoch: [1][68/105]  Time 1.068 (0.957)  Speed 1438.587 (1604.681)  Loss 3.1935465336 (3.4043)  Prec@1 21.940 (21.578)  Prec@5 49.805 (49.360)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 69, lr = 0.016\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/105] loss: 3.1831\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/105] loss: 3.1930\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/105] loss: 3.1893\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/105] loss: 3.1865\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/105] loss: 3.1839\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/105] loss: 3.2009\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/105] loss: 3.1837\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 70/105] loss: 3.1878\u001b[0m\n",
      "\u001b[34mEpoch: [1][69/105]  Time 0.577 (0.952)  Speed 2659.855 (1613.960)  Loss 3.1965107918 (3.4013)  Prec@1 22.135 (21.586)  Prec@5 49.886 (49.367)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch = 1, step = 70, lr = 0.016091428571428572\u001b[0m\n",
      "\u001b[34mEpoch: [1][70/105]  Time 0.708 (0.948)  Speed 2168.052 (1619.874)  Loss 3.1806783676 (3.3981)  Prec@1 21.989 (21.592)  Prec@5 49.202 (49.365)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 71, lr = 0.016182857142857145\u001b[0m\n",
      "\u001b[34mEpoch: [1][71/105]  Time 1.088 (0.950)  Speed 1411.992 (1616.522)  Loss 3.1892127991 (3.3952)  Prec@1 22.282 (21.601)  Prec@5 50.342 (49.379)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 72, lr = 0.016274285714285717\u001b[0m\n",
      "\u001b[34mEpoch: [1][72/105]  Time 1.057 (0.952)  Speed 1453.041 (1614.000)  Loss 3.1924619675 (3.3924)  Prec@1 21.631 (21.602)  Prec@5 49.821 (49.385)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 73, lr = 0.016365714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][73/105]  Time 1.102 (0.954)  Speed 1394.391 (1610.526)  Loss 3.2004394531 (3.3897)  Prec@1 22.884 (21.619)  Prec@5 50.114 (49.395)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 74, lr = 0.016457142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][74/105]  Time 0.906 (0.953)  Speed 1695.575 (1611.618)  Loss 3.1954817772 (3.3871)  Prec@1 21.989 (21.624)  Prec@5 50.000 (49.403)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 75, lr = 0.01654857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][75/105]  Time 1.088 (0.955)  Speed 1412.309 (1608.591)  Loss 3.1996932030 (3.3846)  Prec@1 22.249 (21.633)  Prec@5 50.472 (49.417)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 76, lr = 0.016640000000000002\u001b[0m\n",
      "\u001b[34mEpoch: [1][76/105]  Time 1.045 (0.956)  Speed 1469.434 (1606.589)  Loss 3.1912353039 (3.3821)  Prec@1 22.770 (21.648)  Prec@5 50.114 (49.426)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 77, lr = 0.016731428571428574\u001b[0m\n",
      "\u001b[34mEpoch: [1][77/105]  Time 1.011 (0.957)  Speed 1518.807 (1605.384)  Loss 3.1932575703 (3.3796)  Prec@1 22.005 (21.652)  Prec@5 50.863 (49.445)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 78, lr = 0.016822857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][78/105]  Time 0.807 (0.955)  Speed 1904.234 (1608.621)  Loss 3.2044987679 (3.3774)  Prec@1 21.354 (21.648)  Prec@5 49.170 (49.442)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 79, lr = 0.016914285714285715\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/105] loss: 3.1754\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/105] loss: 3.1968\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/105] loss: 3.1963\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/105] loss: 3.1941\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/105] loss: 3.2043\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/105] loss: 3.2013\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/105] loss: 3.1928\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 80/105] loss: 3.1923\u001b[0m\n",
      "\u001b[34mEpoch: [1][79/105]  Time 1.078 (0.956)  Speed 1425.171 (1606.004)  Loss 3.1946272850 (3.3751)  Prec@1 22.510 (21.659)  Prec@5 50.716 (49.458)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 80, lr = 0.017005714285714287\u001b[0m\n",
      "\u001b[34mEpoch: [1][80/105]  Time 1.076 (0.958)  Speed 1427.100 (1603.491)  Loss 3.1885502338 (3.3727)  Prec@1 22.298 (21.667)  Prec@5 49.886 (49.463)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 81, lr = 0.01709714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [1][81/105]  Time 0.640 (0.954)  Speed 2401.516 (1610.097)  Loss 3.1774330139 (3.3703)  Prec@1 22.933 (21.683)  Prec@5 49.626 (49.465)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 82, lr = 0.01718857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][82/105]  Time 1.085 (0.956)  Speed 1415.898 (1607.408)  Loss 3.1841499805 (3.3681)  Prec@1 22.168 (21.689)  Prec@5 50.911 (49.483)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 83, lr = 0.01728\u001b[0m\n",
      "\u001b[34mEpoch: [1][83/105]  Time 0.867 (0.955)  Speed 1772.451 (1609.213)  Loss 3.1914956570 (3.3659)  Prec@1 22.266 (21.696)  Prec@5 49.756 (49.486)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 84, lr = 0.017371428571428572\u001b[0m\n",
      "\u001b[34mEpoch: [1][84/105]  Time 1.049 (0.956)  Speed 1464.651 (1607.325)  Loss 3.1970481873 (3.3639)  Prec@1 21.680 (21.696)  Prec@5 49.821 (49.490)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 85, lr = 0.017462857142857145\u001b[0m\n",
      "\u001b[34mEpoch: [1][85/105]  Time 1.073 (0.957)  Speed 1431.785 (1605.010)  Loss 3.2024435997 (3.3620)  Prec@1 22.249 (21.702)  Prec@5 49.756 (49.493)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 86, lr = 0.017554285714285717\u001b[0m\n",
      "\u001b[34mEpoch: [1][86/105]  Time 1.000 (0.958)  Speed 1535.244 (1604.162)  Loss 3.1986248493 (3.3601)  Prec@1 22.021 (21.706)  Prec@5 48.926 (49.487)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 87, lr = 0.017645714285714285\u001b[0m\n",
      "\u001b[34mEpoch: [1][87/105]  Time 0.982 (0.958)  Speed 1563.774 (1603.686)  Loss 3.1926400661 (3.3582)  Prec@1 21.924 (21.708)  Prec@5 50.146 (49.494)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 88, lr = 0.017737142857142858\u001b[0m\n",
      "\u001b[34mEpoch: [1][88/105]  Time 1.129 (0.960)  Speed 1360.731 (1600.439)  Loss 3.2157864571 (3.3566)  Prec@1 21.419 (21.705)  Prec@5 49.740 (49.497)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 89, lr = 0.017828571428571426\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/105] loss: 3.1853\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/105] loss: 3.2067\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/105] loss: 3.1933\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/105] loss: 3.2010\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/105] loss: 3.1888\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/105] loss: 3.1769\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/105] loss: 3.2070\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 90/105] loss: 3.1978\u001b[0m\n",
      "\u001b[34mEpoch: [1][89/105]  Time 1.004 (0.960)  Speed 1530.104 (1599.613)  Loss 3.1978685856 (3.3548)  Prec@1 21.452 (21.702)  Prec@5 49.902 (49.501)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 90, lr = 0.01792\u001b[0m\n",
      "\u001b[34mEpoch: [1][90/105]  Time 1.110 (0.962)  Speed 1384.333 (1596.853)  Loss 3.1793966293 (3.3528)  Prec@1 22.266 (21.708)  Prec@5 50.065 (49.508)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 91, lr = 0.01801142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [1][91/105]  Time 0.603 (0.958)  Speed 2546.631 (1603.425)  Loss 3.1832127571 (3.3510)  Prec@1 22.168 (21.713)  Prec@5 49.691 (49.510)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 92, lr = 0.018102857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][92/105]  Time 1.073 (0.959)  Speed 1431.814 (1601.339)  Loss 3.1814785004 (3.3491)  Prec@1 22.038 (21.717)  Prec@5 49.398 (49.509)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 93, lr = 0.018194285714285715\u001b[0m\n",
      "\u001b[34mEpoch: [1][93/105]  Time 1.093 (0.961)  Speed 1405.801 (1598.947)  Loss 3.1784615517 (3.3473)  Prec@1 22.493 (21.725)  Prec@5 50.049 (49.514)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 94, lr = 0.018285714285714284\u001b[0m\n",
      "\u001b[34mEpoch: [1][94/105]  Time 1.114 (0.962)  Speed 1379.265 (1596.243)  Loss 3.1748597622 (3.3455)  Prec@1 23.031 (21.739)  Prec@5 51.009 (49.530)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 95, lr = 0.018377142857142856\u001b[0m\n",
      "\u001b[34mEpoch: [1][95/105]  Time 1.015 (0.963)  Speed 1513.869 (1595.329)  Loss 3.1879401207 (3.3438)  Prec@1 21.729 (21.739)  Prec@5 50.407 (49.539)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 96, lr = 0.018468571428571428\u001b[0m\n",
      "\u001b[34mEpoch: [1][96/105]  Time 1.043 (0.964)  Speed 1472.457 (1593.943)  Loss 3.1960153580 (3.3423)  Prec@1 22.591 (21.748)  Prec@5 50.570 (49.550)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 97, lr = 0.01856\u001b[0m\n",
      "\u001b[34mEpoch: [1][97/105]  Time 1.079 (0.965)  Speed 1423.142 (1591.974)  Loss 3.1891226768 (3.3407)  Prec@1 22.542 (21.756)  Prec@5 50.260 (49.558)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 98, lr = 0.018651428571428572\u001b[0m\n",
      "\u001b[34mEpoch: [1][98/105]  Time 1.048 (0.966)  Speed 1465.220 (1590.570)  Loss 3.1759300232 (3.3390)  Prec@1 21.533 (21.754)  Prec@5 50.293 (49.565)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 99, lr = 0.01874285714285714\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/105] loss: 3.1763\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/105] loss: 3.1693\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/105] loss: 3.1998\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/105] loss: 3.1812\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/105] loss: 3.1851\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/105] loss: 3.1900\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/105] loss: 3.1909\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100/105] loss: 3.1677\u001b[0m\n",
      "\u001b[34mEpoch: [1][99/105]  Time 1.077 (0.967)  Speed 1425.668 (1588.713)  Loss 3.1788606644 (3.3374)  Prec@1 21.533 (21.752)  Prec@5 50.879 (49.578)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 100, lr = 0.018834285714285713\u001b[0m\n",
      "\u001b[34mEpoch: [1][100/105]  Time 0.846 (0.966)  Speed 1815.499 (1590.700)  Loss 3.1855115891 (3.3359)  Prec@1 22.526 (21.759)  Prec@5 50.228 (49.585)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 101, lr = 0.018925714285714285\u001b[0m\n",
      "\u001b[34mEpoch: [1][101/105]  Time 1.129 (0.967)  Speed 1360.909 (1588.046)  Loss 3.2006824017 (3.3345)  Prec@1 21.566 (21.758)  Prec@5 50.521 (49.594)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 102, lr = 0.019017142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [1][102/105]  Time 1.062 (0.968)  Speed 1445.740 (1586.515)  Loss 3.2131731510 (3.3333)  Prec@1 21.159 (21.752)  Prec@5 50.000 (49.598)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 103, lr = 0.01910857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [1][103/105]  Time 1.110 (0.970)  Speed 1383.438 (1584.257)  Loss 3.1914875507 (3.3320)  Prec@1 21.891 (21.753)  Prec@5 50.244 (49.604)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 104, lr = 0.019200000000000002\u001b[0m\n",
      "\u001b[34mEpoch: [1][104/105]  Time 0.777 (0.968)  Speed 1977.412 (1587.291)  Loss 3.1901605129 (3.3306)  Prec@1 22.152 (21.757)  Prec@5 49.495 (49.603)\u001b[0m\n",
      "\u001b[34mepoch = 1, step = 105, lr = 0.01929142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [1][105/105]  Time 0.174 (0.960)  Speed 8810.913 (1599.783)  Loss 3.1823558807 (3.3297)  Prec@1 22.037 (21.758)  Prec@5 51.078 (49.612)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTest: [10/26]  Time 5.820 (5.839)  Speed 263.934 (263.055)  Loss 3.6357 (3.7058)  Prec@1 21.688 (22.019)  Prec@5 49.391 (50.031)\u001b[0m\n",
      "\u001b[34mTest: [20/26]  Time 6.122 (5.620)  Speed 250.912 (273.309)  Loss 3.6809 (3.7354)  Prec@1 22.047 (22.015)  Prec@5 49.969 (49.891)\u001b[0m\n",
      "\u001b[34m  Prec@1 22.024 Prec@5 49.899\u001b[0m\n",
      "\u001b[34m[Epoch 1] trn_loss: 3.3257, vld_loss: 3.8875, score: 0.0616, score_each: [0.0062, 0.0910, 0.1429]\n",
      "  Prec@1 22.024 Prec@5 49.899\n",
      "  Prec@1 22.024 Prec@5 49.899\n",
      "  Prec@1 22.024 Prec@5 49.899\n",
      "  Prec@1 22.024 Prec@5 49.899\n",
      "  Prec@1 22.024 Prec@5 49.899\n",
      "  Prec@1 22.024 Prec@5 49.899\n",
      "  Prec@1 22.024 Prec@5 49.899\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 1, lr = 0.019382857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][1/105]  Time 1.253 (1.253)  Speed 1226.285 (1226.285)  Loss 3.1802086830 (3.1802)  Prec@1 23.242 (23.242)  Prec@5 50.114 (50.114)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 2, lr = 0.019474285714285715\u001b[0m\n",
      "\u001b[34mEpoch: [2][2/105]  Time 1.029 (1.141)  Speed 1492.000 (1346.155)  Loss 3.1944029331 (3.1873)  Prec@1 22.005 (22.624)  Prec@5 50.846 (50.480)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 3, lr = 0.019565714285714287\u001b[0m\n",
      "\u001b[34mEpoch: [2][3/105]  Time 1.049 (1.110)  Speed 1463.738 (1383.193)  Loss 3.1781389713 (3.1843)  Prec@1 22.249 (22.499)  Prec@5 50.846 (50.602)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 4, lr = 0.01965714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][4/105]  Time 1.089 (1.105)  Speed 1410.032 (1389.806)  Loss 3.1715772152 (3.1811)  Prec@1 22.526 (22.506)  Prec@5 50.212 (50.505)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 5, lr = 0.019748571428571428\u001b[0m\n",
      "\u001b[34mEpoch: [2][5/105]  Time 1.125 (1.109)  Speed 1365.342 (1384.844)  Loss 3.1747326851 (3.1798)  Prec@1 22.575 (22.520)  Prec@5 49.870 (50.378)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 6, lr = 0.01984\u001b[0m\n",
      "\u001b[34mEpoch: [2][6/105]  Time 0.579 (1.021)  Speed 2651.272 (1504.629)  Loss 3.1880488396 (3.1812)  Prec@1 21.875 (22.412)  Prec@5 49.707 (50.266)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 7, lr = 0.019931428571428572\u001b[0m\n",
      "\u001b[34mEpoch: [2][7/105]  Time 1.011 (1.019)  Speed 1519.306 (1506.708)  Loss 3.2004833221 (3.1839)  Prec@1 21.777 (22.321)  Prec@5 50.016 (50.230)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 8, lr = 0.020022857142857144\u001b[0m\n",
      "\u001b[34mEpoch: [2][8/105]  Time 0.830 (0.996)  Speed 1850.643 (1542.543)  Loss 3.1752204895 (3.1829)  Prec@1 21.745 (22.249)  Prec@5 50.423 (50.254)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 9, lr = 0.020114285714285716\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 10/105] loss: 2.8546\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 10/105] loss: 2.8609\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 10/105] loss: 2.8721\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 10/105] loss: 2.8666\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 10/105] loss: 2.8599\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 10/105] loss: 2.8794\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 10/105] loss: 2.8702\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 10/105] loss: 2.8614\u001b[0m\n",
      "\u001b[34mEpoch: [2][9/105]  Time 1.083 (1.005)  Speed 1418.543 (1527.705)  Loss 3.1936590672 (3.1841)  Prec@1 22.607 (22.289)  Prec@5 49.854 (50.210)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 10, lr = 0.020205714285714285\u001b[0m\n",
      "\u001b[34mEpoch: [2][10/105]  Time 0.701 (0.975)  Speed 2190.143 (1575.354)  Loss 3.1825575829 (3.1839)  Prec@1 22.396 (22.300)  Prec@5 50.716 (50.260)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 11, lr = 0.020297142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][11/105]  Time 0.963 (0.974)  Speed 1594.211 (1577.049)  Loss 3.1983642578 (3.1852)  Prec@1 21.924 (22.266)  Prec@5 50.456 (50.278)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 12, lr = 0.02038857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][12/105]  Time 1.108 (0.985)  Speed 1386.881 (1559.233)  Loss 3.2100467682 (3.1873)  Prec@1 22.363 (22.274)  Prec@5 50.081 (50.262)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 13, lr = 0.02048\u001b[0m\n",
      "\u001b[34mEpoch: [2][13/105]  Time 0.587 (0.954)  Speed 2616.227 (1609.245)  Loss 3.1966588497 (3.1880)  Prec@1 23.063 (22.334)  Prec@5 50.635 (50.290)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 14, lr = 0.020571428571428574\u001b[0m\n",
      "\u001b[34mEpoch: [2][14/105]  Time 0.907 (0.951)  Speed 1692.592 (1614.925)  Loss 3.1780817509 (3.1873)  Prec@1 22.266 (22.330)  Prec@5 50.277 (50.289)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 15, lr = 0.020662857142857146\u001b[0m\n",
      "\u001b[34mEpoch: [2][15/105]  Time 1.025 (0.956)  Speed 1498.229 (1606.583)  Loss 3.1854531765 (3.1872)  Prec@1 22.689 (22.354)  Prec@5 49.902 (50.264)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 16, lr = 0.020754285714285715\u001b[0m\n",
      "\u001b[34mEpoch: [2][16/105]  Time 1.066 (0.963)  Speed 1440.523 (1595.090)  Loss 3.1909663677 (3.1874)  Prec@1 22.103 (22.338)  Prec@5 50.570 (50.283)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 17, lr = 0.020845714285714287\u001b[0m\n",
      "\u001b[34mEpoch: [2][17/105]  Time 1.009 (0.966)  Speed 1522.683 (1590.641)  Loss 3.1911549568 (3.1876)  Prec@1 22.168 (22.328)  Prec@5 50.016 (50.267)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 18, lr = 0.02093714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][18/105]  Time 1.081 (0.972)  Speed 1420.470 (1580.125)  Loss 3.1841280460 (3.1874)  Prec@1 22.917 (22.361)  Prec@5 50.016 (50.253)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 19, lr = 0.02102857142857143\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 20/105] loss: 3.1760\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 20/105] loss: 3.1899\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 20/105] loss: 3.1892\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 20/105] loss: 3.2116\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 20/105] loss: 3.1934\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 20/105] loss: 3.1850\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 20/105] loss: 3.1889\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 20/105] loss: 3.1934\u001b[0m\n",
      "\u001b[34mEpoch: [2][19/105]  Time 1.073 (0.977)  Speed 1432.069 (1571.573)  Loss 3.1917040348 (3.1877)  Prec@1 22.070 (22.345)  Prec@5 50.098 (50.245)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 20, lr = 0.021120000000000003\u001b[0m\n",
      "\u001b[34mEpoch: [2][20/105]  Time 1.076 (0.982)  Speed 1428.052 (1563.715)  Loss 3.1915192604 (3.1879)  Prec@1 22.038 (22.330)  Prec@5 49.023 (50.184)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 21, lr = 0.021211428571428572\u001b[0m\n",
      "\u001b[34mEpoch: [2][21/105]  Time 1.066 (0.986)  Speed 1440.256 (1557.358)  Loss 3.1806218624 (3.1875)  Prec@1 21.956 (22.312)  Prec@5 49.854 (50.168)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 22, lr = 0.021302857142857144\u001b[0m\n",
      "\u001b[34mEpoch: [2][22/105]  Time 1.122 (0.992)  Speed 1369.251 (1547.694)  Loss 3.1925401688 (3.1877)  Prec@1 22.168 (22.306)  Prec@5 50.212 (50.170)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 23, lr = 0.021394285714285716\u001b[0m\n",
      "\u001b[34mEpoch: [2][23/105]  Time 0.793 (0.984)  Speed 1936.612 (1561.326)  Loss 3.1903018951 (3.1879)  Prec@1 22.314 (22.306)  Prec@5 50.456 (50.183)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 24, lr = 0.021485714285714285\u001b[0m\n",
      "\u001b[34mEpoch: [2][24/105]  Time 1.148 (0.991)  Speed 1337.927 (1550.539)  Loss 3.1968867779 (3.1882)  Prec@1 22.266 (22.304)  Prec@5 49.984 (50.174)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 25, lr = 0.021577142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][25/105]  Time 0.956 (0.989)  Speed 1606.431 (1552.700)  Loss 3.1772079468 (3.1878)  Prec@1 22.021 (22.293)  Prec@5 49.512 (50.148)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 26, lr = 0.021668571428571426\u001b[0m\n",
      "\u001b[34mEpoch: [2][26/105]  Time 0.971 (0.989)  Speed 1581.889 (1553.802)  Loss 3.2006721497 (3.1883)  Prec@1 22.493 (22.301)  Prec@5 49.854 (50.136)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 27, lr = 0.021759999999999998\u001b[0m\n",
      "\u001b[34mEpoch: [2][27/105]  Time 0.970 (0.988)  Speed 1582.919 (1554.862)  Loss 3.1830046177 (3.1881)  Prec@1 21.484 (22.270)  Prec@5 51.270 (50.178)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 28, lr = 0.02185142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][28/105]  Time 1.016 (0.989)  Speed 1511.800 (1553.282)  Loss 3.1939783096 (3.1883)  Prec@1 21.370 (22.238)  Prec@5 50.326 (50.184)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 29, lr = 0.021942857142857142\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 30/105] loss: 3.1868\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 30/105] loss: 3.1812\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 30/105] loss: 3.1939\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 30/105] loss: 3.1945\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 30/105] loss: 3.1962\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 30/105] loss: 3.1852\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 30/105] loss: 3.1902\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 30/105] loss: 3.1865\u001b[0m\n",
      "\u001b[34mEpoch: [2][29/105]  Time 1.125 (0.994)  Speed 1365.341 (1545.944)  Loss 3.1863565445 (3.1882)  Prec@1 21.419 (22.210)  Prec@5 51.074 (50.214)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 30, lr = 0.022034285714285715\u001b[0m\n",
      "\u001b[34mEpoch: [2][30/105]  Time 0.876 (0.990)  Speed 1753.352 (1552.064)  Loss 3.2093918324 (3.1889)  Prec@1 21.598 (22.190)  Prec@5 50.374 (50.220)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 31, lr = 0.022125714285714287\u001b[0m\n",
      "\u001b[34mEpoch: [2][31/105]  Time 0.596 (0.977)  Speed 2579.025 (1572.259)  Loss 3.1887276173 (3.1889)  Prec@1 22.721 (22.207)  Prec@5 49.854 (50.208)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 32, lr = 0.022217142857142855\u001b[0m\n",
      "\u001b[34mEpoch: [2][32/105]  Time 1.042 (0.979)  Speed 1474.213 (1568.998)  Loss 3.1830086708 (3.1887)  Prec@1 23.047 (22.233)  Prec@5 49.626 (50.190)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 33, lr = 0.022308571428571428\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch: [2][33/105]  Time 0.882 (0.976)  Speed 1740.836 (1573.706)  Loss 3.2045943737 (3.1892)  Prec@1 22.038 (22.227)  Prec@5 49.707 (50.175)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 34, lr = 0.0224\u001b[0m\n",
      "\u001b[34mEpoch: [2][34/105]  Time 1.062 (0.979)  Speed 1446.448 (1569.644)  Loss 3.1720867157 (3.1887)  Prec@1 21.322 (22.201)  Prec@5 50.651 (50.189)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 35, lr = 0.022491428571428572\u001b[0m\n",
      "\u001b[34mEpoch: [2][35/105]  Time 1.124 (0.983)  Speed 1367.015 (1563.025)  Loss 3.2039194107 (3.1892)  Prec@1 22.233 (22.201)  Prec@5 48.535 (50.142)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 36, lr = 0.022582857142857144\u001b[0m\n",
      "\u001b[34mEpoch: [2][36/105]  Time 1.115 (0.986)  Speed 1378.133 (1557.221)  Loss 3.1866955757 (3.1891)  Prec@1 22.428 (22.208)  Prec@5 49.561 (50.126)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 37, lr = 0.022674285714285713\u001b[0m\n",
      "\u001b[34mEpoch: [2][37/105]  Time 0.844 (0.983)  Speed 1819.113 (1563.304)  Loss 3.1947102547 (3.1892)  Prec@1 22.054 (22.204)  Prec@5 50.000 (50.122)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 38, lr = 0.022765714285714285\u001b[0m\n",
      "\u001b[34mEpoch: [2][38/105]  Time 1.100 (0.986)  Speed 1396.686 (1558.412)  Loss 3.1816554070 (3.1890)  Prec@1 22.119 (22.201)  Prec@5 49.854 (50.115)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 39, lr = 0.022857142857142857\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 40/105] loss: 3.1984\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 40/105] loss: 3.1807\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 40/105] loss: 3.1984\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 40/105] loss: 3.1864\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 40/105] loss: 3.2013\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 40/105] loss: 3.1859\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 40/105] loss: 3.1984\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 40/105] loss: 3.1966\u001b[0m\n",
      "\u001b[34mEpoch: [2][39/105]  Time 0.990 (0.986)  Speed 1552.105 (1558.249)  Loss 3.2078580856 (3.1895)  Prec@1 21.436 (22.182)  Prec@5 49.707 (50.105)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 40, lr = 0.02294857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][40/105]  Time 1.039 (0.987)  Speed 1478.320 (1556.146)  Loss 3.2001018524 (3.1898)  Prec@1 22.135 (22.181)  Prec@5 49.561 (50.091)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 41, lr = 0.02304\u001b[0m\n",
      "\u001b[34mEpoch: [2][41/105]  Time 1.100 (0.990)  Speed 1395.997 (1551.804)  Loss 3.2146577835 (3.1904)  Prec@1 21.956 (22.175)  Prec@5 49.707 (50.082)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 42, lr = 0.02313142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][42/105]  Time 0.979 (0.990)  Speed 1568.744 (1552.203)  Loss 3.1946256161 (3.1905)  Prec@1 22.461 (22.182)  Prec@5 50.407 (50.090)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 43, lr = 0.023222857142857142\u001b[0m\n",
      "\u001b[34mEpoch: [2][43/105]  Time 1.090 (0.992)  Speed 1409.810 (1548.566)  Loss 3.1928625107 (3.1905)  Prec@1 22.314 (22.185)  Prec@5 50.065 (50.089)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 44, lr = 0.023314285714285714\u001b[0m\n",
      "\u001b[34mEpoch: [2][44/105]  Time 1.119 (0.995)  Speed 1372.854 (1544.074)  Loss 3.1890635490 (3.1905)  Prec@1 22.493 (22.192)  Prec@5 50.407 (50.096)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 45, lr = 0.023405714285714287\u001b[0m\n",
      "\u001b[34mEpoch: [2][45/105]  Time 1.083 (0.997)  Speed 1418.623 (1541.046)  Loss 3.1858139038 (3.1904)  Prec@1 22.119 (22.190)  Prec@5 50.553 (50.106)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 46, lr = 0.02349714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][46/105]  Time 1.001 (0.997)  Speed 1534.207 (1540.896)  Loss 3.1908566952 (3.1904)  Prec@1 21.973 (22.186)  Prec@5 50.928 (50.124)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 47, lr = 0.023588571428571427\u001b[0m\n",
      "\u001b[34mEpoch: [2][47/105]  Time 0.735 (0.991)  Speed 2089.744 (1549.555)  Loss 3.1713793278 (3.1900)  Prec@1 23.047 (22.204)  Prec@5 50.407 (50.130)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 48, lr = 0.02368\u001b[0m\n",
      "\u001b[34mEpoch: [2][48/105]  Time 1.107 (0.994)  Speed 1387.563 (1545.796)  Loss 3.1900770664 (3.1900)  Prec@1 22.233 (22.205)  Prec@5 50.163 (50.131)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 49, lr = 0.023771428571428572\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 50/105] loss: 3.1863\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 50/105] loss: 3.1862\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 50/105] loss: 3.2009\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 50/105] loss: 3.2017\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 50/105] loss: 3.1851\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 50/105] loss: 3.1993\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 50/105] loss: 3.1846\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 50/105] loss: 3.2045\u001b[0m\n",
      "\u001b[34mEpoch: [2][49/105]  Time 0.956 (0.993)  Speed 1607.463 (1547.007)  Loss 3.2062888145 (3.1903)  Prec@1 21.859 (22.198)  Prec@5 49.854 (50.125)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 50, lr = 0.023862857142857144\u001b[0m\n",
      "\u001b[34mEpoch: [2][50/105]  Time 1.068 (0.994)  Speed 1438.268 (1544.671)  Loss 3.1941580772 (3.1904)  Prec@1 22.021 (22.194)  Prec@5 50.244 (50.128)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 51, lr = 0.023954285714285716\u001b[0m\n",
      "\u001b[34mEpoch: [2][51/105]  Time 1.043 (0.995)  Speed 1472.893 (1543.197)  Loss 3.1891942024 (3.1904)  Prec@1 23.161 (22.213)  Prec@5 50.212 (50.129)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 52, lr = 0.02404571428571429\u001b[0m\n",
      "\u001b[34mEpoch: [2][52/105]  Time 1.097 (0.997)  Speed 1400.142 (1540.170)  Loss 3.1999156475 (3.1906)  Prec@1 22.184 (22.212)  Prec@5 50.212 (50.131)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 53, lr = 0.024137142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][53/105]  Time 1.113 (0.999)  Speed 1380.587 (1536.819)  Loss 3.1896512508 (3.1906)  Prec@1 22.038 (22.209)  Prec@5 51.156 (50.150)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 54, lr = 0.02422857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][54/105]  Time 1.068 (1.001)  Speed 1438.745 (1534.881)  Loss 3.1855702400 (3.1905)  Prec@1 22.347 (22.212)  Prec@5 50.602 (50.159)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 55, lr = 0.02432\u001b[0m\n",
      "\u001b[34mEpoch: [2][55/105]  Time 0.636 (0.994)  Speed 2413.938 (1545.111)  Loss 3.1912932396 (3.1905)  Prec@1 22.575 (22.218)  Prec@5 50.374 (50.162)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 56, lr = 0.024411428571428574\u001b[0m\n",
      "\u001b[34mEpoch: [2][56/105]  Time 1.134 (0.997)  Speed 1353.969 (1541.226)  Loss 3.2002861500 (3.1907)  Prec@1 21.859 (22.212)  Prec@5 50.407 (50.167)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 57, lr = 0.024502857142857146\u001b[0m\n",
      "\u001b[34mEpoch: [2][57/105]  Time 0.866 (0.994)  Speed 1772.951 (1544.768)  Loss 3.2087454796 (3.1910)  Prec@1 21.745 (22.204)  Prec@5 49.365 (50.153)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 58, lr = 0.024594285714285714\u001b[0m\n",
      "\u001b[34mEpoch: [2][58/105]  Time 0.853 (0.992)  Speed 1800.396 (1548.559)  Loss 3.1836884022 (3.1909)  Prec@1 21.452 (22.191)  Prec@5 49.967 (50.150)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 59, lr = 0.024685714285714287\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 60/105] loss: 3.2170\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 60/105] loss: 3.1907\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 60/105] loss: 3.1991\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 60/105] loss: 3.1918\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 60/105] loss: 3.2005\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 60/105] loss: 3.1923\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 60/105] loss: 3.1753\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 60/105] loss: 3.1789\u001b[0m\n",
      "\u001b[34mEpoch: [2][59/105]  Time 0.755 (0.988)  Speed 2034.495 (1554.854)  Loss 3.1895532608 (3.1908)  Prec@1 22.070 (22.189)  Prec@5 49.886 (50.145)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 60, lr = 0.02477714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][60/105]  Time 1.031 (0.989)  Speed 1489.337 (1553.714)  Loss 3.2089915276 (3.1911)  Prec@1 21.549 (22.178)  Prec@5 49.691 (50.138)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 61, lr = 0.02486857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][61/105]  Time 1.102 (0.990)  Speed 1394.326 (1550.808)  Loss 3.1880393028 (3.1911)  Prec@1 22.998 (22.191)  Prec@5 49.593 (50.129)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 62, lr = 0.024960000000000003\u001b[0m\n",
      "\u001b[34mEpoch: [2][62/105]  Time 1.111 (0.992)  Speed 1382.020 (1547.759)  Loss 3.1889023781 (3.1910)  Prec@1 22.217 (22.192)  Prec@5 50.146 (50.129)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 63, lr = 0.02505142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][63/105]  Time 1.061 (0.993)  Speed 1447.883 (1546.067)  Loss 3.1898403168 (3.1910)  Prec@1 22.526 (22.197)  Prec@5 49.691 (50.122)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 64, lr = 0.025142857142857144\u001b[0m\n",
      "\u001b[34mEpoch: [2][64/105]  Time 1.002 (0.994)  Speed 1533.466 (1545.868)  Loss 3.2004811764 (3.1912)  Prec@1 22.380 (22.200)  Prec@5 49.251 (50.108)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 65, lr = 0.025234285714285716\u001b[0m\n",
      "\u001b[34mEpoch: [2][65/105]  Time 1.077 (0.995)  Speed 1426.755 (1543.885)  Loss 3.1907322407 (3.1912)  Prec@1 21.403 (22.188)  Prec@5 49.967 (50.106)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 66, lr = 0.025325714285714288\u001b[0m\n",
      "\u001b[34mEpoch: [2][66/105]  Time 0.973 (0.995)  Speed 1577.905 (1544.390)  Loss 3.1894662380 (3.1911)  Prec@1 21.061 (22.171)  Prec@5 49.447 (50.096)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 67, lr = 0.025417142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][67/105]  Time 0.980 (0.994)  Speed 1566.642 (1544.717)  Loss 3.1799349785 (3.1910)  Prec@1 21.631 (22.163)  Prec@5 50.260 (50.099)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch = 2, step = 68, lr = 0.02550857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][68/105]  Time 1.070 (0.995)  Speed 1435.600 (1542.992)  Loss 3.1949069500 (3.1910)  Prec@1 22.070 (22.161)  Prec@5 50.179 (50.100)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 69, lr = 0.025599999999999998\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 70/105] loss: 3.1892\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 70/105] loss: 3.1896\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 70/105] loss: 3.1831\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 70/105] loss: 3.2041\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 70/105] loss: 3.1817\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 70/105] loss: 3.1974\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 70/105] loss: 3.1937\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 70/105] loss: 3.1885\u001b[0m\n",
      "\u001b[34mEpoch: [2][69/105]  Time 1.141 (0.998)  Speed 1345.737 (1539.722)  Loss 3.1780173779 (3.1908)  Prec@1 22.347 (22.164)  Prec@5 50.618 (50.107)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 70, lr = 0.02569142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][70/105]  Time 1.120 (0.999)  Speed 1370.917 (1537.018)  Loss 3.1602549553 (3.1904)  Prec@1 22.282 (22.166)  Prec@5 51.449 (50.126)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 71, lr = 0.025782857142857142\u001b[0m\n",
      "\u001b[34mEpoch: [2][71/105]  Time 1.063 (1.000)  Speed 1444.976 (1535.640)  Loss 3.1928300858 (3.1904)  Prec@1 22.852 (22.175)  Prec@5 50.586 (50.133)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 72, lr = 0.025874285714285714\u001b[0m\n",
      "\u001b[34mEpoch: [2][72/105]  Time 0.959 (1.000)  Speed 1601.009 (1536.512)  Loss 3.1829154491 (3.1903)  Prec@1 22.152 (22.175)  Prec@5 50.635 (50.140)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 73, lr = 0.025965714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][73/105]  Time 1.022 (1.000)  Speed 1503.459 (1536.049)  Loss 3.1979746819 (3.1904)  Prec@1 21.631 (22.168)  Prec@5 50.586 (50.146)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 74, lr = 0.026057142857142855\u001b[0m\n",
      "\u001b[34mEpoch: [2][74/105]  Time 1.095 (1.001)  Speed 1403.249 (1534.087)  Loss 3.1995754242 (3.1906)  Prec@1 22.347 (22.170)  Prec@5 50.228 (50.147)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 75, lr = 0.026148571428571427\u001b[0m\n",
      "\u001b[34mEpoch: [2][75/105]  Time 1.037 (1.002)  Speed 1480.821 (1533.352)  Loss 3.1992940903 (3.1907)  Prec@1 22.624 (22.176)  Prec@5 50.146 (50.147)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 76, lr = 0.02624\u001b[0m\n",
      "\u001b[34mEpoch: [2][76/105]  Time 0.991 (1.002)  Speed 1549.969 (1533.568)  Loss 3.1927402020 (3.1907)  Prec@1 21.615 (22.169)  Prec@5 50.423 (50.151)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 77, lr = 0.02633142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][77/105]  Time 0.803 (0.999)  Speed 1912.845 (1537.527)  Loss 3.2130031586 (3.1910)  Prec@1 21.533 (22.160)  Prec@5 49.316 (50.140)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 78, lr = 0.026422857142857144\u001b[0m\n",
      "\u001b[34mEpoch: [2][78/105]  Time 1.091 (1.000)  Speed 1407.796 (1535.713)  Loss 3.1959092617 (3.1911)  Prec@1 21.338 (22.150)  Prec@5 49.219 (50.128)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 79, lr = 0.026514285714285712\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 80/105] loss: 3.1873\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 80/105] loss: 3.1776\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 80/105] loss: 3.2006\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 80/105] loss: 3.2059\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 80/105] loss: 3.1803\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 80/105] loss: 3.2025\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 80/105] loss: 3.1883\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 80/105] loss: 3.1934\u001b[0m\n",
      "\u001b[34mEpoch: [2][79/105]  Time 1.122 (1.002)  Speed 1368.621 (1533.343)  Loss 3.1852321625 (3.1910)  Prec@1 22.217 (22.151)  Prec@5 49.674 (50.122)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 80, lr = 0.026605714285714285\u001b[0m\n",
      "\u001b[34mEpoch: [2][80/105]  Time 1.064 (1.003)  Speed 1443.449 (1532.150)  Loss 3.1992192268 (3.1911)  Prec@1 22.184 (22.151)  Prec@5 50.326 (50.125)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 81, lr = 0.026697142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][81/105]  Time 1.017 (1.003)  Speed 1510.251 (1531.876)  Loss 3.1695594788 (3.1908)  Prec@1 21.387 (22.142)  Prec@5 50.732 (50.132)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 82, lr = 0.02678857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][82/105]  Time 1.012 (1.003)  Speed 1517.567 (1531.700)  Loss 3.1890869141 (3.1908)  Prec@1 22.510 (22.146)  Prec@5 50.309 (50.135)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 83, lr = 0.02688\u001b[0m\n",
      "\u001b[34mEpoch: [2][83/105]  Time 1.020 (1.003)  Speed 1505.952 (1531.385)  Loss 3.1834402084 (3.1907)  Prec@1 22.103 (22.146)  Prec@5 50.391 (50.138)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 84, lr = 0.026971428571428573\u001b[0m\n",
      "\u001b[34mEpoch: [2][84/105]  Time 0.853 (1.001)  Speed 1801.238 (1534.121)  Loss 3.1920895576 (3.1907)  Prec@1 22.152 (22.146)  Prec@5 49.577 (50.131)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 85, lr = 0.027062857142857142\u001b[0m\n",
      "\u001b[34mEpoch: [2][85/105]  Time 0.988 (1.001)  Speed 1555.356 (1534.367)  Loss 3.1949830055 (3.1908)  Prec@1 22.591 (22.151)  Prec@5 50.114 (50.131)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 86, lr = 0.027154285714285714\u001b[0m\n",
      "\u001b[34mEpoch: [2][86/105]  Time 1.047 (1.002)  Speed 1466.562 (1533.543)  Loss 3.1995882988 (3.1909)  Prec@1 22.054 (22.150)  Prec@5 50.277 (50.132)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 87, lr = 0.027245714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][87/105]  Time 0.942 (1.001)  Speed 1631.222 (1534.599)  Loss 3.1777470112 (3.1907)  Prec@1 23.324 (22.163)  Prec@5 49.723 (50.128)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 88, lr = 0.02733714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][88/105]  Time 1.082 (1.002)  Speed 1419.202 (1533.182)  Loss 3.1913259029 (3.1907)  Prec@1 21.956 (22.161)  Prec@5 50.195 (50.129)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 89, lr = 0.02742857142857143\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 90/105] loss: 3.2003\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 90/105] loss: 3.1803\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 90/105] loss: 3.1890\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 90/105] loss: 3.1924\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 90/105] loss: 3.1846\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 90/105] loss: 3.1885\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 90/105] loss: 3.1824\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 90/105] loss: 3.1896\u001b[0m\n",
      "\u001b[34mEpoch: [2][89/105]  Time 1.012 (1.002)  Speed 1517.354 (1533.003)  Loss 3.1867885590 (3.1907)  Prec@1 22.233 (22.162)  Prec@5 49.463 (50.121)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 90, lr = 0.02752\u001b[0m\n",
      "\u001b[34mEpoch: [2][90/105]  Time 1.110 (1.003)  Speed 1383.668 (1531.166)  Loss 3.1783306599 (3.1906)  Prec@1 21.842 (22.158)  Prec@5 50.163 (50.122)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 91, lr = 0.02761142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][91/105]  Time 1.125 (1.004)  Speed 1365.098 (1529.122)  Loss 3.1925578117 (3.1906)  Prec@1 21.696 (22.153)  Prec@5 50.553 (50.126)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 92, lr = 0.027702857142857144\u001b[0m\n",
      "\u001b[34mEpoch: [2][92/105]  Time 0.594 (1.000)  Speed 2585.702 (1535.944)  Loss 3.1795492172 (3.1905)  Prec@1 22.038 (22.152)  Prec@5 50.130 (50.126)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 93, lr = 0.027794285714285716\u001b[0m\n",
      "\u001b[34mEpoch: [2][93/105]  Time 1.105 (1.001)  Speed 1389.857 (1534.210)  Loss 3.1876006126 (3.1904)  Prec@1 22.087 (22.151)  Prec@5 49.544 (50.120)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 94, lr = 0.027885714285714288\u001b[0m\n",
      "\u001b[34mEpoch: [2][94/105]  Time 0.953 (1.001)  Speed 1611.579 (1534.994)  Loss 3.1882047653 (3.1904)  Prec@1 23.145 (22.162)  Prec@5 50.960 (50.129)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 95, lr = 0.027977142857142857\u001b[0m\n",
      "\u001b[34mEpoch: [2][95/105]  Time 1.023 (1.001)  Speed 1501.233 (1534.631)  Loss 3.1849753857 (3.1903)  Prec@1 21.175 (22.151)  Prec@5 51.351 (50.142)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 96, lr = 0.02806857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][96/105]  Time 0.953 (1.000)  Speed 1612.237 (1535.401)  Loss 3.1930389404 (3.1904)  Prec@1 22.412 (22.154)  Prec@5 50.326 (50.144)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 97, lr = 0.02816\u001b[0m\n",
      "\u001b[34mEpoch: [2][97/105]  Time 1.008 (1.000)  Speed 1523.975 (1535.282)  Loss 3.1962594986 (3.1904)  Prec@1 21.484 (22.147)  Prec@5 49.447 (50.137)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 98, lr = 0.028251428571428573\u001b[0m\n",
      "\u001b[34mEpoch: [2][98/105]  Time 0.713 (0.998)  Speed 2154.735 (1539.799)  Loss 3.1886284351 (3.1904)  Prec@1 22.233 (22.148)  Prec@5 50.195 (50.137)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 99, lr = 0.028342857142857145\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100/105] loss: 3.1852\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100/105] loss: 3.1592\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100/105] loss: 3.1837\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100/105] loss: 3.1926\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100/105] loss: 3.1808\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100/105] loss: 3.1864\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100/105] loss: 3.1888\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100/105] loss: 3.2089\u001b[0m\n",
      "\u001b[34mEpoch: [2][99/105]  Time 1.082 (0.998)  Speed 1419.443 (1538.482)  Loss 3.1677858829 (3.1902)  Prec@1 23.014 (22.157)  Prec@5 50.830 (50.144)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 100, lr = 0.028434285714285717\u001b[0m\n",
      "\u001b[34mEpoch: [2][100/105]  Time 1.060 (0.999)  Speed 1448.830 (1537.530)  Loss 3.2203257084 (3.1905)  Prec@1 21.598 (22.151)  Prec@5 50.326 (50.146)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch = 2, step = 101, lr = 0.028525714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][101/105]  Time 0.910 (0.998)  Speed 1687.206 (1538.882)  Loss 3.2020938396 (3.1906)  Prec@1 21.745 (22.147)  Prec@5 50.277 (50.147)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 102, lr = 0.02861714285714286\u001b[0m\n",
      "\u001b[34mEpoch: [2][102/105]  Time 1.012 (0.998)  Speed 1517.420 (1538.668)  Loss 3.1916179657 (3.1906)  Prec@1 22.298 (22.149)  Prec@5 50.293 (50.149)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 103, lr = 0.02870857142857143\u001b[0m\n",
      "\u001b[34mEpoch: [2][103/105]  Time 1.013 (0.998)  Speed 1516.100 (1538.446)  Loss 3.1998703480 (3.1907)  Prec@1 21.794 (22.145)  Prec@5 50.081 (50.148)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 104, lr = 0.028800000000000003\u001b[0m\n",
      "\u001b[34mEpoch: [2][104/105]  Time 0.755 (0.996)  Speed 2034.413 (1542.061)  Loss 3.1916184425 (3.1907)  Prec@1 21.973 (22.144)  Prec@5 50.016 (50.147)\u001b[0m\n",
      "\u001b[34mepoch = 2, step = 105, lr = 0.028891428571428575\u001b[0m\n",
      "\u001b[34mEpoch: [2][105/105]  Time 0.174 (0.988)  Speed 8803.756 (1554.271)  Loss 3.1788201332 (3.1906)  Prec@1 22.818 (22.147)  Prec@5 50.027 (50.146)\u001b[0m\n",
      "\u001b[34mTest: [10/26]  Time 6.507 (6.284)  Speed 236.049 (244.429)  Loss 3.3799 (3.3915)  Prec@1 21.703 (22.363)  Prec@5 49.828 (49.877)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.logs_for_job(estimator.latest_training_job.name, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = estimator.model_data.replace('model.tar.gz', '')\n",
    "print(artifacts_dir)\n",
    "!aws s3 ls --human-readable {artifacts_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model'\n",
    "output_dir = './output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $model_dir\n",
    "!rm -rf $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "!aws s3 cp {artifacts_dir}model.tar.gz {model_dir}/model.tar.gz\n",
    "!tar -xzf {model_dir}/model.tar.gz -C {model_dir}\n",
    "!aws s3 cp {artifacts_dir}output.tar.gz {output_dir}/output.tar.gz\n",
    "!tar -xzf {output_dir}/output.tar.gz -C {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "\n",
    "with open(os.path.join(output_dir, 'model_history.p'), \"r\") as f:\n",
    "    model_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(history): \n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(18, 4), sharex=True)\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.plot(history['epoch'], history['losses'], label='train')\n",
    "    ax.plot(history['val_avg_epoch'], history['val_avg_losses'], label='validation')\n",
    "    ax.set(\n",
    "        title='model loss',\n",
    "        ylabel='loss',\n",
    "        xlabel='epoch')\n",
    "    ax.legend()\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.plot(history['epoch'], history['batch_time'], label='train')\n",
    "    ax.plot(history['val_avg_epoch'], history['val_avg_batch_time'], label='validation')\n",
    "    ax.set(\n",
    "        title='model batch_time',\n",
    "        ylabel='batch_time',\n",
    "        xlabel='epoch')\n",
    "    ax.legend()\n",
    "    \n",
    "    \n",
    "    ax = axes[2]\n",
    "    ax.plot(history['epoch'], history['top1'], label='train')\n",
    "    ax.plot(history['val_avg_epoch'], history['val_avg_top1'], label='validation')\n",
    "    ax.set(\n",
    "        title='top1 accuracy',\n",
    "        ylabel='accuracy',\n",
    "        xlabel='epoch')\n",
    "    ax.legend()\n",
    "    \n",
    "    ax = axes[3]\n",
    "    ax.plot(history['epoch'], history['top5'], label='train')\n",
    "    ax.plot(history['val_avg_epoch'], history['val_avg_top5'], label='validation')\n",
    "    ax.set(\n",
    "        title='top5 accuracy',\n",
    "        ylabel='accuracy',\n",
    "        xlabel='epoch')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "plot_training_curves(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store hyperparameters model_dir output_dir artifacts_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check training results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "display(df_stats)\n",
    "\n",
    "#model.load_state_dict(torch.load('./model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
