{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modlue 2. Training with CutMix\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amzaon SageMaker API 호출 없이 PyTorch 프레임워크 자체 구현만으로 모델 훈련을 수행해 봅니다. PyTorch의 문법 및 용법에 익숙하신 분들은 이 모듈을 건너 뛰고 Module 3으로 곧바로 진행하셔도 됩니다.\n",
    "\n",
    "훈련을 원활하게 수행하시려면 GPU가 장착된 SageMaker notebook instance를 사용하셔야 합니다. (예: `g4dn.xlarge, p2.xlarge, p3.2xlarge`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time, datetime\n",
    "import gc, glob2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch, albumentations를 최신 버전으로 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U torch \n",
    "#!pip install albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Splits Data\n",
    "\n",
    "Metric 평가를 위해, 훈련 데이터셋/검증 데이터셋을 분리합니다. 본 핸즈온에서는 훈련 데이터셋을 5-fold로 분리 후 4번째 fold를 검증 데이터셋으로 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input/train_folds.csv')\n",
    "\n",
    "num_folds = 5\n",
    "vld_fold_idx = 4\n",
    "trn_fold = [i for i in range(num_folds) if i not in [vld_fold_idx]]\n",
    "vld_fold = [vld_fold_idx]\n",
    "\n",
    "trn_idx = train_df.loc[train_df['fold'].isin(trn_fold)].index\n",
    "vld_idx = train_df.loc[train_df['fold'].isin(vld_fold)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Transformation w/ Augmentation\n",
    "\n",
    "본 핸즈온에서는 `albumentations` 패키지를 사용하여 Data augmentation을 수행합니다. 물론 `torchvision.transforms`에서도 이미 augmentation을 지원하고 있지만, 더 많은 augmentation 기법들을 지원하고 있으며 수행 속도 또한 훨씬 빠릅니다.\n",
    "관련 내용은 아래 URL을 참조해 주세요.\n",
    "\n",
    "- [Albumentations: fast and flexible image augmentations](https://arxiv.org/pdf/1809.06839.pdf)\n",
    "- [migrating_from_torchvision_to_albumentations.ipynb](https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Rotate,HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    ")\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "\n",
    "train_transforms = Compose([\n",
    "    Rotate(20),\n",
    "        OneOf([\n",
    "            IAAAdditiveGaussianNoise(),\n",
    "            GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            MotionBlur(p=.2),\n",
    "            MedianBlur(blur_limit=3, p=0.1),\n",
    "            Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        OneOf([\n",
    "            OpticalDistortion(p=0.3),\n",
    "            GridDistortion(p=.1),\n",
    "            IAAPiecewiseAffine(p=0.3),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            CLAHE(clip_limit=2),\n",
    "            IAASharpen(),\n",
    "            IAAEmboss(),\n",
    "            RandomBrightnessContrast(),            \n",
    "        ], p=0.3),\n",
    "        HueSaturationValue(p=0.3),\n",
    "    ToTensor()\n",
    "    ], p=1.0)\n",
    "\n",
    "\n",
    "valid_transforms = Compose([\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make DataLoader\n",
    "PyTorch는 미니배치를 쉽게 로딩할 수 있는 `torch.utils.data.Dataset`과 `torch.utils.data.DataLoader`를 제공하며, 이를 사용하여\n",
    "미니배치 셔플링(shuffling), 병렬 처리를 쉽게 구현할 수 있습니다.\n",
    "\n",
    "또한, `torch.utils.data.Dataset`을 상속받아 사용자 정의 Dataset을 아래와 같이 쉽게 구성할 수 있습니다.<br>\n",
    "사용자 정의 Dataset 구현의 기본적인 뼈대는 데이터셋 초기화에 필요한 `__init__(self)` 메서드, 데이터셋의 총 샘플 수를 리턴해 주는 `__len__(self)` 메서드, 특정 1개의 샘플을 가져오는 `__getitem__(self, index)` 메서드의 구현입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Images\n",
    "parquet 파일을 로딩하여 dataframe로 저장한 후, numpy array로 변환하여 전체 데이터셋을 메모리에 저장합니다.\n",
    "본 핸즈온은 데이터셋 크기가 크지 않아 전체 이미지 데이터를 메모리에 올렸지만, 수백만장 이상의 대용량 데이터 사용 시에는 데이터셋을 TFRecord, RecordIO와 같은 직렬 파일로 인코딩하여 저장하거나 각 이미지를 pickle로 저장해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(data_dir='input', data_type='train'):\n",
    "    files = sorted(glob2.glob(f'{data_dir}/{data_type}_*.parquet'))\n",
    "\n",
    "    image_df_list = [pd.read_parquet(f) for f in files]\n",
    "    images = [df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 1min 41s, total: 3min 17s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "imgs = get_images(data_type='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset 클래스 정의\n",
    "`__getitem__(self, idx)`을 통해 데이터셋 전체를 메모리에 올리지 않고 특정 샘플이 필요할 때에만 불러와서 읽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BangaliDataset(Dataset):\n",
    "    def __init__(self, imgs, label_df=None, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.label_df = label_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_idx = self.label_df.iloc[idx].id\n",
    "        img = (self.imgs[img_idx]).astype(np.uint8)\n",
    "        img = 255 - img\n",
    "    \n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']        \n",
    "        \n",
    "        if self.label_df is not None:\n",
    "            label_1 = self.label_df.iloc[idx].grapheme_root\n",
    "            label_2 = self.label_df.iloc[idx].vowel_diacritic\n",
    "            label_3 = self.label_df.iloc[idx].consonant_diacritic           \n",
    "            return img, np.array([label_1, label_2, label_3])        \n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "trn_dataset = BangaliDataset(imgs=imgs, label_df=train_df.loc[trn_idx], transform=train_transforms)\n",
    "vld_dataset = BangaliDataset(imgs=imgs, label_df=train_df.loc[vld_idx], transform=valid_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe92a0030b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADlCAYAAABd5zyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztvWtwHFd25/m/WVlvvAiAAEEAJCGS4kukSIqkJEokJerFltwtdbSlUI/t1Uz3hnoj7Nl5xrh7+oPnw2yE1+sdr9cxYy931Gv1hsPql7pFrSRKoixREiXxTYlP8AUSBEAQJAigANQzq+5+yLoHNwtVQKHwrML5RTBYlZmVefNW4eS5/3vuOUJKCYZhGKZ0MWa7AQzDMMz0woaeYRimxGFDzzAMU+KwoWcYhilx2NAzDMOUOGzoGYZhSpxpM/RCiD1CiFYhxGUhxI+n6zoMwzDM2IjpiKMXQrgAXATwFIAOAEcBfF9KeW7KL8YwDMOMyXR59NsAXJZSXpVSxgG8AeD5aboWwzAMMwbmNJ23EcAN7X0HgAdzHWwYhnS5XPS+kFGGlBJCiAl/rlDGu5ZqT6570bdnnkvtm8n7mQoy7zmfPspGsd33dDDW72Mq0M+Zz99b5m9ZCIFUKjVqn2EYo47Ldi9Sypz3mNmeQu+/WP+OgPzsmRAClmXdkVIuHO9802Xos7XQ8e0JIV4F8Cpg/zjKysqyniiRsAAAbnfupqpjFJnHqodIMpnMenw+5Dqnft5s2wq5FjN5xvq9TAW5vtepuu5c+d3o9zPZNmX2zVy5x6kmV5+53abj/XjPH9McOY9lWVm39/f3X8+nTdMl3XQAaNbeNwHo0g+QUu6VUm6RUm4pxicuwzBMsTBdbs9RACuFEC0AOgG8DOCfFXIi9XR0uVyIRmN5fSbzKaq866kgkbDg83npvX7uzOsU6rFMhzdaqt5TKjXS54bhGuPI3Exl30zG05/r39FUevelTK6+mWif6V58PtvHYloMvZTSEkL8CYD3AbgA/ExKeTafzyYSFv2g9NcTNdZj/WFl2zfel6DvH6stc/UPoBT+SFOpJBlz/TVgG3ll9A3DNeaQWWe6JR5FPhLkXPuOxmtrvm3MdR52aLJjmmZBxnzMc07p2TSklO8CeHe6zs8wDMPkx8y4MxNEfyrrnlAhHk+mZzdV7StkVDCR8wPT522O1Y9zzavUyfTgc23LZKz7mKl71Eep2bbN1MhiIszVdpU6U+3NA3PE0Esp85JOJjJczHbcdGmxYw1NJ3vNqWhzpvHW3+cjJcw1gz9dTMX3lc81xts21/t7spLNRM/DTB7OdcMwDFPizBmPXo+eyEUigbyOAwqPwCgE3TPJNSE42fNO5XnYk8pOvlLFdPTfXP1OJiMjZrunYpWCZmK0N50UVa/na+Qneux0UOj1sz2gpvpe8nkITuaa2SJj1Gv9QTgXmco/Zv3+sz1EitlwZJJvmGux3nOxtltRVIZ+PjATD6jpvoZ+/nxeZ2Osh4X+Wn9wZL7O5xzTzWRHeBNp43Q+PLO1PVfbMo3+bD3cM9s3V52LmYA1eoZhmBKHPXpmTjLRUUG214WOJqaDUhipFTISyuz3XF71XOqfmZBPx0LPmTVVsKFnGCYvpsLYzfbcWT4UQxsnCks3DMMwJQ4beoZhmCJkIvm/2NAzDMOUOGzoGYZhShw29AzDMCUOR90wDMPMAaYjrFLBHj3DMEyJw4aeYRimxGFDzzAMU+KwRs8wDJNmNlM0TCfs0TMMw5Q4BXv0QohmAD8HsAhACsBeKeVfCyGqAfwCwDIA1wC8JKXsm3xTSxcp5Ww3gZllhBCz3YR5y3xIXzwZj94C8O+klGsAPATgj4UQawH8GMBHUsqVAD5Kv2c0pJSOfwzDv4e5QyqVHPUPsB8IxfpQKNjQSylvSilPpF8PAjgPoBHA8wBeTx/2OoAXJtvIUoD/kJl8YUdg7qEb+GI0+FOi0QshlgHYBOAwgHop5U3AfhgAqJuKazAMwzCFMemoGyFEGYDfAPjXUspQvlqjEOJVAK+mX0+2GXMS9saYqUD/HZXq38psMlapQ32b1+sBYGeNtKziisKZlKEXQrhhG/l/kFK+md58SwjRIKW8KYRoANCT7bNSyr0A9gKAYRglYxFLybjnWpKdKz2qy+WaUOrUiV53upiKNs8UbPSnj7FqzJqmC4ZhpLcbRWfoC5ZuhP0rew3AeSnlf9F27QPwSvr1KwDeKrx5DMMwzGSZjEf/CIA/AnBaCHEqve0/AvhzAL8UQvwQQDuAFyfXxOJhKrx5l6uwrySZtOizaohpb7c9D90DcbtHrqHarLxD0zTTx1tIJBJZr+XxOM+ve8T6Po/Hg3g8TsfpnvpYo4J4PD6uV6/2q/OMd7x+Pf0es50rH3JdXz/HWG3K1h8THckkk0n26qeRzBq3qVRqFlszOcRckBoMw5A+n2+2m1EQU91/EzH0Xq8Hfr8fgP0jDIfDtE8ZXNM0IaVENBoFYLdX7VNGGLANvWmaNDzVf9SWZTneq2Myj1P7lCF1u93UP5ZlwTAMuqZpmmRoU6kUAoEAfd6yLEfb1AMn0xD6fD66vmma8Hq9AIBIJAK3241YLDaqzdFolK4VDocRCAToHIZhjLp/fZ++TbXd6/XmfEBYluX4fajPZN5/tvf5ol+bjf7UMxsrZfN94CeTSUQikeNSyi3jHcsrYxmGYUocznVTIHNhJBSLjUgcpmmSp657ebrnCdjeqPKQMyf24vG4Q3pRZEodqVTKIVlkfkZ545Zlkffs8/lgWRbUyE0fJeijANUGNQIxDAPBYJDOrV9LCEH3UFVVhfvvvx8AsGTJEvj9fly+fBkA0N7ejkgkQv1QW1sLAOjs7MSNGzfQ39+fpXfzQ7VzPHw+HyoqKuj7klI6RhyFePOAUzLKlOGYyTMbOW4KkfHGgw19AcwFIw8AgYCfDLhhGPTjiMViZDgBp1HVdfexJBjAKe1kGnN1LSW1qGP0c8bjcYcBi0ajDtlowYIFAGwjqCSoRCIB0zRRXl4OACgrK0N9fT3dV0VFBQBg0aJFWLlyJb33+XxYunQpANvQu91utLe3AwC6u7vpXjweD6qrqwEAN27cwPXr10ny0h9gqs/U/ehzFqlUCt3d3QDsP8ry8nLqj3g8jqGhIWrT7du3AQBXr15FT0/WADT4fD54vV76LqPRaMGGH7B/n2zsi5upjgRjQz9B5oqRV6j26EZKR+nuynC43W7HPrVdSgnLshxeu0LX1tV7/UGi9qVSKZim6dinzhMIBFBfX08efWVlJRobGwEA1dXVaGlpAQD4/X643W4yvn6/nwy9PkKoq6vD6tWrySvu7u6mh0goFHJ47s3NzWTcQ6EQGe/Vq1ejsrISAwMDAOyHjGqvZVkQQjhGSYODgwDsh9yVK1fo/qqqqsjQW5aFUCgEAAgGg+js7AQAfP311+js7HQ8VPr67BRQN27coDYA7JEzUw9r9AzDMCUOe/RFTDgccUgSypM2DIM0aSU3lJWV0XG6t65LMDp6lAnglHFSqVRWWUeP2lHvlRyxfv167N69GwsXLgTglGsMw8D69etpu8fjIQ83EAjQ+ROJhEOvD4VCuH79OgA70qauro7aU1lZSbLJ7du3qR8GBwepb6LRKGpqamiUI4Sge04mk/B4PLRPH8X4/X6sXbsWgK35m6ZJIwtdcvF4PHS/y5Yto8gjwJahrl27BgA4efIkjh49io6ODmojw0wlbOiLGNN0ZTXU5eXlWLVqFQCgtrYWhmGQXn39+nWHTJALn8/nkGD0B4lu6LOFV+oThMow33ffffjBD35ARuzGjRtkfBOJBHp7ewEAFRUViMfjpL0nk8mssfjqIaJr+8rIXr58GZcvX6Y5gLKyMjQ3N2e9T12HnwxKNovFYg55T7WvpqYGLpeLpCuPx4OtW7cCAJYvX45gMIi3334bAEjSYZipgqUbhmGYEoc9+iJGzy0TDodx3333AQCefvpprF69GgDQ2NgIwzBw7NgxAPakoJoQtCyLIkSSySTcbje9HxwcJA95eHgYd+7cyXpdAA75SE3IAk5vPxKJ4OzZs3Ts4OAgeeeJRAI3b94EYHvq4XCYPPxQKITh4WE6TklQqh26hKRe9/T0IBQKYdGiRQBsr1pJKIZhkFetFpOp9urSk1o8pfbpsg4wMgne2NiIVCpF/dbU1ERtTKVSJB8NDw9jwYIFNLKor68nb//hhx9Ga2srRRoxzFTDhr5IMU0XYrE4li+/BwCwYcMGPPLIIwCAF154gSJOlKFSmnJbWxsZqaGhIdy9exeALTn4fD4ysN3d3RQ9cvv2bXR3dzsiRtRDIB6Pkz6tXqvj9FDOL774AkNDQ3jooYcA2MZNxa+fOnUKH374IQD7gZBMJh3XVkZU/a/IfOCoe1bhiUq6GRwcJOMejUbJmKuVteq9buhdLheEEPQw0uOaDcOgyJhly5YhlUqRJNXc3EyRQclkkl4vWrQIS5cupc+lUimStTo6OtDZ2Zl3TD7DTBSWbhiGYUoc9uiLDNMc8Sw3b96El19+GQDw1FNPoampCYAtVeh5WYQQ5O2Wl5eTF5xMJskbTyaT8Pv9tE+XTHp6etDf3++I5FEe/eDgIE0ednd34/bt2/T+zp07dP7e3l4cP36cPHV9odKVK1dw/PhxALbX7vP5HCtoldzx6KOPkhfsdrthGAZ50nfu3MGBAwcAjEg4qr2RSIQ89Xg8TvcYjUYLWoWoLx7r7+93RDkNDAyQTLRgwQIsWbIEALBmzRrU19dTFJLf78fFixcBAPv378ehQ4doNFVo3huGyQUb+iJmy5YteOmllwDYC5CUsdHTHKhMkMq4dXd30z6V9gCwNWev14vKykoAtiFVr6uqquB2u8lI6RE4sVgMXV1dAICuri709fWRxNLf3++Qe4QQDoOrjNm6deuwfPlyALahDIfDtMApEAjQwqo/+IM/QE1NDQD7AeByueiB1traSitPjx07hkgkQtd2uVwOWUQ37HoKh0zjmitboW6IVZ+r/lixYgUefPBBALZef++99wKwNfl4PE6ymcvlon5ra2vDrVu3cmYLnW5U4i59uX+2bUzxwoa+iEkkEmRo3G43LctXRhWwPXXd8Jum6Yhl1/X0iooKRwZI5X1LKUfF3itDp6c1WLp0KdatW0fnUNcDbF07FovRg0WtXgVsbV3dR1dXFwYGBsjwl5WV0XH9/f1kzMPhMIQQZPjr6+uxePFiR//o+XjGSmecbTXwWESjUcexHo+H5kB27NiBxx57DIC94ldfA9DX1+cYTakJbiEEvF6vI91Cvkx2qXxmLdSx9ivY+BcfrNEzDMOUOOzRzxHyyUOv6/MAcO7cORw+fBiArV8rb1lp64Cdb0WXOAYHB8mTNAzDsSq0r6+PvGeXy0Xhfn6/H/F43JEHRoUN3r17l7xkr9dLMg9ge75q4ZPX68WtW7dIs29oaKDzG4ZBIYn33nsvpJR0/r6+PhoV9Pb2UttTqRSqq6tp7qG3t5dGNHrIJeD0eqcqWZSeP98wDMqc+cgjj5Aur0ckxWIxDA4OkqxlWRZp8rdv30YkEilIly+kaIrd7rHnJTLrqGYW4WCKCzb0RYplJXHu3DmcPXsWALBr1y4ylolEgoydy+XCggULKJTx0qVL+OabbwAAt27dovMpeUNJPpWVlZRoTGn16v+WlhZHPLsy7NXV1Q45KBKJkPGNRCIYHh52VLLS48aVbKHkqLa2NvqcijdPJpN0bq/Xi2AwSO2orKwkzXuqyEznoAyxntohmUxiyZIltG5BhVsCI8YdsOcehoeHqf36JLAQwpFQbbYmY7NJMvq2bBp+rs8xcwuWbhiGYUoc9uiLDL32ayQSIS+2v7+fPPD6+nqafPV4POjr68OpU3ZZ33PnzlFYn150IxqNIh6Pk0xSXl5OScKU/KM88NraWvLuq6urKfxx8+bN8Hq9NJqor6+n8w0PD6Orq4smXfW8L0IIakckEkEikaB9iUSCPN/a2lpqU01NDcLhMN555x0AdoimGj1MhInUbdVX0CqpxOv1YteuXdiyxa7mpq+SHRgYcISvplIpun8hBFauXAnADl/t6elxSG6zzVhRN4bhcsiI8Th79LPBRMKCJ23ohRAuAMcAdEopf08I0QLgDQDVAE4A+CMpZXysczD5YVlJxx+Y2+3GmTNnAAC/+tWvsGvXLgB2/LYKJ/R4PDh+/DgZxI6ODkdIZWaFKl1CUTnXR64/Iicojbq2tpY06a+++gpVVVWO3PfqOJfLBcMwKEpm6dKldK2+vj7S3sPhMCoqKuhzy5Yto2veuHEDn3/+OR0XCoUoEdi1a9fouoZhTEizznZs5ja9+paet3/Lli3Yvn07hYACIw+EQCBAhl6vG6D6Rj2Yb926hXPnzpFmX8gK2cxVwmMxltau9qkC87GYSl6X+9yG4WL5Zo4zFdLNvwJwXnv/vwL4KynlSgB9AH44BddgGIZhCmRSHr0QognAcwD+FwD/VtgzbbsB/LP0Ia8D+E8A/nYy12FG0KUbwC5RB9ge/YkTJwDYsef64pvu7m46Ts8Xk1lKcKzSgrmqSrW3t5N8dOzYMUe+GP0cfr8fK1aswJNPPgnA9nCVB9vb20sx+4At+ahJ1vb2dnzxxRcAgNOnT9MK2oGBAbhcLoqrtyzLUaZQZyxJRveCM73izM+pvlu6dCnF7O/atQurV68mDz8QCFDbb968SZPlV69eRXl5Oa1h0Nci1NfXo7GxkfLTT3UZuXwxDBd58pmlI2OxuMNrzxxdMjPPRH4nk5Vu/g8A/wGACp+oAdAvpVRj/A4Ajdk+yDhJJu0uyxZmqfZl264MfyTS5og60fPFq8yUCr24Ri4yDaPH48maF16/FuA0uLqhV8W7n3jiCQDA4sWLSa6prq4m4+jz+SClxHvvvQfATg9w5MgRALakoXLpqyyX2R4qmffl8Xjyulf9vvSFTep/lWLiqaeewubNmwHYefaTyaSj4Lp6+Lzzzjv47LPPANiGvqGhAdu3bwcArFy5kvT65uZmNDU1UXqHicgwE0VJM8pIW1bSEUqZmcRN/W7sugD2cSrUMtPpYOYuBRt6IcTvAeiRUh4XQjymNmc5NGuRVSHEqwBeTb8utBklRzJp5RVTrx+vsKywtsfZ7blSAIw+n9PA6duzrTTVyQwJ9Pl8lMpg69atePrpp3HPPXa2zZ6eHmpHMBgkI1dZWYl9+/bh9ddfB2BntlQhiTrq4aKPXPSJXo/HQ+/11/r9q4eXuhd9TiEejzv2rVy5Es899xwA4Nlnn8WaNWsA2Ct89TDKb775hkZWhw4doonvcDiMvr4++pzP56NzV1RUoK6ujia79Yeq6nsdNbLS+3uiD4bxjHTmnELm8RxXX1xMxqN/BMB3hBDPAvABqIDt4VcJIcy0V98EIGtws5RyL4C9AGAYxtyquM0wDFNCFGzopZQ/AfATAEh79P9eSvkHQohfAfh92JE3rwB4awraOa/IJdXk+xmXyxzTwytEFhhrdWlm5I7yAoPBIDZt2gQAePLJJ7F582ZHIQ7Fp59+SjJOU1MTDh06ROGgmdp7vm1X4ZmZbdRr4fp8PsTjcZInfD4fjS5VlM26desA2Dls9uzZA8BOwqaPbqSUtEL5yy+/xIULFwDY8wsqk6eqg6uicICREcjdu3cRDAZRVVXlaFc29HmSQhZWZXrimdEy4bBK0hZPL+Sysh6XeU5mbjMdcfR/CuANIcR/BnASwGvTcA1mDAp5UEwVhmHQStaWlhbs3r0bgD1pefPmTTKkgUAAn3zyCQDgd7/7HcXRL1y4kIpkA7bRUxO1+Rr5ZDKJaDRKMow+0evz+UjGUsZcGc/BwUGST+655x7U1dXhmWeeAWBr8SqdAzBSCzYWi+HgwYM4ePAgAHudgirmkkgk6CFlGAbi8ThJPOFw2CE7VVVVob6+HgAcSeEy79myLIexL4Rchlnfnk9sPBv44mFKDL2U8hMAn6RfXwWwbSrOyzAMw0weXhnLTBnKa1USxObNm7Fx40YAtuerVocCwMmTJymyRoUgArbcoXu6qjjIRLEsi64VCAQci7j0oiyWZZGE0tTURKtVH374YdTU1FAd3sWLF9NIYHBwkOSe/fv348CBAzh/3l5KoqQa1R966mHTNClqaHBwkCKNDMPAsmXLaOHZggULKPRUZ7bCLpnihw09M2WkUinU1NRQbvYNGzZQdsnh4WH4/X58+eWXAID33nuPwib10EiVhkFtU4VTgPwNncvlgmmaZJgDgYAjz77Sv6PRKBYtWkSx7Xv27KGVuPfccw/cbjeteLUsi4z0Bx98QCGU586dQ2trq0Me0nVzvdBKIBCgz925c4ekmmAwiCVLllAO/kWLFlGobDgcJqkmHo876tiqhyfDjAcb+gkihKDQM8bJggULsGPHDkrFsGXLFsek6Jdffklx5adPnyZDnE1z1rcpnVsPkcwH/RxqEtSyLBoxrF69Gg888ABNuD7++ON03PDwMEzTpEnjffv20UTymTNn6PXAwICjXZkPLV2jB0AGvLe3l/Y1NDSgsrISGzZsAAAcP34c169fp3Po/ZBKpRyT37kMvZSSw5ZLnInkuuHslQzDMCUOe/QFwF69ExWpsnr1auzYsYN0+VQqRR7poUOH8NZbb1EKAxVlA9jerp6tMtNzzxVqOB56ARQ1eqioqKA6ri+88ALWrFmDpUuXAnAuBKutrUVnZyf2798PwJZrlHTT399PowIVrqk8+cxUEQoVCqnOEYlEKMFbY2Mj3G43Vq1aBcAO31TpEOLxOH3GsizHwjXVZiC7rKV+o+zZM2zoC0T98ZSqwc9WQDubdOJyuchQ7tixAy0tLRRCGI1GyWAdOnQIhw8fdhh4XcvWM08CGHNlaD7oxjaVSqGhoQGAnW7gO9/5DgBg+/btWLBgAZ1/eHiYHjitra34/PPPKc/OlStXHCtSM425nidIb4O6L/UZPYZfafRqslidY+PGjTSpa1kWvv76awB2f0opJ9wfLOOUJhP5HbB0wzAMU+KwRz9JdE+plLx75cUDoIyUyjvVi4EsXLiQVr+uX78e5eXlDo9e5cu/cOGCw4PXV7nq4ZQTjbDRV6jqGIZB562srMTjjz8OwA75fOihhwDYi55U7VnAnmRVk6VHjx7FiRMnHJ61Hv2ik5mrXnnten5/lR1U7UskEiQnJZNJkmcAO6+Ouq+Ojg5cvnwZgC0ZZebaz7ef9N8me/fzDzb0U0gpafe6AVHx8XpEiQpXbGlpwdatWwHYVZ9SqRQlKGtra6MY+evXryMWi5GRyVUcpNBwwcyMmlJKaseWLVvw7W9/G4Ctf9+8eROA/QALBAJk3D/88ENK53zlyhWEQiEy1D6fz5H1U0+gZmd2HC016Q8A9V49LDo7O3Hu3DkAdlrpcDhM7S8vL6fX+oMjUx4qtK9Yypl/sHTDMAxT4rBHP8WUopRjWRZcLhd5yMPDw7TIaNu2bVixYgUA28uPRCIUe3758mW0t7cDsPtCl1Omk2QyCcuyaAL2xRdfJLkmHA6TZBKNRtHR0UGjjn/6p39y5KLxeDwkN+meuR6dkyuts9qme/r6iKitrY0mqteuXYtYLEZ1eC3LciRG09M0Z8pck/HqFezdlz5s6KeRYjb6fr+fZIZoNOpIHRAMBmn167Zt2xyrX4eGhnD69GkA9sIfZZgqKioQi8Uo5FHP4jjVqJWxixYtAgA89thjZKhv3LhB99XT04MjR45QyGc8HidJKpVKOQqbANkXduWL0ufV+bq7u0mXz5RmpJRk3CORCBnzzKpfUwUb/dKHDf0MUWxGX089kEqlHIa5ubkZDz/8MAC7eLe6HyklpJS4desWAHsVqF7ZKhaLTauBVwYxEAigoqKCRiBlZWVUBlAvwOLxeLBmzRrKShmJRMijP3HiBDo6OuhzqVSqIEOve/wq4yVgjxh0A69756ZpUkz9kiVLaDIWcObqmY70B5m/TTb8cxdeGcswDMMQ7NHPAple0lz08JPJJHnfen50wC5mvXr1agC2h6lCEIUQsCyLMi9Go1GSbkKh0ISLZEwU3cPR23Hs2DFaDet2u6m/q6ursWLFCrpP0zRpNFJXV4ezZ89SWcBQKDQp79blcsEwDBpRlJWVORZgxeNxWqzlcrnw4IMPAgAOHjxII5P+/v5RtXunG5Z1SgM29HOAuWn4BckupmkiEomQ9LBw4UJKRayKjwN26t3W1laSGoaGhmifMqbOe51qwzFSHSocDuPGjRsAgN/+9rc0GVtRUYGWlha6L5fLRfe1aNEi2ldeXo7169fT/MOxY8foIVCIgdXDMwH7gZO5ZkCXylQ/qfkGtb3QdBBTAcs6cwteGcswDMMQ7NHPQeaCh59MWlQ/1O02IaWk3CwbNmygohlSSkd+9MuXL1MpPcuyyJOPxWIz5o1aVtKRQGz//v24cuUKAGDFihV47LHHANgLqfRiKJFIhO5l6dKlWLZsGerq6gDYKZhVucDr16/TJG2+qCInuhemvmev1+uQuSzLckwe69//dEXeFEK23yV7+XMTNvRFwFRE7Lhc+X3V2erNSinh8XjI0Dc1NZGs43a7HcZcN5zRaJRWkMZitpE3zfwjBSZC5v3F43EKVxweHqbCIJcuXaJc8qFQCFVVVVRcRMWxA7bsVFFRgW3btqXbbZJWfu7cOZKn2tvbRyV70+cKlGHPTISm0kro11WGXtfrfT4f9bWuz89VWNOfOTjqhmEYhiEm5dELIaoA/HcA9wGQAH4AoBXALwAsA3ANwEtSyr4cp2AmSCHefb7e/Fgkk0nKO79w4UJ67fF4KOrm4sWLuHjxIkkmE5U38muHc8Qx1r3pq0iVRx8KhWjlbmdnJxYvXkyVnTZv3oz169cDsCdtdfln5cqVNGn74IMP4tNPPwUAnDp1CmfOnHEkJdMTnCmvXVWHcrbd6fmriJxwOEy56hcsWEBx/vpK3fFxetPZRmrTDefDn14mMhk7WQvw1wD2Syl/XwjhARAA8B8BfCSl/HMhxI8B/BjAn07yOkwWpjuJmpJZlGSgpIva2loy9OFwmKSQU6dO4eLFi2RIp5qJGqtcfwi5v3kdAAAgAElEQVTqwfT111/j6tWruHTpEgBbe1chmTU1Ndi0aRMlPHO73ZT2oaWlhUJO161bh3fffZdyxvf19eXU0fWMnX6/n87R19eH4eFhR2SSMu7V1dUk7WSurp0IhT7sp+IBwQZ/9inY0AshKgDsBPDPAUBKGQcQF0I8D+Cx9GGvA/gEbOinjekqgOJymeRxGoYBr9dLxj0QCNB1Q6EQGc7e3l5Eo1FY1mgDO13avG6IMo2Z3o5s108mk+jv76cH0/DwMGnvjY2NiEQiWLJkCQDb0Ot1YlXY5QMPPICqqirywI8fP46Ojg4AI/lygNGTqBUVFQgEAnRuYGQEkkgkyJjX1NTQxLeeOhoovPLWbMH6/ewxGY3+HgC3Afw/QoiTQoj/LoQIAqiXUt4EgPT/ddk+LIR4VQhxTAhxbBJtYBiGYcZhMtKNCWAzgH8ppTwshPhr2DJNXkgp9wLYCwCGYcyFFUJFzXTm0kkkEli4cCHuueceAHbEiL5qNnNRlO49q1GBijhRx2Tz+meCbB6xklCGhoZw7Jjtd7S2tiISieD5558HACxfvpw88OHhYbrnUCiEXbt20Tlqa2up/ODZs2cduXUMw3Dkt1fnq66uRigUcnj06vXq1atJMvL5fNMmi800nBN/ZpmMoe8A0CGlPJx+/2vYhv6WEKJBSnlTCNEAoGeyjWQmxlRp97rG3dzcTLVhdWMZi8XQ3d0NwJY09AIgmYWsdUzTVZCxV/LMeNqxZSWzyjV6sjZ90hRwTt5GIhF8/vnnZFh37NhBk7ZNTU10jv7+fkgpsWXLFgBAQ0MDyT1vvvkmFTLp6+ujdM+ArdErySaVSiEQCDhSTajslXV1dbQK2e12O8JGcxVvKRZYu585CpZupJTdAG4IIValNz0B4ByAfQBeSW97BcBbk2ohwzAMMykmG3XzLwH8Qzri5iqAfwH74fFLIcQPAbQDeHGS12BmAd1jDgYDo8L8lPc5ODiIb775hl5norx25WGPVxM20wufiNev2pwtwiTXeTJLJiq8Xi+Gh4dx9OhRAHbu+tbWVgDA448/jmXLlgGwwx89Hg9N1C5dupQKnng8Hnz11VcA7Aifa9eu0ehhYGAAPT32YDcajaKqqsoRjqqnc1Z9lun5zsQq2XxHUMzcZlKGXkp5CsCWLLuemMx5mdnH5TLJ6FqWBa/XS6s1hRCOmqb9/f0A4Ch2bZ9jfMOuyBWVk0viyTTmuiFKJq28wgntSlQjEo9uOFUCMfVAu379Oq5fvw7ALl5y3333AQB2796NTZs20erYmzdv0j1/97vfxeLFiwHYefs//vhjCtfs6urCgQMHANhyz86dO0mzl1I6ZBwVdVNeXl4Uq2OZuQevjGUYhilxONcNMy7JZBIVFRUU/VFeXu6I39erKOnet55iV6HeJ5PZJ0tzTaLa58v9c3W5zJzyQmY8vT7KMM2R2PRsk8f6BK2SU7744gu0tbUBAO7evYvh4WE0NTXRcWph2a1bt8jz37hxI2pqaqg+7dGjR0nW2blzJyzLIs89FotRP+klEWtqauDz+abco9f7e7aioZjphQ39vECPwMk/wkEZRFUYvLq6GoCtZStDn0wmcxb81o2mXiZPnTObnDMiFyUd7zP3j0UyaY1p9Ecbs7hjv9pmWRbJUXrqAdM0KRrn3XffRX9/P7Zu3QrAXimronMikQj1k8/nw4svvkiZM//u7/6ODHYwGHScX5eQhBBYsGABgJFiJerBpBt8VdhEtc+yLEfkVebDTslw5eXlJBnF43FKFTEwEMraf0xxwoZ+3iGRj7G3jaVtVL1eL9xutyNeXhmjoaEh8nRHdPvRBtzr9eZcup8t26My6LrOr+9X51Q4DbErnUkz96pZnWztVefLlVtG3/7FF1/g2rVrAGwtX81ZbNiwgVbM3rlzB0NDQ5Qv57HHHqN7qa6uhmVZpPMbhkHnt0cddtuFsIvBqLDMeDxOr/1+v6OfVCZR+95d0CJiUVlZiRUrVgAANm3aRMVW7ty5g8OH7Wjp06dPo7e3lz38EoE1eoZhmBKHPfp5iRrSj+3ZK2/O6/WipqaGpIBoNEpefHd3N4UF6oulFLoHri+0ykQ/LtODV59zuVyO3O96CGLm+S0rOSoSJ5PsXv74C82EENQOr9cL0zQpIqenp4dy3fT09OC5554DYIdhdnZ2or29HYCdAfTb3/427QuHwySB6Sto4/E45Rjyer2QUtJxQgi6Z7fbTaOszH7R+8bv9+ORRx7BH/7hHwKwF4LpI6MjR44AAP7iL/4C+/e/z2GVJQIb+nnN2Nq9yzVSGCMQCJChj8VitHJzeHh4lEafK/GWvipV1+j1B4TL5RqlPSsyNf3MidvM/fnHgE98FbG6Zykl6d2AvZbg9OnTAOwkb6qfdu/ejfr6empjV1cXae+hUMhR3crn89Fx4XCYEqj5/X4kk0l6yPj9fjLueppkVawk28Pz/vvvx/PPP0/F3Xt7e2lfVVUVnc+eZGcjXyqwdMMwDFPisEfPpHF6tS6XSUP6YDDoKHqRSqUoGiUUCjmKa2SGKOrefeYqVD16RE0qKtlCeZZ6eKZlWaMiTnKNEAq553zJHMFkXk+lbY5Go3jjjTcA2P30wAMPYO3atQCAJUuWUOROeXk5hoaG6H0ikXDk31ETusFgEIZhUJ9EIhGa3HW73Y7vZ2hoiEZgCxYswMqVKwEAjzzyCFauXEn9NjQ0RCOSu3fvUtioysvPlAZs6JkcSNJ6g8EgysrKHJEmyijdvHnTkVExU4bRja/H46FY8ebmZqpBGwgE6NyqzqyKXOnu7s6aWgGwo2uyyRMzkehLj4QBRqJwMgt5q9QIb775Js6ePUua/c6dO7F8+XIAtpYfjUbp4WUYBp0nkUhQjL7H44FpmvRQVPsB29DrWrvb7aYHxI4dO/Dd734XALBq1SrEYjGaR3C73WToW1tb8f777wMArl69OubaBKa4YEM/D8g2SToRTNOEaZoO7Vw9BPr6+uh1PB53pEfQwykrKyuxdOlSbNy4EYBdtk8ZusrKSjJYoVAI/f39uHLlCgDgzJkzlDagt7eXDCdgGzllEEcvdhoxuGPp/IWgG1uVUlg3vsrox2Ix8rjv3LmDwcFBeoAJIfDkk08CsFMbu1wu3L17F4BtfNWDRE/nq7KSqr7SH7x68fHKykpUV1djz549AICXX36ZMo+GQiEkk0la1OXxeOj7u3r1Kk0WG4aBYDBAcfXM3IOLgzMMwzAEe/TzBH3xzHjHZGKaJlKpFHl+uheoe/lqQY/y5KuqqqhYiVoxqjTq5cuXk4yj589PpVJIJBK4//77AQCPPvooefEXL17EJ598AsBe0HP37t2sq0TzZaIjnUy5Rr3OleYhkUiQ7KRGRRcvXgQA/PrXvyZvef369RRZA9iaveoP/b4yi3VkXlf1hc/nw7e+9S1873vfAwAsWrSINHev14tFixZRnvyTJ09Svdtjx47RSGpgYIAXS5UQbOjnGYVIF4FAwBFe6fF4SJIIBoNkfNxuN8rKysi4b926FZs2bQIArFixAs3NzfQ5KSWFBOo1UtUKWpVXp7Gxka67fv16KsIxNDSEwcFBx0SwYiyjnyv1Qj7oUok+IasbXH0iNZN4PE6fO378OKU9vv/++7Fu3TpKMbF9+3Z6rZ/L7XbD4/FQyoJwOEySkRCC+nbPnj146aWXaPXr3bt3afLc4/Ggvb0dBw8eBAD87ne/o+Loer/Zen+MNfo5zER+xyzdMAzDlDjs0TN5obx1wPbGVUbFVatW4csvvwRgT+CtX78e3/nOdwAA27Ztc4QGejweitDRwwmllI789kIIKnJSVlZG121sbMSuXbsAAJcuXUJHRwfJHx6Ph0Iyx5NxcpU61L199Vo/NtdrfbWu2pZ5LcuyHN55Mpkk+evgwYM4cuQIjUqam5tpRCOlpPsqKysjb16hrm2aJmXK3L59O5YtW0ZevD4pbJomzp8/j/379wMAzp0756hrq7ebpZvZYTw5sZARKRt6Zlz6+voQCoVoKX4kEiFDvHDhQgrrq6urw549e/DEE3bdmZqaGqqidP36dYTDYZJrkskkPQQWL15MBsyyLIf8EQqF6IGgp0p+9NFHceHCBRw6dIg+pyi0Bmm2bJvZto31eqxzjnXM0NAQ/YHrkS6GYZAxb2xsRHV1NW7evEn7lXRTVVVFUTw7d+6E3++n1BR+v58eIu+//z5ee+010uL19QxCCHr4xGJc3GQmmchcUSERdCzdMAzDlDjs0TNZcblGcpbfunULnZ2d5Lnr+wKBAE2ItrS04JFHHqEIkrNnz1KUzKlTp9DT00Mebk1NDTZv3gwAuPfee8mjDwaD8Pl8JFd4PB7KCROJREjGWbNmDZqbmx2ed668+MVGe3s7efVSSnR3dwOwPfoFCxZQ3+gTsE8//TR59F6vFwMDA9RXVVVV+OijjwAA+/btQ1dXF3mFiUQia24hr9c7KtUzU7xMytALIf4NgP8R9uqU07CLgzcAeANANYATAP5ISsnjwCJEGYDbt2/jzp07tF2PMjEMg4yDlBKhUIhymh84cIAMfVdXFyX4AuwHxI0bNwDYkoySIILBIEzTpAfJvffei6effhqALQ2ph0owGITf76eHj746N1fUy1xH9cHVq1fpoVVeXk79u2nTJlRXV9M9x2IxWgi1e/duinZSKMnnk08+wc9//nMAtiZvWRY9IA3DoL5OJBIZKSdYoy8VCjb0QohGAP8zgLVSyogQ4pcAXgbwLIC/klK+IYT4OwA/BPC3U9JaZsbQM02Gw2Ekk0kysqlUylHqTmm8ly5dwi9+8Qsy6MeOHSONXgjhyMoYi8VoFeborJPO0cS9994LwJ74VddVE4zK29dT8wohitK7V21ubW2lFbR6QRG/3w/DMOi4RCJB4atqlTEA9Pf3w+/3U96aAwcOUEbNvr4+xyraXCue2ciXFpPV6E0AfiGECSAA4CaA3QB+nd7/OoAXJnkNhmEYZhIU7NFLKTuFEH8JoB1ABMAHAI4D6JdSqrFzB4DGSbeSmRWUlODxeBCJREg3VqXvADsLo1rF+tFHH+Hjjz8mDfn27dujFjTpOWIywxLVdtM0ycvv7u6mMn269HPr1i2UlZXRfEBvby9dt9ilmzt37tC9VFdX0/yFSnamrzxWHv3SpUtRWVkJwJbFUqkUjQr0MFSXy4Xy8nLy3NUxwEgRFfs4iyNvSojJSDcLADwPoAVAP4BfAfhWlkOz5oIVQrwK4NX060KbwUwTLpeLjE00GsW1a9dw7tw5AHZ8vDKm999/PxmbQ4cOob29nYyFntQslUrBMAySCXT5IB6P07F+v99hqCORCC3XP3/+PEk1/f39WLp0KRYvXgzAljvUOYpRtgFG2t3f30+rgfWsoYlEAtFolL6X5557jia0fT4fZaRU4a9nz54FAFy+fNlxnb6+PvoePB4P/f2ZpplRd5YNfakwGenmSQBtUsrbUsoEgDcBbAdQlZZyAKAJQFe2D0sp90opt0gpt0yiDQzDMMw4TCbqph3AQ0KIAGzp5gkAxwB8DOD3YUfevALgrck2kpl59FS58XgcbW1tlJtly5YtDq+7oaEBgL14qqury+EtKlKpFHmi+jUU+kpT0zTp/JFIhOqxtra2Um72QCCA+vp6kjVSqdSM5KGfCfQ0yBUVFZS+WBVeUSOXjRs3UkGRWCxGqYcbGxtx7tw5ymEzNDTkKE3o8XhosjtzVayKXsqsucsUN5PR6A8LIX4NO4TSAnASwF4A7wB4Qwjxn9PbXpuKhjIzS2ZkTTQaJaMgpaSsjEIINDc3AwDWrl2LS5cukZHW5Rn1OV3OUejROCqqREkXpmlSgq94PE7HBQIBXL16Fbdu3aLPlQputztrioXa2lpHuojbt2/T/ZumSRJMdXU1Lly4QIbenl8ZTJ/PckTU6MacjXtxMRHHZlJx9FLKPwPwZxmbrwLYNpnzMgzDMFMHr4xlsqJWXAK2Z97f30+1UBOJBMkykUiE5INNmzbhs88+I88/lUo5YrTdbjdNOKp0xOo4Hb28XXNzMx5++GEAwIYNG0iqSSQSOHnyJE3U6vHlxU4kEqE+6enpoXuW0i7v+P3vfx8AcN9995G3X1FR4ZDarl27RjlxMksx5vLa2ZsvLiZUS2Ea28EUMfoCJIVKhHXx4kWsWrUKgB0KqHTdNWvWYPPmzfRZlVRLocsrsVjModHrMlEwGKRFUjt27KCMlYsWLaKHzYcffoivvvqKwgP1h4Wu8RcTeuipSv7W0NBAi856e3vx05/+lJLB6SGlgUCAwiuHh4cxMDDgCEc1zbEKzhRfXzETgw09kxU930w0GoVlWfjqq68A2J67qhTlcrnIoK9evRo7duzAhQsXANhhgsrgut1upFIpRz1Z9VoIQUZq3bp1ePjhh+lB0tLSQhr98PAwPvvsMwDA22+/jcuXL9P5DcMo+jBdNSIxTZP69O7du3Rft2/fRkNDAx0XDAZJp/V6vTTKunLliqO2rsfjKagCFzO34cIjDMMwDMEePZMVPWLG4/HAsiyqO3r9+nVKcub3+x2a/K5duyjb4q9+9SsKDTQMA7FYzOGBq+RcS5YswTPPPAMA2Lx5M1atWkVzBHYBDPszp06dwgcffADArhkbCoVI7tD1/2KUbQBnsji1MMzr9VIfer1eXL9+ne6vtraW9Hvdu2tra0NnZycdl0wmOXfNPIcNPZOVSCRCxkZJCSrt7dGjR6lC0UsvvUTx26lUCo2NjXjllVcA2KmI33vvPQD26kw9WZnf78fq1asB2Cl2d+7cCcCOxU8mk7RkP5VKkUb9ySefUCHrwcFBuN1uMvSlFF5pWRatQn7yySepTu6FCxfg9XrpITg8POyotau4ePEiOjo6Rk3C5sLlss0Aa/XFQSHrRUrnr4NhGIbJCnv0jAM9Ja4ugbjdbgrlu3DhAt5//30AdjKtxx9/HIA9advd3U1RId/73vewbt06AMDNmzcp3TFgy0GqLOCqVatoIrG/vx+maVLZwtu3b+Po0aMAgMOHD5NkJIRwRO2oNgJ2GOJckW8yQ+DG8sZUH3g8Hopw6uvro+2BQACBQIAijwDQCEmfiL5z5w4GBwdJrjFNF0XdsIRTOnB4JVMwyhDF43GK1NCLYSsuXrwIAPjlL39J8sEDDzzgCOmrqKjAxo0baZ9lWaTnu1wuCsMMh8MUohkIBCCEoFWdn332GRUvaW9vp/ObpolwOOzQtfONupmpVAnZ/hD1bZntCIftB6nX66EEZYlEgh6IkUgEAwMD1N8qkglAuhpUkj6jk8246zKNkm701yzjlBYs3TAMw5Q47NEzWdG9TZVobARBEThHjhyhiUCPx4OtW7fSsUNDQ/RaJTVTOWwSiQRdw+fz0QTjnTt3cPz4cXz88ccA7IlftcIzHI44asRmtlP3TOdGgrOxRxh6e4GRRU2xWJxy8CeTSYckZZqmI+++Phmr5J5QKIRUKoXKygoAtqSmpxw2TReCwQC9VyMr03SN8v6dfcpefrHChp7JwdhGShmE3t5eHDx4EICtr7e1tdFK1iVLltDxsVgM8XicZAW32426ujoAtiE6ceIEAODjjz/G4cOHqQyeXfpOGfWxDU2pGKJAwE9RR9euXaNasOXl5YhEIjT/EAwGqT9tTd7eruf8B+CowQvY0Tr6ymclJ5mmCS14Jx0OO/pBqvpZf8Ar2alUvoNSgw09My4ul+n4gzdNlyO7ojIaJ0+eRGdnJ9WCffTRR8nY19fXo6qqioxQZ2cnTbKeOXMG33zzDQA7FXFfXx9N/NoTkWquoLSNiOrjuroqWhl79uxZbN++HYA9KopGo+Thx2IxmpeorKykMFcpJdxut8OY6ykivF6vozbsyIPU6c1nZrNUhj0YDDgK02TmKmLmHqzRMwzDlDjs0TM50RNh6R59po6r57Pp7u7Gb37zGwDAiRMnKFd9ZWWlI2NlT08PRe60t7eTB6/2qxFDPB6fNyGBXu9IRlAltaxYscIRTplMJmlUpOrrAvZ3oCKSMvP+6xJMtuRm+jY1V2IYRlrbH32MuoYaWdhzOGqEV9qjrrnEjOWjZ0qbzDjsXMbe67XlA3upvUW6sS7JKJQBGi0TjBiIuTGROvOotQP9/f1U9FwIQUnd7t6965BJ9FTSQ0ND9Bl1vE62yVe1XcXp6+c2DAM+n4/26Q8Oy7IQj8dpYr2UViWXKvwNMQzDlDjs0TNZyRyCm6aLpAXTNB1eoZIPpJSjZAPl9blcrnQOm5H8K7qXOXLdpCNvi70QaH7IAapPvV4vyTN79+7Ftm12wTaVxEx52ZZl0UphADQZ+8ILL6CtrQ2ffvrpqGuYpgnTNOk7yvTUlYSWrR6BjstlOkI258t3NJeYyMpYof4QZxPDMKQe/sVMnkK/VxVCp2uysVgMLpeZU7NX210ul6PohzIqwEgcvZ5RMfv1XY6YfV17LjYy4+THQ/VjZlTMj370KgBg9+7dAEZWJTc1NeHRRx8FYEs8SstfvHgxTp8+jX379gEA3nnnHarENRZ1dXVYtGgRADs0tra2luSksrIyigQ6c+YMzp8/TyGgAMjoF2Lwi72OwFQwEaOt/nbStSCOSym3jPeZcX+JQoifAfg9AD1SyvvS26oB/ALAMgDXALwkpewT9jf21wCeBRAG8M+llCfyvgNmTpFZRHq8EnTKyxuZmEuSYVeGINvDQt9mh2vGHdvm47L8kVFNDK+99hoAe3I7Ho9T+GpDQwMZiF27dpFGH4lEsHHjRjLaO3fuxNmzZwEAXV1dkFI6MpOqrKRLliwh3b+8vByNjY1oamoCYC9kU5lDu7q60jH3dhvnSl4hJjf5aPR/D2BPxrYfA/hISrkSwEfp9wDwLQAr0/9eBfC3U9NMhmEYplDykm6EEMsA/H+aR98K4DEp5U0hRAOAT6SUq4QQ/1f69T9mHjfW+Vm6mXpmUpKbqERRKMXm0RfaL/qCtMzRTSwWo2IjsVgcW7fao/Yf/ehHePbZZwHYK1/j8Th57YFAgOSvrq4uAKDSjV6vl0I5E4kEzQ2UlZUhHA7j8uXLAOzRhF70ZXBwkH5j8Xh8XE1/LFi6KUy6AYBIJDI10k0O6pXxThv7uvT2RgA3tOM60tvGNPTM1KP+eObCHAwzMfTVquq9wuUy6Q+9srKCCpT8zd/8DU2kPvPMMygvL3cYdRXy6vP5HFlEE4kEae+VlZX0ELly5QqOHDmCN998EwBw/vx5Om54OJxzvoaZm0y1K5bt0ZzV0gghXoUt7/ATnWEYZhop1NDfEkI0aNJNT3p7B4Bm7bgmAF3ZTiCl3AtgL2BLNwW2gxkHIQR79SWG8qCllDQh2tbWhp///OcA7Ainp556ivIMDQ0NYWBgAMBIfhwVVllTU0NefGdnJ2XNfPvtt3Ho0CHcunWLrpuZCE3JDZYVKeg+2MGbHDNReGQfgFcA/Hn6/7e07X8ihHgDwIMABsbT55npp9iNfbFp8zOFy+Wi79WyLNy4Yaumf/mXf4mOjg5KhhYMBtHS0kKv9RWw4XCYomnee+89Sg/d2dmJwcFBitk3TVMz7KML0UwUNvIzSz7hlf8I4DEAtUKIDgB/BtvA/1II8UMA7QBeTB/+LuzQysuwwyv/xTS0mWEYhpkAvGBqHjLV3/l0R90Uq0c/3f1SVVXp8Iz1EoKGYVAM/Nq1a2lhVU1NDVKpFH3u6tWrOHDgAAC7FnB/f3+67a70OghbJgoE/CT3RKPRUROw+XxH7MXnZtYXTDGlB0fkFDcq4kXX2uPxOEXS+P1+hMNh0tt7e3vx5ZdfArD1e8Mw6DdgZ6iM0Ws9iZltfEbSHGTmnVftsH9HuRe1sYGffdjQz2P0P8DJGP2p9ri5fF1+CCEoDYGUkox0MpmEx+MhL3FwcJDi49V8jZ5PSKWc8Pl8jtw3Qgg6p54uWmUz1R0GNvBzG85eyTAMU+KwR88AmFtyTql48cmkNS06vfKsBwcHkUjYfeX3+8iDd7lc6WIjI/2oioQo1JxYMpl0aL7qtZKBRqJ6RjR59VtRUk4ymRq1j5lbsKFnHEyVnMM4mQ6jn/n9qJWxPp/PMTGbDT3zpF6larzPAaCHi8tlaP+rLKW8SnYuwtINwzBMicMePZOTzGE4e/gTR5eh8pWk7Hw2VtbXOrqcovLQZG5X1x1rNKE89LFIJq0xZRn25CdHMjn5RWhjwYaeyRvWX2eGVCqp6eDJnP2uG1f9mEyjK4SgbYYx2pjk2pfr/Lmuw0yO6ayVzNINwzBMicMePcMUCVPhQY91DvbQiwu1xiEf2KNnGIYpQiai6bOhZxiGKULUaud8YEPPMAxT4rChZxiGKUImEgXHhp5hGKYI0VNcjAcbeoZhmBKHDT3DMEwRonIU5QMbeoZhmCKEwysZhmEYgg09wzBMiTOuoRdC/EwI0SOEOKNt+9+EEBeEEN8IIX4rhKjS9v1ECHFZCNEqhHhmuhrOMAwzn1G1fvMhH4/+7wHsydj2IYD7pJQbAFwE8BMAEEKsBfAygHXpz/w3IcT05d5kGIaZp0zpZKyU8lMAdzO2fSClVEGcXwFoSr9+HsAbUsqYlLINwGUA2/JuDcMwDDPlTIVG/wMA76VfNwK4oe3rSG8bhRDiVSHEMSHEsSloA8MwDJODSaUpFkL8FIAF4B/UpiyHZS1LJKXcC2AvABiGwaWLGIZhJsBEwisLNvRCiFcA/B6AJ+RIjbkOAM3aYU0Augq9BsMwDDN5CpJuhBB7APwpgO9IKcParn0AXhZCeIUQLQBWAjgy+WYyDMMwOoaRv/ke16MXQvwjgMcA1AohOgD8GewoGy+AD9MZ1L6SUv5PUsqzQohfAjgHW9L5Yykll61hGIaZYvSC8OMhRlSX2cMwDOnz+Wa7GQzDMEWDy+XC0NDQcSnllvGO5ZWxDMMwJQ4beoZhmCKEs1cyDG0O0pwAAAQ5SURBVMOUOFx4hGEYhiHY0DMMwxQhyWT+AY1s6BmGYYoQLjzCMAzDEGzoGYZhShw29AzDMCUOG3qGYZgihOPoGYZhGIINPcMwTBHCC6YYhmFKnIkkgmRDzzAMU+KwoWcYhilCotFo3seyoWcYhilCOAUCwzAMQ7ChZxiGKUK8Xm/ex86JUoJCiNsAhgHcme22zFFqwX2TC+6b3HDf5KZU+maplHLheAfNCUMPAEKIY/nUPpyPcN/khvsmN9w3uZlvfcPSDcMwTInDhp5hGKbEmUuGfu9sN2AOw32TG+6b3HDf5GZe9c2c0egZhmGY6WEuefQMwzDMNDDrhl4IsUcI0SqEuCyE+PFst2e2EUJcE0KcFkKcEkIcS2+rFkJ8KIS4lP5/wWy3cyYQQvxMCNEjhDijbcvaF8Lm/0z/jr4RQmyevZZPPzn65j8JITrTv51TQohntX0/SfdNqxDimdlp9cwghGgWQnwshDgvhDgrhPhX6e3z9rczq4ZeCOEC8F8BfAvAWgDfF0Ksnc02zREel1Ju1MK/fgzgIynlSgAfpd/PB/4ewJ6Mbbn64lsAVqb/vQrgb2eojbPF32N03wDAX6V/OxullO8CQPpv6mUA69Kf+W/pv71SxQLw76SUawA8BOCP030wb387s+3RbwNwWUp5VUoZB/AGgOdnuU1zkecBvJ5+/TqAF2axLTOGlPJTAHczNufqi+cB/FzafAWgSgjRMDMtnXly9E0ungfwhpQyJqVsA3AZ9t9eSSKlvCmlPJF+PQjgPIBGzOPfzmwb+kYAN7T3Helt8xkJ4AMhxHEhxKvpbfVSypuA/SMGUDdrrZt9cvUF/5Zs/iQtP/xMk/jmbd8IIZYB2ATgMObxb2e2Db3Ism2+hwE9IqXcDHs4+cdCiJ2z3aAigX9LtuSwHMBGADcB/O/p7fOyb4QQZQB+A+BfSylDYx2aZVtJ9c9sG/oOAM3a+yYAXbPUljmBlLIr/X8PgN/CHmLfUkPJ9P89s9fCWSdXX8z735KU8paUMimlTAH4vzEiz8y7vhFCuGEb+X+QUr6Z3jxvfzuzbeiPAlgphGgRQnhgTxjtm+U2zRpCiKAQoly9BvA0gDOw++SV9GGvAHhrdlo4J8jVF/sA/A/pCIqHAAyoYfp8IUNX/i7s3w5g983LQgivEKIF9qTjkZlu30whhBAAXgNwXkr5X7Rd8/e3I6Wc1X8AngVwEcAVAD+d7fbMcl/cA+Dr9L+zqj8A1MCOEriU/r96tts6Q/3xj7AliARsr+uHufoC9vD7v6Z/R6cBbJnt9s9C3/y/6Xv/BrbxatCO/2m6b1oBfGu22z/NffMobOnlGwCn0v+enc+/HV4ZyzAMU+LMtnTDMAzDTDNs6BmGYUocNvQMwzAlDht6hmGYEocNPcMwTInDhp5hGKbEYUPPMAxT4rChZxiGKXH+f+6AHwOb4gIiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(len(trn_idx))\n",
    "img_sample = trn_dataset[idx][0].permute(1, 2, 0).numpy().squeeze()\n",
    "plt.imshow(img_sample, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(trn_dataset, shuffle=True, num_workers=4, batch_size=BATCH_SIZE)\n",
    "vld_loader = DataLoader(vld_dataset, shuffle=False, num_workers=4, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make model, optimizer, and criterion for Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 파라메터들을 random 분포로 초기화화여 훈련하는 것보다 이미 ImageNet 데이터로 pre-trained되어 있는 모델 파라메터를 사용하는 것이 더 효율적으로 훈련을 수행하는 방법입니다. \n",
    "어떤 모델을 사용해도 무방하지만, 본 핸즈온에서는 비교적 간단한 모델인 ResNet-18을 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "last_hidden_units = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(last_hidden_units, 186)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
    "                                                      verbose=True, patience=5, \n",
    "                                                      factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "`p3.2xlarge` 노트북 인스턴스 기준으로 epoch당 5분 30초~5분 40초 정도 소요됩니다. 원활한 핸즈온을 위해 본 section은 건너뛰어도 되며, 이미 훈련이 완료된 모델을 그대로 사용하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 10/628] loss: 3.1508\n",
      "[Epoch 0 Batch 20/628] loss: 2.7415\n",
      "[Epoch 0 Batch 30/628] loss: 2.6588\n",
      "[Epoch 0 Batch 40/628] loss: 2.2074\n",
      "[Epoch 0 Batch 50/628] loss: 2.4823\n",
      "[Epoch 0 Batch 60/628] loss: 2.2221\n",
      "[Epoch 0 Batch 70/628] loss: 1.8909\n",
      "[Epoch 0 Batch 80/628] loss: 1.6422\n",
      "[Epoch 0 Batch 90/628] loss: 2.2726\n",
      "[Epoch 0 Batch 100/628] loss: 1.4553\n",
      "[Epoch 0 Batch 110/628] loss: 1.6459\n",
      "[Epoch 0 Batch 120/628] loss: 1.9580\n",
      "[Epoch 0 Batch 130/628] loss: 1.5981\n",
      "[Epoch 0 Batch 140/628] loss: 1.8985\n",
      "[Epoch 0 Batch 150/628] loss: 1.7036\n",
      "[Epoch 0 Batch 160/628] loss: 1.5649\n",
      "[Epoch 0 Batch 170/628] loss: 1.7639\n",
      "[Epoch 0 Batch 180/628] loss: 1.4936\n",
      "[Epoch 0 Batch 190/628] loss: 1.1015\n",
      "[Epoch 0 Batch 200/628] loss: 2.0834\n",
      "[Epoch 0 Batch 210/628] loss: 1.8302\n",
      "[Epoch 0 Batch 220/628] loss: 1.2091\n",
      "[Epoch 0 Batch 230/628] loss: 1.5838\n",
      "[Epoch 0 Batch 240/628] loss: 1.4027\n",
      "[Epoch 0 Batch 250/628] loss: 1.5748\n",
      "[Epoch 0 Batch 260/628] loss: 1.2333\n",
      "[Epoch 0 Batch 270/628] loss: 1.5405\n",
      "[Epoch 0 Batch 280/628] loss: 1.0223\n",
      "[Epoch 0 Batch 290/628] loss: 1.5216\n",
      "[Epoch 0 Batch 300/628] loss: 1.2704\n",
      "[Epoch 0 Batch 310/628] loss: 1.7270\n",
      "[Epoch 0 Batch 320/628] loss: 1.4181\n",
      "[Epoch 0 Batch 330/628] loss: 0.7801\n",
      "[Epoch 0 Batch 340/628] loss: 1.9166\n",
      "[Epoch 0 Batch 350/628] loss: 1.7332\n",
      "[Epoch 0 Batch 360/628] loss: 1.4336\n",
      "[Epoch 0 Batch 370/628] loss: 1.6455\n",
      "[Epoch 0 Batch 380/628] loss: 2.1148\n",
      "[Epoch 0 Batch 390/628] loss: 0.8678\n",
      "[Epoch 0 Batch 400/628] loss: 0.9205\n",
      "[Epoch 0 Batch 410/628] loss: 1.3361\n",
      "[Epoch 0 Batch 420/628] loss: 0.8298\n",
      "[Epoch 0 Batch 430/628] loss: 1.4895\n",
      "[Epoch 0 Batch 440/628] loss: 1.6704\n",
      "[Epoch 0 Batch 450/628] loss: 1.0054\n",
      "[Epoch 0 Batch 460/628] loss: 1.0254\n",
      "[Epoch 0 Batch 470/628] loss: 1.3762\n",
      "[Epoch 0 Batch 480/628] loss: 1.4398\n",
      "[Epoch 0 Batch 490/628] loss: 1.6620\n",
      "[Epoch 0 Batch 500/628] loss: 1.1178\n",
      "[Epoch 0 Batch 510/628] loss: 0.8277\n",
      "[Epoch 0 Batch 520/628] loss: 1.2201\n",
      "[Epoch 0 Batch 530/628] loss: 1.3103\n",
      "[Epoch 0 Batch 540/628] loss: 1.5474\n",
      "[Epoch 0 Batch 550/628] loss: 1.3435\n",
      "[Epoch 0 Batch 560/628] loss: 1.0034\n",
      "[Epoch 0 Batch 570/628] loss: 1.3102\n",
      "[Epoch 0 Batch 580/628] loss: 1.3117\n",
      "[Epoch 0 Batch 590/628] loss: 1.2184\n",
      "[Epoch 0 Batch 600/628] loss: 1.5493\n",
      "[Epoch 0 Batch 610/628] loss: 1.9158\n",
      "[Epoch 0 Batch 620/628] loss: 0.8644\n",
      "== Start Validation ==\n",
      "[Epoch 0] trn_loss: 1.5552, vld_loss: 1.3846, score: 0.8557, score_each: [0.8070, 0.9268, 0.8818]\n",
      "[Epoch 1 Batch 10/628] loss: 1.7388\n",
      "[Epoch 1 Batch 20/628] loss: 1.0611\n",
      "[Epoch 1 Batch 30/628] loss: 0.9283\n",
      "[Epoch 1 Batch 40/628] loss: 1.5120\n",
      "[Epoch 1 Batch 50/628] loss: 1.6737\n",
      "[Epoch 1 Batch 60/628] loss: 1.8157\n",
      "[Epoch 1 Batch 70/628] loss: 1.3749\n",
      "[Epoch 1 Batch 80/628] loss: 0.6684\n",
      "[Epoch 1 Batch 90/628] loss: 1.0144\n",
      "[Epoch 1 Batch 100/628] loss: 1.1125\n",
      "[Epoch 1 Batch 110/628] loss: 0.8450\n",
      "[Epoch 1 Batch 120/628] loss: 1.4147\n",
      "[Epoch 1 Batch 130/628] loss: 1.1757\n",
      "[Epoch 1 Batch 140/628] loss: 0.9799\n",
      "[Epoch 1 Batch 150/628] loss: 1.0729\n",
      "[Epoch 1 Batch 160/628] loss: 1.0795\n",
      "[Epoch 1 Batch 170/628] loss: 1.3938\n",
      "[Epoch 1 Batch 180/628] loss: 0.9923\n",
      "[Epoch 1 Batch 190/628] loss: 1.3547\n",
      "[Epoch 1 Batch 200/628] loss: 1.1607\n",
      "[Epoch 1 Batch 210/628] loss: 1.1368\n",
      "[Epoch 1 Batch 220/628] loss: 1.2473\n",
      "[Epoch 1 Batch 230/628] loss: 1.1958\n",
      "[Epoch 1 Batch 240/628] loss: 0.9265\n",
      "[Epoch 1 Batch 250/628] loss: 1.1500\n",
      "[Epoch 1 Batch 260/628] loss: 0.7542\n",
      "[Epoch 1 Batch 270/628] loss: 1.5164\n",
      "[Epoch 1 Batch 280/628] loss: 1.1098\n",
      "[Epoch 1 Batch 290/628] loss: 1.1418\n",
      "[Epoch 1 Batch 300/628] loss: 1.1494\n",
      "[Epoch 1 Batch 310/628] loss: 1.7314\n",
      "[Epoch 1 Batch 320/628] loss: 0.9016\n",
      "[Epoch 1 Batch 330/628] loss: 1.0422\n",
      "[Epoch 1 Batch 340/628] loss: 1.2305\n",
      "[Epoch 1 Batch 350/628] loss: 1.5580\n",
      "[Epoch 1 Batch 360/628] loss: 0.6982\n",
      "[Epoch 1 Batch 370/628] loss: 1.4630\n",
      "[Epoch 1 Batch 380/628] loss: 1.2127\n",
      "[Epoch 1 Batch 390/628] loss: 1.0205\n",
      "[Epoch 1 Batch 400/628] loss: 1.7365\n",
      "[Epoch 1 Batch 410/628] loss: 1.4578\n",
      "[Epoch 1 Batch 420/628] loss: 1.7272\n",
      "[Epoch 1 Batch 430/628] loss: 0.8208\n",
      "[Epoch 1 Batch 440/628] loss: 1.6877\n",
      "[Epoch 1 Batch 450/628] loss: 0.8132\n",
      "[Epoch 1 Batch 460/628] loss: 0.9885\n",
      "[Epoch 1 Batch 470/628] loss: 1.1336\n",
      "[Epoch 1 Batch 480/628] loss: 1.5022\n",
      "[Epoch 1 Batch 490/628] loss: 1.1350\n",
      "[Epoch 1 Batch 500/628] loss: 1.1438\n",
      "[Epoch 1 Batch 510/628] loss: 1.1775\n",
      "[Epoch 1 Batch 520/628] loss: 0.9624\n",
      "[Epoch 1 Batch 530/628] loss: 1.1226\n",
      "[Epoch 1 Batch 540/628] loss: 0.9989\n",
      "[Epoch 1 Batch 550/628] loss: 1.4295\n",
      "[Epoch 1 Batch 560/628] loss: 0.6554\n",
      "[Epoch 1 Batch 570/628] loss: 0.9323\n",
      "[Epoch 1 Batch 580/628] loss: 0.8478\n",
      "[Epoch 1 Batch 590/628] loss: 1.1454\n",
      "[Epoch 1 Batch 600/628] loss: 0.7774\n",
      "[Epoch 1 Batch 610/628] loss: 0.4096\n",
      "[Epoch 1 Batch 620/628] loss: 0.7809\n",
      "== Start Validation ==\n",
      "[Epoch 1] trn_loss: 1.1554, vld_loss: 1.2237, score: 0.8973, score_each: [0.8797, 0.9438, 0.8857]\n",
      "[Epoch 2 Batch 10/628] loss: 1.2288\n",
      "[Epoch 2 Batch 20/628] loss: 1.2366\n",
      "[Epoch 2 Batch 30/628] loss: 0.7051\n",
      "[Epoch 2 Batch 40/628] loss: 1.2552\n",
      "[Epoch 2 Batch 50/628] loss: 1.4535\n",
      "[Epoch 2 Batch 60/628] loss: 1.2432\n",
      "[Epoch 2 Batch 70/628] loss: 1.0280\n",
      "[Epoch 2 Batch 80/628] loss: 0.5121\n",
      "[Epoch 2 Batch 90/628] loss: 1.1518\n",
      "[Epoch 2 Batch 100/628] loss: 0.8770\n",
      "[Epoch 2 Batch 110/628] loss: 0.6756\n",
      "[Epoch 2 Batch 120/628] loss: 0.3208\n",
      "[Epoch 2 Batch 130/628] loss: 1.4369\n",
      "[Epoch 2 Batch 140/628] loss: 0.7838\n",
      "[Epoch 2 Batch 150/628] loss: 1.0803\n",
      "[Epoch 2 Batch 160/628] loss: 0.8470\n",
      "[Epoch 2 Batch 170/628] loss: 1.1697\n",
      "[Epoch 2 Batch 180/628] loss: 0.9911\n",
      "[Epoch 2 Batch 190/628] loss: 1.0582\n",
      "[Epoch 2 Batch 200/628] loss: 0.4391\n",
      "[Epoch 2 Batch 210/628] loss: 1.3375\n",
      "[Epoch 2 Batch 220/628] loss: 1.0161\n",
      "[Epoch 2 Batch 230/628] loss: 1.0582\n",
      "[Epoch 2 Batch 240/628] loss: 0.8675\n",
      "[Epoch 2 Batch 250/628] loss: 1.2825\n",
      "[Epoch 2 Batch 260/628] loss: 0.8313\n",
      "[Epoch 2 Batch 270/628] loss: 1.3572\n",
      "[Epoch 2 Batch 280/628] loss: 0.7698\n",
      "[Epoch 2 Batch 290/628] loss: 1.2430\n",
      "[Epoch 2 Batch 300/628] loss: 0.8926\n",
      "[Epoch 2 Batch 310/628] loss: 0.6754\n",
      "[Epoch 2 Batch 320/628] loss: 0.7116\n",
      "[Epoch 2 Batch 330/628] loss: 0.5759\n",
      "[Epoch 2 Batch 340/628] loss: 1.3093\n",
      "[Epoch 2 Batch 350/628] loss: 0.6915\n",
      "[Epoch 2 Batch 360/628] loss: 1.2432\n",
      "[Epoch 2 Batch 370/628] loss: 0.9820\n",
      "[Epoch 2 Batch 380/628] loss: 0.6369\n",
      "[Epoch 2 Batch 390/628] loss: 1.3171\n",
      "[Epoch 2 Batch 400/628] loss: 0.8414\n",
      "[Epoch 2 Batch 410/628] loss: 0.8873\n",
      "[Epoch 2 Batch 420/628] loss: 0.9016\n",
      "[Epoch 2 Batch 430/628] loss: 1.2340\n",
      "[Epoch 2 Batch 440/628] loss: 0.8569\n",
      "[Epoch 2 Batch 450/628] loss: 1.0541\n",
      "[Epoch 2 Batch 460/628] loss: 1.2247\n",
      "[Epoch 2 Batch 470/628] loss: 0.9937\n",
      "[Epoch 2 Batch 480/628] loss: 1.0324\n",
      "[Epoch 2 Batch 490/628] loss: 0.8364\n",
      "[Epoch 2 Batch 500/628] loss: 1.1140\n",
      "[Epoch 2 Batch 510/628] loss: 1.0237\n",
      "[Epoch 2 Batch 520/628] loss: 1.0603\n",
      "[Epoch 2 Batch 530/628] loss: 0.7003\n",
      "[Epoch 2 Batch 540/628] loss: 0.7729\n",
      "[Epoch 2 Batch 550/628] loss: 0.8640\n",
      "[Epoch 2 Batch 560/628] loss: 0.7442\n",
      "[Epoch 2 Batch 570/628] loss: 1.5299\n",
      "[Epoch 2 Batch 580/628] loss: 1.1240\n",
      "[Epoch 2 Batch 590/628] loss: 0.9334\n",
      "[Epoch 2 Batch 600/628] loss: 0.9759\n",
      "[Epoch 2 Batch 610/628] loss: 0.6782\n",
      "[Epoch 2 Batch 620/628] loss: 0.9496\n",
      "== Start Validation ==\n",
      "[Epoch 2] trn_loss: 0.9740, vld_loss: 1.2295, score: 0.9180, score_each: [0.8961, 0.9577, 0.9222]\n",
      "[Epoch 3 Batch 10/628] loss: 1.2816\n",
      "[Epoch 3 Batch 20/628] loss: 1.1559\n",
      "[Epoch 3 Batch 30/628] loss: 0.3732\n",
      "[Epoch 3 Batch 40/628] loss: 1.0382\n",
      "[Epoch 3 Batch 50/628] loss: 1.1544\n",
      "[Epoch 3 Batch 60/628] loss: 1.1309\n",
      "[Epoch 3 Batch 70/628] loss: 1.0228\n",
      "[Epoch 3 Batch 80/628] loss: 0.9145\n",
      "[Epoch 3 Batch 90/628] loss: 1.0645\n",
      "[Epoch 3 Batch 100/628] loss: 0.8038\n",
      "[Epoch 3 Batch 110/628] loss: 1.0180\n",
      "[Epoch 3 Batch 120/628] loss: 1.3194\n",
      "[Epoch 3 Batch 130/628] loss: 1.4629\n",
      "[Epoch 3 Batch 140/628] loss: 0.5475\n",
      "[Epoch 3 Batch 150/628] loss: 1.2257\n",
      "[Epoch 3 Batch 160/628] loss: 1.1296\n",
      "[Epoch 3 Batch 170/628] loss: 0.5604\n",
      "[Epoch 3 Batch 180/628] loss: 1.1776\n",
      "[Epoch 3 Batch 190/628] loss: 0.8746\n",
      "[Epoch 3 Batch 200/628] loss: 1.2029\n",
      "[Epoch 3 Batch 210/628] loss: 0.7897\n",
      "[Epoch 3 Batch 220/628] loss: 1.0891\n",
      "[Epoch 3 Batch 230/628] loss: 1.0204\n",
      "[Epoch 3 Batch 240/628] loss: 1.0252\n",
      "[Epoch 3 Batch 250/628] loss: 1.0131\n",
      "[Epoch 3 Batch 260/628] loss: 1.1770\n",
      "[Epoch 3 Batch 270/628] loss: 1.5817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 Batch 280/628] loss: 0.8032\n",
      "[Epoch 3 Batch 290/628] loss: 1.3646\n",
      "[Epoch 3 Batch 300/628] loss: 1.0958\n",
      "[Epoch 3 Batch 310/628] loss: 0.6190\n",
      "[Epoch 3 Batch 320/628] loss: 0.9752\n",
      "[Epoch 3 Batch 330/628] loss: 0.8791\n",
      "[Epoch 3 Batch 340/628] loss: 1.2496\n",
      "[Epoch 3 Batch 350/628] loss: 1.6788\n",
      "[Epoch 3 Batch 360/628] loss: 0.9395\n",
      "[Epoch 3 Batch 370/628] loss: 1.2074\n",
      "[Epoch 3 Batch 380/628] loss: 0.9555\n",
      "[Epoch 3 Batch 390/628] loss: 1.4485\n",
      "[Epoch 3 Batch 400/628] loss: 0.9147\n",
      "[Epoch 3 Batch 410/628] loss: 1.1292\n",
      "[Epoch 3 Batch 420/628] loss: 1.1092\n",
      "[Epoch 3 Batch 430/628] loss: 1.1162\n",
      "[Epoch 3 Batch 440/628] loss: 1.4043\n",
      "[Epoch 3 Batch 450/628] loss: 0.6262\n",
      "[Epoch 3 Batch 460/628] loss: 0.4814\n",
      "[Epoch 3 Batch 470/628] loss: 1.6520\n",
      "[Epoch 3 Batch 480/628] loss: 1.1657\n",
      "[Epoch 3 Batch 490/628] loss: 0.6932\n",
      "[Epoch 3 Batch 500/628] loss: 1.4897\n",
      "[Epoch 3 Batch 510/628] loss: 0.8185\n",
      "[Epoch 3 Batch 520/628] loss: 0.8882\n",
      "[Epoch 3 Batch 530/628] loss: 1.0504\n",
      "[Epoch 3 Batch 540/628] loss: 0.8451\n",
      "[Epoch 3 Batch 550/628] loss: 0.6120\n",
      "[Epoch 3 Batch 560/628] loss: 1.0193\n",
      "[Epoch 3 Batch 570/628] loss: 0.6198\n",
      "[Epoch 3 Batch 580/628] loss: 0.9379\n",
      "[Epoch 3 Batch 590/628] loss: 1.4534\n",
      "[Epoch 3 Batch 600/628] loss: 0.9325\n",
      "[Epoch 3 Batch 610/628] loss: 1.1722\n",
      "[Epoch 3 Batch 620/628] loss: 1.2956\n",
      "== Start Validation ==\n",
      "[Epoch 3] trn_loss: 1.0461, vld_loss: 1.1424, score: 0.9348, score_each: [0.9157, 0.9610, 0.9468]\n",
      "[Epoch 4 Batch 10/628] loss: 0.6737\n",
      "[Epoch 4 Batch 20/628] loss: 1.4530\n",
      "[Epoch 4 Batch 30/628] loss: 0.6701\n",
      "[Epoch 4 Batch 40/628] loss: 0.9159\n",
      "[Epoch 4 Batch 50/628] loss: 1.2007\n",
      "[Epoch 4 Batch 60/628] loss: 0.7194\n",
      "[Epoch 4 Batch 70/628] loss: 0.7006\n",
      "[Epoch 4 Batch 80/628] loss: 0.8294\n",
      "[Epoch 4 Batch 90/628] loss: 0.8035\n",
      "[Epoch 4 Batch 100/628] loss: 0.5750\n",
      "[Epoch 4 Batch 110/628] loss: 1.1389\n",
      "[Epoch 4 Batch 120/628] loss: 0.7992\n",
      "[Epoch 4 Batch 130/628] loss: 1.3889\n",
      "[Epoch 4 Batch 140/628] loss: 1.5621\n",
      "[Epoch 4 Batch 150/628] loss: 1.8147\n",
      "[Epoch 4 Batch 160/628] loss: 1.2277\n",
      "[Epoch 4 Batch 170/628] loss: 1.2561\n",
      "[Epoch 4 Batch 180/628] loss: 1.3467\n",
      "[Epoch 4 Batch 190/628] loss: 1.0712\n",
      "[Epoch 4 Batch 200/628] loss: 0.7067\n",
      "[Epoch 4 Batch 210/628] loss: 0.9809\n",
      "[Epoch 4 Batch 220/628] loss: 1.1307\n",
      "[Epoch 4 Batch 230/628] loss: 0.4296\n",
      "[Epoch 4 Batch 240/628] loss: 0.5170\n",
      "[Epoch 4 Batch 250/628] loss: 1.1590\n",
      "[Epoch 4 Batch 260/628] loss: 0.9115\n",
      "[Epoch 4 Batch 270/628] loss: 1.7376\n",
      "[Epoch 4 Batch 280/628] loss: 0.8356\n",
      "[Epoch 4 Batch 290/628] loss: 1.0523\n",
      "[Epoch 4 Batch 300/628] loss: 1.0048\n",
      "[Epoch 4 Batch 310/628] loss: 1.1220\n",
      "[Epoch 4 Batch 320/628] loss: 1.0725\n",
      "[Epoch 4 Batch 330/628] loss: 0.6245\n",
      "[Epoch 4 Batch 340/628] loss: 1.5427\n",
      "[Epoch 4 Batch 350/628] loss: 1.2245\n",
      "[Epoch 4 Batch 360/628] loss: 0.9870\n",
      "[Epoch 4 Batch 370/628] loss: 1.3414\n",
      "[Epoch 4 Batch 380/628] loss: 1.1910\n",
      "[Epoch 4 Batch 390/628] loss: 0.3654\n",
      "[Epoch 4 Batch 400/628] loss: 0.5760\n",
      "[Epoch 4 Batch 410/628] loss: 0.9769\n",
      "[Epoch 4 Batch 420/628] loss: 0.9404\n",
      "[Epoch 4 Batch 430/628] loss: 1.3749\n",
      "[Epoch 4 Batch 440/628] loss: 0.4190\n",
      "[Epoch 4 Batch 450/628] loss: 0.6254\n",
      "[Epoch 4 Batch 460/628] loss: 0.6947\n",
      "[Epoch 4 Batch 470/628] loss: 0.6069\n",
      "[Epoch 4 Batch 480/628] loss: 0.9056\n",
      "[Epoch 4 Batch 490/628] loss: 0.9659\n",
      "[Epoch 4 Batch 500/628] loss: 0.7380\n",
      "[Epoch 4 Batch 510/628] loss: 1.1998\n",
      "[Epoch 4 Batch 520/628] loss: 0.9345\n",
      "[Epoch 4 Batch 530/628] loss: 0.7216\n",
      "[Epoch 4 Batch 540/628] loss: 1.2507\n",
      "[Epoch 4 Batch 550/628] loss: 0.8932\n",
      "[Epoch 4 Batch 560/628] loss: 0.8180\n",
      "[Epoch 4 Batch 570/628] loss: 0.9920\n",
      "[Epoch 4 Batch 580/628] loss: 0.8656\n",
      "[Epoch 4 Batch 590/628] loss: 1.4435\n",
      "[Epoch 4 Batch 600/628] loss: 0.8313\n",
      "[Epoch 4 Batch 610/628] loss: 0.8674\n",
      "[Epoch 4 Batch 620/628] loss: 0.7485\n",
      "== Start Validation ==\n",
      "[Epoch 4] trn_loss: 0.9749, vld_loss: 1.2042, score: 0.9429, score_each: [0.9191, 0.9711, 0.9622]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    '''\n",
    "    CutMix Helper function.\n",
    "    Retrieved from https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "    '''\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    # 폭과 높이는 주어진 이미지의 폭과 높이의 beta distribution에서 뽑은 lambda로 얻는다\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    \n",
    "    # patch size 의 w, h 는 original image 의 w,h 에 np.sqrt(1-lambda) 를 곱해준 값입니다.\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # patch의 중심점은 uniform하게 뽑힘\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "best_score = -1\n",
    "log_interval = 10\n",
    "training_stats = []\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch_id in range(num_epochs):\n",
    "\n",
    "    ################################################################################\n",
    "    # ==> Training phase\n",
    "    ################################################################################    \n",
    "    trn_loss = []\n",
    "    model.train()\n",
    "    \n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_id, (inputs, targets) in enumerate((trn_loader)):\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        targets_gra = targets[:, 0]\n",
    "        targets_vow = targets[:, 1]\n",
    "        targets_con = targets[:, 2]\n",
    "                    \n",
    "        # 50%의 확률로 원본 데이터 그대로 사용    \n",
    "        if np.random.rand() < 0.5:\n",
    "            logits = model(inputs)\n",
    "            grapheme = logits[:, :168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "            \n",
    "            loss1 = loss_fn(grapheme, targets_gra)\n",
    "            loss2 = loss_fn(vowel, targets_vow)\n",
    "            loss3 = loss_fn(cons, targets_con) \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            lam = np.random.beta(1.0, 1.0) \n",
    "            rand_index = torch.randperm(inputs.size()[0])\n",
    "            shuffled_targets_gra = targets_gra[rand_index]\n",
    "            shuffled_targets_vow = targets_vow[rand_index]\n",
    "            shuffled_targets_con = targets_con[rand_index]\n",
    "            \n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            # 픽셀 비율과 정확히 일치하도록 lambda 파라메터 조정  \n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            grapheme = logits[:,:168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "            \n",
    "            loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "            loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "            loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "        \n",
    "        loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3    \n",
    "        trn_loss.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Printing vital information\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            s = f'[Epoch {epoch_id} Batch {batch_id+1}/{len(trn_loader)}] ' \\\n",
    "            f'loss: {running_loss / log_interval:.4f}'\n",
    "            print(s)\n",
    "            running_loss = 0\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    trn_time = format_time(time.time() - t0)        \n",
    "\n",
    "    ################################################################################\n",
    "    # ==> Validation phase\n",
    "    ################################################################################\n",
    "    val_loss = []\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in vld_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            logits = model(inputs)\n",
    "            grapheme = logits[:,:168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "\n",
    "            loss= 0.5* loss_fn(grapheme, targets[:,0]) + 0.25*loss_fn(vowel, targets[:,1]) + \\\n",
    "            0.25*loss_fn(vowel, targets[:,2])\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "\n",
    "            grapheme = grapheme.cpu().argmax(dim=1).data.numpy()\n",
    "            vowel = vowel.cpu().argmax(dim=1).data.numpy()\n",
    "            cons = cons.cpu().argmax(dim=1).data.numpy()\n",
    "\n",
    "            val_true.append(targets.cpu().numpy())\n",
    "            val_pred.append(np.stack([grapheme, vowel, cons], axis=1))                \n",
    "\n",
    "    val_true = np.concatenate(val_true)\n",
    "    val_pred = np.concatenate(val_pred)\n",
    "    val_loss = np.mean(val_loss)\n",
    "    trn_loss = np.mean(trn_loss)\n",
    "\n",
    "    score_g = recall_score(val_true[:,0], val_pred[:,0], average='macro')\n",
    "    score_v = recall_score(val_true[:,1], val_pred[:,1], average='macro')\n",
    "    score_c = recall_score(val_true[:,2], val_pred[:,2], average='macro')\n",
    "    final_score = np.average([score_g, score_v, score_c], weights=[2,1,1])\n",
    "   \n",
    "    # Printing vital information\n",
    "    print('== Start Validation ==')\n",
    "    s = f'[Epoch {epoch_id}] ' \\\n",
    "    f'trn_loss: {trn_loss:.4f}, vld_loss: {val_loss:.4f}, score: {final_score:.4f}, ' \\\n",
    "    f'score_each: [{score_g:.4f}, {score_v:.4f}, {score_c:.4f}]'          \n",
    "    print(s)\n",
    "\n",
    "    ################################################################################\n",
    "    # ==> Save checkpoint and training stats\n",
    "    ################################################################################        \n",
    "    if final_score > best_score:\n",
    "        best_score = final_score\n",
    "        state_dict = model.cpu().state_dict()\n",
    "        model = model.cuda()\n",
    "        torch.save(state_dict, 'model.pt')\n",
    "        \n",
    "    # Record all statistics from this epoch\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_id + 1,\n",
    "            'trn_loss': trn_loss,\n",
    "            'trn_time': trn_time,            \n",
    "            'val_loss': val_loss,\n",
    "            'score': final_score,\n",
    "            'score_g': score_g,\n",
    "            'score_v': score_v,\n",
    "            'score_c': score_c            \n",
    "        }\n",
    "    )            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check training results\n",
    "\n",
    "훈련 결과를 간단히 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>score_c</th>\n",
       "      <th>score_g</th>\n",
       "      <th>score_v</th>\n",
       "      <th>trn_loss</th>\n",
       "      <th>trn_time</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8557</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.8070</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>1.5552</td>\n",
       "      <td>0:05:39</td>\n",
       "      <td>1.3846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8973</td>\n",
       "      <td>0.8857</td>\n",
       "      <td>0.8797</td>\n",
       "      <td>0.9438</td>\n",
       "      <td>1.1554</td>\n",
       "      <td>0:05:28</td>\n",
       "      <td>1.2237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9180</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>0:05:34</td>\n",
       "      <td>1.2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9348</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>0.9157</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>1.0461</td>\n",
       "      <td>0:05:34</td>\n",
       "      <td>1.1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9429</td>\n",
       "      <td>0.9622</td>\n",
       "      <td>0.9191</td>\n",
       "      <td>0.9711</td>\n",
       "      <td>0.9749</td>\n",
       "      <td>0:05:36</td>\n",
       "      <td>1.2042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score  score_c  score_g  score_v  trn_loss trn_time  val_loss\n",
       "epoch                                                                \n",
       "1      0.8557   0.8818   0.8070   0.9268    1.5552  0:05:39    1.3846\n",
       "2      0.8973   0.8857   0.8797   0.9438    1.1554  0:05:28    1.2237\n",
       "3      0.9180   0.9222   0.8961   0.9577    0.9740  0:05:34    1.2295\n",
       "4      0.9348   0.9468   0.9157   0.9610    1.0461  0:05:34    1.1424\n",
       "5      0.9429   0.9622   0.9191   0.9711    0.9749  0:05:36    1.2042"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "display(df_stats)\n",
    "\n",
    "#model.load_state_dict(torch.load('./model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
