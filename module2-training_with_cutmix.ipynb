{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modlue 2. Training with CutMix\n",
    "---\n",
    "\n",
    "본 모듈에서는 Amzaon SageMaker API 호출 없이 PyTorch 프레임워크 자체 구현만으로 모델 훈련을 수행해 봅니다. PyTorch의 문법 및 용법에 익숙하신 분들은 이 모듈을 건너 뛰고 Module 3으로 곧바로 진행하셔도 됩니다.\n",
    "\n",
    "훈련을 원활하게 수행하시려면 GPU가 장착된 SageMaker notebook instance를 사용하셔야 합니다. (예: `g4dn.xlarge, p2.xlarge, p3.xlarge`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time, datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 버전을 최신 버전으로 업데이트합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U torch \n",
    "#!pip install albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Splits Data\n",
    "\n",
    "Metric 평가를 위해, 훈련 데이터셋/검증 데이터셋을 분리합니다. 본 핸즈온에서는 훈련 데이터셋을 5-fold로 분리 후 4번째 fold를 검증 데이터셋으로 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./input/train_folds.csv')\n",
    "\n",
    "num_folds = 5\n",
    "vld_fold_idx = 4\n",
    "trn_fold = [i for i in range(num_folds) if i not in [vld_fold_idx]]\n",
    "vld_fold = [vld_fold_idx]\n",
    "\n",
    "trn_idx = train_df.loc[train_df['fold'].isin(trn_fold)].index\n",
    "vld_idx = train_df.loc[train_df['fold'].isin(vld_fold)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Transformation w/ Augmentation\n",
    "\n",
    "본 핸즈온에서는 `albumentations` 패키지를 사용하여 Data augmentation을 수행합니다. 물론 `torchvision.transforms`에서도 이미 augmentation을 지원하고 있지만, 더 많은 augmentation 기법들을 지원하고 있으며 수행 속도 또한 훨씬 빠릅니다.\n",
    "관련 내용은 아래 URL을 참조해 주세요.\n",
    "\n",
    "- [Albumentations: fast and flexible image augmentations](https://arxiv.org/pdf/1809.06839.pdf)\n",
    "- [migrating_from_torchvision_to_albumentations.ipynb](https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Rotate,HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, RandomBrightnessContrast, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, Flip, OneOf, Compose\n",
    ")\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    Rotate(20),\n",
    "        OneOf([\n",
    "            IAAAdditiveGaussianNoise(),\n",
    "            GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            MotionBlur(p=.2),\n",
    "            MedianBlur(blur_limit=3, p=0.1),\n",
    "            Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        OneOf([\n",
    "            OpticalDistortion(p=0.3),\n",
    "            GridDistortion(p=.1),\n",
    "            IAAPiecewiseAffine(p=0.3),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            CLAHE(clip_limit=2),\n",
    "            IAASharpen(),\n",
    "            IAAEmboss(),\n",
    "            RandomBrightnessContrast(),            \n",
    "        ], p=0.3),\n",
    "        HueSaturationValue(p=0.3),\n",
    "    ToTensor()\n",
    "    ], p=1.0)\n",
    "\n",
    "\n",
    "valid_transforms = Compose([\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make DataLoader\n",
    "PyTorch는 미니배치를 쉽게 로딩할 수 있는 `torch.utils.data.Dataset`과 `torch.utils.data.DataLoader`를 제공하며, 이를 사용하여\n",
    "미니배치 셔플링(shuffling), 병렬 처리를 쉽게 구현할 수 있습니다.\n",
    "\n",
    "또한, `torch.utils.data.Dataset`을 상속받아 사용자 정의 Dataset을 아래와 같이 쉽게 구성할 수 있습니다.<br>\n",
    "사용자 정의 Dataset 구현의 기본적인 뼈대는 데이터셋 초기화에 필요한 `__init__(self)` 메서드, 데이터셋의 총 샘플 수를 리턴해 주는 `__len__(self)` 메서드, 특정 1개의 샘플을 가져오는 `__getitem__(self, index)` 메서드의 구현입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BangaliDataset(Dataset):\n",
    "    def __init__(self, df, img_height, img_width, transform):\n",
    "        self.df = df.reset_index()\n",
    "        self.img_ids = df['image_id'].values\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index]  # e.g., Train_5\n",
    "        img = joblib.load(f'./input/train_images/{img_id}.pkl')\n",
    "        img = img.reshape(self.img_height,self.img_width).astype(np.uint8)\n",
    "        img = 255 - img\n",
    "        \n",
    "        img = img[:,:,np.newaxis]\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        label_1 = self.df.iloc[index].grapheme_root\n",
    "        label_2 = self.df.iloc[index].vowel_diacritic\n",
    "        label_3 = self.df.iloc[index].consonant_diacritic\n",
    "        \n",
    "        return img, np.array([label_1, label_2, label_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "trn_dataset = BangaliDataset(df=train_df.loc[trn_idx],\n",
    "                             img_height=HEIGHT, \n",
    "                             img_width=WIDTH, \n",
    "                             transform=train_transforms)\n",
    "vld_dataset = BangaliDataset(df=train_df.loc[vld_idx],\n",
    "                             img_height=HEIGHT, \n",
    "                             img_width=WIDTH, \n",
    "                             transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(trn_dataset, shuffle=True, num_workers=4, batch_size=BATCH_SIZE)\n",
    "vld_loader = DataLoader(vld_dataset, shuffle=False, num_workers=4, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make model, optimizer, and criterion for Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 파라메터들을 random 분포로 초기화화여 훈련하는 것보다 이미 ImageNet 데이터로 pre-trained되어 있는 모데 파라메터를 사용하는 것이 더 효율적으로 훈련을 수행하는 방법입니다.\n",
    "어떤 모델을 사용해도 무방하지만, 본 핸즈온에서는 비교적 간단한 모델인 ResNet-18을 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "last_hidden_units = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(last_hidden_units, 186)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
    "                                                      verbose=True, patience=5, \n",
    "                                                      factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CutMix Helper function\n",
    "# Retrieved from https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    # 폭과 높이는 주어진 이미지의 폭과 높이의 beta distribution에서 뽑은 lambda로 얻는다\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    \n",
    "    # patch size 의 w, h 는 original image 의 w,h 에 np.sqrt(1-lambda) 를 곱해준 값입니다.\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # patch의 중심점은 uniform하게 뽑힘\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 10/628] loss: 3.2058\n",
      "[Epoch 0 Batch 20/628] loss: 2.7928\n",
      "[Epoch 0 Batch 30/628] loss: 2.5698\n",
      "[Epoch 0 Batch 40/628] loss: 2.4763\n",
      "[Epoch 0 Batch 50/628] loss: 2.3434\n",
      "[Epoch 0 Batch 60/628] loss: 2.2434\n",
      "[Epoch 0 Batch 70/628] loss: 1.7695\n",
      "[Epoch 0 Batch 80/628] loss: 1.9345\n",
      "[Epoch 0 Batch 90/628] loss: 2.0418\n",
      "[Epoch 0 Batch 100/628] loss: 2.0295\n",
      "[Epoch 0 Batch 110/628] loss: 1.7038\n",
      "[Epoch 0 Batch 120/628] loss: 1.7669\n",
      "[Epoch 0 Batch 130/628] loss: 1.5865\n",
      "[Epoch 0 Batch 140/628] loss: 1.7775\n",
      "[Epoch 0 Batch 150/628] loss: 1.7740\n",
      "[Epoch 0 Batch 160/628] loss: 1.4519\n",
      "[Epoch 0 Batch 170/628] loss: 1.5003\n",
      "[Epoch 0 Batch 180/628] loss: 1.7282\n",
      "[Epoch 0 Batch 190/628] loss: 1.5326\n",
      "[Epoch 0 Batch 200/628] loss: 1.4592\n",
      "[Epoch 0 Batch 210/628] loss: 1.9410\n",
      "[Epoch 0 Batch 220/628] loss: 1.3942\n",
      "[Epoch 0 Batch 230/628] loss: 1.6176\n",
      "[Epoch 0 Batch 240/628] loss: 1.7659\n",
      "[Epoch 0 Batch 250/628] loss: 1.7061\n",
      "[Epoch 0 Batch 260/628] loss: 1.5860\n",
      "[Epoch 0 Batch 270/628] loss: 1.4983\n",
      "[Epoch 0 Batch 280/628] loss: 1.2540\n",
      "[Epoch 0 Batch 290/628] loss: 1.5342\n",
      "[Epoch 0 Batch 300/628] loss: 1.1836\n",
      "[Epoch 0 Batch 310/628] loss: 1.6546\n",
      "[Epoch 0 Batch 320/628] loss: 2.1177\n",
      "[Epoch 0 Batch 330/628] loss: 1.7108\n",
      "[Epoch 0 Batch 340/628] loss: 1.4343\n",
      "[Epoch 0 Batch 350/628] loss: 1.9014\n",
      "[Epoch 0 Batch 360/628] loss: 1.1129\n",
      "[Epoch 0 Batch 370/628] loss: 1.5434\n",
      "[Epoch 0 Batch 380/628] loss: 0.9184\n",
      "[Epoch 0 Batch 390/628] loss: 1.0179\n",
      "[Epoch 0 Batch 400/628] loss: 0.8482\n",
      "[Epoch 0 Batch 410/628] loss: 1.1706\n",
      "[Epoch 0 Batch 420/628] loss: 1.1563\n",
      "[Epoch 0 Batch 430/628] loss: 1.2016\n",
      "[Epoch 0 Batch 440/628] loss: 0.8045\n",
      "[Epoch 0 Batch 450/628] loss: 1.4486\n",
      "[Epoch 0 Batch 460/628] loss: 1.3062\n",
      "[Epoch 0 Batch 470/628] loss: 1.8858\n",
      "[Epoch 0 Batch 480/628] loss: 1.9526\n",
      "[Epoch 0 Batch 490/628] loss: 1.5310\n",
      "[Epoch 0 Batch 500/628] loss: 1.4081\n",
      "[Epoch 0 Batch 510/628] loss: 1.6457\n",
      "[Epoch 0 Batch 520/628] loss: 1.4802\n",
      "[Epoch 0 Batch 530/628] loss: 0.9103\n",
      "[Epoch 0 Batch 540/628] loss: 1.4965\n",
      "[Epoch 0 Batch 550/628] loss: 0.9197\n",
      "[Epoch 0 Batch 560/628] loss: 1.2741\n",
      "[Epoch 0 Batch 570/628] loss: 1.3188\n",
      "[Epoch 0 Batch 580/628] loss: 0.9014\n",
      "[Epoch 0 Batch 590/628] loss: 1.2335\n",
      "[Epoch 0 Batch 600/628] loss: 0.4172\n",
      "[Epoch 0 Batch 610/628] loss: 1.4075\n",
      "[Epoch 0 Batch 620/628] loss: 1.1043\n",
      "== Start Validation ==\n",
      "[Epoch 0] trn_loss: 1.5630, vld_loss: 1.4987, score: 0.8163, score_each: [0.7921, 0.8829, 0.7981]\n",
      "[Epoch 1 Batch 10/628] loss: 1.4177\n",
      "[Epoch 1 Batch 20/628] loss: 1.1224\n",
      "[Epoch 1 Batch 30/628] loss: 1.3304\n",
      "[Epoch 1 Batch 40/628] loss: 1.6945\n",
      "[Epoch 1 Batch 50/628] loss: 1.3244\n",
      "[Epoch 1 Batch 60/628] loss: 0.6347\n",
      "[Epoch 1 Batch 70/628] loss: 1.1205\n",
      "[Epoch 1 Batch 80/628] loss: 1.1484\n",
      "[Epoch 1 Batch 90/628] loss: 1.0550\n",
      "[Epoch 1 Batch 100/628] loss: 0.6481\n",
      "[Epoch 1 Batch 110/628] loss: 1.5655\n",
      "[Epoch 1 Batch 120/628] loss: 1.7878\n",
      "[Epoch 1 Batch 130/628] loss: 1.6213\n",
      "[Epoch 1 Batch 140/628] loss: 1.1124\n",
      "[Epoch 1 Batch 150/628] loss: 0.7222\n",
      "[Epoch 1 Batch 160/628] loss: 1.5840\n",
      "[Epoch 1 Batch 170/628] loss: 1.2949\n",
      "[Epoch 1 Batch 180/628] loss: 1.2194\n",
      "[Epoch 1 Batch 190/628] loss: 1.6195\n",
      "[Epoch 1 Batch 200/628] loss: 1.2564\n",
      "[Epoch 1 Batch 210/628] loss: 1.1974\n",
      "[Epoch 1 Batch 220/628] loss: 1.6255\n",
      "[Epoch 1 Batch 230/628] loss: 0.7021\n",
      "[Epoch 1 Batch 240/628] loss: 1.7196\n",
      "[Epoch 1 Batch 250/628] loss: 0.8287\n",
      "[Epoch 1 Batch 260/628] loss: 1.6582\n",
      "[Epoch 1 Batch 270/628] loss: 1.3237\n",
      "[Epoch 1 Batch 280/628] loss: 1.4687\n",
      "[Epoch 1 Batch 290/628] loss: 1.5397\n",
      "[Epoch 1 Batch 300/628] loss: 1.1899\n",
      "[Epoch 1 Batch 310/628] loss: 1.2367\n",
      "[Epoch 1 Batch 320/628] loss: 1.5977\n",
      "[Epoch 1 Batch 330/628] loss: 1.1253\n",
      "[Epoch 1 Batch 340/628] loss: 0.9440\n",
      "[Epoch 1 Batch 350/628] loss: 1.5043\n",
      "[Epoch 1 Batch 360/628] loss: 1.1139\n",
      "[Epoch 1 Batch 370/628] loss: 0.9077\n",
      "[Epoch 1 Batch 380/628] loss: 0.8563\n",
      "[Epoch 1 Batch 390/628] loss: 1.7461\n",
      "[Epoch 1 Batch 400/628] loss: 1.3040\n",
      "[Epoch 1 Batch 410/628] loss: 1.0601\n",
      "[Epoch 1 Batch 420/628] loss: 0.7873\n",
      "[Epoch 1 Batch 430/628] loss: 0.4259\n",
      "[Epoch 1 Batch 440/628] loss: 1.5522\n",
      "[Epoch 1 Batch 450/628] loss: 1.6100\n",
      "[Epoch 1 Batch 460/628] loss: 0.7281\n",
      "[Epoch 1 Batch 470/628] loss: 0.9364\n",
      "[Epoch 1 Batch 480/628] loss: 0.2980\n",
      "[Epoch 1 Batch 490/628] loss: 1.3049\n",
      "[Epoch 1 Batch 500/628] loss: 1.0156\n",
      "[Epoch 1 Batch 510/628] loss: 1.3623\n",
      "[Epoch 1 Batch 520/628] loss: 1.0964\n",
      "[Epoch 1 Batch 530/628] loss: 1.1838\n",
      "[Epoch 1 Batch 540/628] loss: 1.8013\n",
      "[Epoch 1 Batch 550/628] loss: 0.9830\n",
      "[Epoch 1 Batch 560/628] loss: 1.3366\n",
      "[Epoch 1 Batch 570/628] loss: 1.2748\n",
      "[Epoch 1 Batch 580/628] loss: 1.0932\n",
      "[Epoch 1 Batch 590/628] loss: 1.4069\n",
      "[Epoch 1 Batch 600/628] loss: 1.2700\n",
      "[Epoch 1 Batch 610/628] loss: 1.4065\n",
      "[Epoch 1 Batch 620/628] loss: 1.4059\n",
      "== Start Validation ==\n",
      "[Epoch 1] trn_loss: 1.2241, vld_loss: 1.3153, score: 0.8587, score_each: [0.8274, 0.9255, 0.8546]\n",
      "[Epoch 2 Batch 10/628] loss: 0.9948\n",
      "[Epoch 2 Batch 20/628] loss: 1.3811\n",
      "[Epoch 2 Batch 30/628] loss: 1.5647\n",
      "[Epoch 2 Batch 40/628] loss: 0.8538\n",
      "[Epoch 2 Batch 50/628] loss: 1.0015\n",
      "[Epoch 2 Batch 60/628] loss: 1.2568\n",
      "[Epoch 2 Batch 70/628] loss: 0.9053\n",
      "[Epoch 2 Batch 80/628] loss: 0.9152\n",
      "[Epoch 2 Batch 90/628] loss: 0.9322\n",
      "[Epoch 2 Batch 100/628] loss: 0.8650\n",
      "[Epoch 2 Batch 110/628] loss: 1.1137\n",
      "[Epoch 2 Batch 120/628] loss: 0.5671\n",
      "[Epoch 2 Batch 130/628] loss: 1.3411\n",
      "[Epoch 2 Batch 140/628] loss: 0.8501\n",
      "[Epoch 2 Batch 150/628] loss: 1.1655\n",
      "[Epoch 2 Batch 160/628] loss: 1.5371\n",
      "[Epoch 2 Batch 170/628] loss: 0.9919\n",
      "[Epoch 2 Batch 180/628] loss: 1.0458\n",
      "[Epoch 2 Batch 190/628] loss: 0.9236\n",
      "[Epoch 2 Batch 200/628] loss: 1.6322\n",
      "[Epoch 2 Batch 210/628] loss: 1.2239\n",
      "[Epoch 2 Batch 220/628] loss: 1.2128\n",
      "[Epoch 2 Batch 230/628] loss: 1.1814\n",
      "[Epoch 2 Batch 240/628] loss: 1.2411\n",
      "[Epoch 2 Batch 250/628] loss: 1.4136\n",
      "[Epoch 2 Batch 260/628] loss: 1.0566\n",
      "[Epoch 2 Batch 270/628] loss: 0.5197\n",
      "[Epoch 2 Batch 280/628] loss: 0.7172\n",
      "[Epoch 2 Batch 290/628] loss: 1.4461\n",
      "[Epoch 2 Batch 300/628] loss: 1.3074\n",
      "[Epoch 2 Batch 310/628] loss: 1.0747\n",
      "[Epoch 2 Batch 320/628] loss: 0.9702\n",
      "[Epoch 2 Batch 330/628] loss: 1.7744\n",
      "[Epoch 2 Batch 340/628] loss: 0.9775\n",
      "[Epoch 2 Batch 350/628] loss: 1.1957\n",
      "[Epoch 2 Batch 360/628] loss: 1.2984\n",
      "[Epoch 2 Batch 370/628] loss: 0.9001\n",
      "[Epoch 2 Batch 380/628] loss: 1.3666\n",
      "[Epoch 2 Batch 390/628] loss: 1.0492\n",
      "[Epoch 2 Batch 400/628] loss: 1.1164\n",
      "[Epoch 2 Batch 410/628] loss: 1.0778\n",
      "[Epoch 2 Batch 420/628] loss: 1.0909\n",
      "[Epoch 2 Batch 430/628] loss: 1.1101\n",
      "[Epoch 2 Batch 440/628] loss: 0.9142\n",
      "[Epoch 2 Batch 450/628] loss: 1.1224\n",
      "[Epoch 2 Batch 460/628] loss: 1.4476\n",
      "[Epoch 2 Batch 470/628] loss: 0.8682\n",
      "[Epoch 2 Batch 480/628] loss: 0.7673\n",
      "[Epoch 2 Batch 490/628] loss: 1.2893\n",
      "[Epoch 2 Batch 500/628] loss: 1.2700\n",
      "[Epoch 2 Batch 510/628] loss: 1.2600\n",
      "[Epoch 2 Batch 520/628] loss: 1.5480\n",
      "[Epoch 2 Batch 530/628] loss: 0.8809\n",
      "[Epoch 2 Batch 540/628] loss: 1.3517\n",
      "[Epoch 2 Batch 550/628] loss: 0.4848\n",
      "[Epoch 2 Batch 560/628] loss: 1.0123\n",
      "[Epoch 2 Batch 570/628] loss: 0.9341\n",
      "[Epoch 2 Batch 580/628] loss: 0.7228\n",
      "[Epoch 2 Batch 590/628] loss: 0.7487\n",
      "[Epoch 2 Batch 600/628] loss: 1.1153\n",
      "[Epoch 2 Batch 610/628] loss: 0.7398\n",
      "[Epoch 2 Batch 620/628] loss: 0.8307\n",
      "== Start Validation ==\n",
      "[Epoch 2] trn_loss: 1.0880, vld_loss: 1.1834, score: 0.8915, score_each: [0.8701, 0.9382, 0.8876]\n",
      "[Epoch 3 Batch 10/628] loss: 0.7456\n",
      "[Epoch 3 Batch 20/628] loss: 1.1073\n",
      "[Epoch 3 Batch 30/628] loss: 1.1557\n",
      "[Epoch 3 Batch 40/628] loss: 1.0533\n",
      "[Epoch 3 Batch 50/628] loss: 1.0952\n",
      "[Epoch 3 Batch 60/628] loss: 0.8840\n",
      "[Epoch 3 Batch 70/628] loss: 1.1332\n",
      "[Epoch 3 Batch 80/628] loss: 0.8640\n",
      "[Epoch 3 Batch 90/628] loss: 1.2960\n",
      "[Epoch 3 Batch 100/628] loss: 1.0174\n",
      "[Epoch 3 Batch 110/628] loss: 1.0439\n",
      "[Epoch 3 Batch 120/628] loss: 1.1125\n",
      "[Epoch 3 Batch 130/628] loss: 1.1444\n",
      "[Epoch 3 Batch 140/628] loss: 0.5205\n",
      "[Epoch 3 Batch 150/628] loss: 0.9644\n",
      "[Epoch 3 Batch 160/628] loss: 1.1132\n",
      "[Epoch 3 Batch 170/628] loss: 1.0973\n",
      "[Epoch 3 Batch 180/628] loss: 0.7690\n",
      "[Epoch 3 Batch 190/628] loss: 0.8744\n",
      "[Epoch 3 Batch 200/628] loss: 1.4323\n",
      "[Epoch 3 Batch 210/628] loss: 1.1899\n",
      "[Epoch 3 Batch 220/628] loss: 1.0427\n",
      "[Epoch 3 Batch 230/628] loss: 1.0055\n",
      "[Epoch 3 Batch 240/628] loss: 1.2009\n",
      "[Epoch 3 Batch 250/628] loss: 0.8041\n",
      "[Epoch 3 Batch 260/628] loss: 0.8652\n",
      "[Epoch 3 Batch 270/628] loss: 0.6636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 Batch 280/628] loss: 0.9319\n",
      "[Epoch 3 Batch 290/628] loss: 1.2873\n",
      "[Epoch 3 Batch 300/628] loss: 0.7365\n",
      "[Epoch 3 Batch 310/628] loss: 1.2526\n",
      "[Epoch 3 Batch 320/628] loss: 0.7849\n",
      "[Epoch 3 Batch 330/628] loss: 1.0753\n",
      "[Epoch 3 Batch 340/628] loss: 0.5896\n",
      "[Epoch 3 Batch 350/628] loss: 1.1099\n",
      "[Epoch 3 Batch 360/628] loss: 0.8727\n",
      "[Epoch 3 Batch 370/628] loss: 1.3390\n",
      "[Epoch 3 Batch 380/628] loss: 1.1826\n",
      "[Epoch 3 Batch 390/628] loss: 0.6095\n",
      "[Epoch 3 Batch 400/628] loss: 1.1146\n",
      "[Epoch 3 Batch 410/628] loss: 1.8879\n",
      "[Epoch 3 Batch 420/628] loss: 1.0777\n",
      "[Epoch 3 Batch 430/628] loss: 0.6966\n",
      "[Epoch 3 Batch 440/628] loss: 0.6807\n",
      "[Epoch 3 Batch 450/628] loss: 1.2837\n",
      "[Epoch 3 Batch 460/628] loss: 0.8716\n",
      "[Epoch 3 Batch 470/628] loss: 0.9771\n",
      "[Epoch 3 Batch 480/628] loss: 0.5411\n",
      "[Epoch 3 Batch 490/628] loss: 0.8564\n",
      "[Epoch 3 Batch 500/628] loss: 1.0098\n",
      "[Epoch 3 Batch 510/628] loss: 1.1918\n",
      "[Epoch 3 Batch 520/628] loss: 0.8603\n",
      "[Epoch 3 Batch 530/628] loss: 1.2427\n",
      "[Epoch 3 Batch 540/628] loss: 1.2388\n",
      "[Epoch 3 Batch 550/628] loss: 1.4705\n",
      "[Epoch 3 Batch 560/628] loss: 1.2519\n",
      "[Epoch 3 Batch 570/628] loss: 0.7188\n",
      "[Epoch 3 Batch 580/628] loss: 0.6077\n",
      "[Epoch 3 Batch 590/628] loss: 0.9942\n",
      "[Epoch 3 Batch 600/628] loss: 0.9171\n",
      "[Epoch 3 Batch 610/628] loss: 1.1260\n",
      "[Epoch 3 Batch 620/628] loss: 0.7314\n",
      "== Start Validation ==\n",
      "[Epoch 3] trn_loss: 1.0103, vld_loss: 1.1960, score: 0.9209, score_each: [0.8961, 0.9521, 0.9395]\n",
      "[Epoch 4 Batch 10/628] loss: 1.1476\n",
      "[Epoch 4 Batch 20/628] loss: 1.5090\n",
      "[Epoch 4 Batch 30/628] loss: 1.2881\n",
      "[Epoch 4 Batch 40/628] loss: 1.2513\n",
      "[Epoch 4 Batch 50/628] loss: 1.1027\n",
      "[Epoch 4 Batch 60/628] loss: 0.9438\n",
      "[Epoch 4 Batch 70/628] loss: 0.5861\n",
      "[Epoch 4 Batch 80/628] loss: 0.5430\n",
      "[Epoch 4 Batch 90/628] loss: 1.0239\n",
      "[Epoch 4 Batch 100/628] loss: 1.1082\n",
      "[Epoch 4 Batch 110/628] loss: 0.6722\n",
      "[Epoch 4 Batch 120/628] loss: 1.4748\n",
      "[Epoch 4 Batch 130/628] loss: 0.9717\n",
      "[Epoch 4 Batch 140/628] loss: 0.9167\n",
      "[Epoch 4 Batch 150/628] loss: 0.6124\n",
      "[Epoch 4 Batch 160/628] loss: 0.8597\n",
      "[Epoch 4 Batch 170/628] loss: 0.9197\n",
      "[Epoch 4 Batch 180/628] loss: 0.7638\n",
      "[Epoch 4 Batch 190/628] loss: 0.7914\n",
      "[Epoch 4 Batch 200/628] loss: 0.8805\n",
      "[Epoch 4 Batch 210/628] loss: 1.1787\n",
      "[Epoch 4 Batch 220/628] loss: 0.6625\n",
      "[Epoch 4 Batch 230/628] loss: 1.1050\n",
      "[Epoch 4 Batch 240/628] loss: 1.1117\n",
      "[Epoch 4 Batch 250/628] loss: 1.1091\n",
      "[Epoch 4 Batch 260/628] loss: 0.8201\n",
      "[Epoch 4 Batch 270/628] loss: 0.6213\n",
      "[Epoch 4 Batch 280/628] loss: 0.5578\n",
      "[Epoch 4 Batch 290/628] loss: 0.7991\n",
      "[Epoch 4 Batch 300/628] loss: 0.9371\n",
      "[Epoch 4 Batch 310/628] loss: 0.9075\n",
      "[Epoch 4 Batch 320/628] loss: 1.2310\n",
      "[Epoch 4 Batch 330/628] loss: 0.7067\n",
      "[Epoch 4 Batch 340/628] loss: 0.9202\n",
      "[Epoch 4 Batch 350/628] loss: 1.3492\n",
      "[Epoch 4 Batch 360/628] loss: 0.8324\n",
      "[Epoch 4 Batch 370/628] loss: 1.2630\n",
      "[Epoch 4 Batch 380/628] loss: 1.1969\n",
      "[Epoch 4 Batch 390/628] loss: 0.9498\n",
      "[Epoch 4 Batch 400/628] loss: 1.0186\n",
      "[Epoch 4 Batch 410/628] loss: 1.2348\n",
      "[Epoch 4 Batch 420/628] loss: 0.6084\n",
      "[Epoch 4 Batch 430/628] loss: 1.0085\n",
      "[Epoch 4 Batch 440/628] loss: 0.9369\n",
      "[Epoch 4 Batch 450/628] loss: 1.3880\n",
      "[Epoch 4 Batch 460/628] loss: 0.8841\n",
      "[Epoch 4 Batch 470/628] loss: 1.2243\n",
      "[Epoch 4 Batch 480/628] loss: 0.6093\n",
      "[Epoch 4 Batch 490/628] loss: 1.1558\n",
      "[Epoch 4 Batch 500/628] loss: 0.5069\n",
      "[Epoch 4 Batch 510/628] loss: 1.1526\n",
      "[Epoch 4 Batch 520/628] loss: 0.9998\n",
      "[Epoch 4 Batch 530/628] loss: 1.0577\n",
      "[Epoch 4 Batch 540/628] loss: 0.9171\n",
      "[Epoch 4 Batch 550/628] loss: 1.0859\n",
      "[Epoch 4 Batch 560/628] loss: 0.7171\n",
      "[Epoch 4 Batch 570/628] loss: 1.0543\n",
      "[Epoch 4 Batch 580/628] loss: 0.9100\n",
      "[Epoch 4 Batch 590/628] loss: 0.9439\n",
      "[Epoch 4 Batch 600/628] loss: 0.7305\n",
      "[Epoch 4 Batch 610/628] loss: 0.7720\n",
      "[Epoch 4 Batch 620/628] loss: 0.9521\n",
      "== Start Validation ==\n",
      "[Epoch 4] trn_loss: 0.9635, vld_loss: 1.1583, score: 0.9199, score_each: [0.9026, 0.9460, 0.9285]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "best_score = -1\n",
    "log_interval = 10\n",
    "training_stats = []\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch_id in range(num_epochs):\n",
    "\n",
    "    ################################################################################\n",
    "    # ==> Training phase\n",
    "    ################################################################################    \n",
    "    trn_loss = []\n",
    "    model.train()\n",
    "    \n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_id, (inputs, targets) in enumerate((trn_loader)):\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        targets_gra = targets[:, 0]\n",
    "        targets_vow = targets[:, 1]\n",
    "        targets_con = targets[:, 2]\n",
    "                    \n",
    "        # 50%의 확률로 원본 데이터 그대로 사용    \n",
    "        if np.random.rand() < 0.5:\n",
    "            logits = model(inputs)\n",
    "            grapheme = logits[:, :168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "            \n",
    "            loss1 = loss_fn(grapheme, targets_gra)\n",
    "            loss2 = loss_fn(vowel, targets_vow)\n",
    "            loss3 = loss_fn(cons, targets_con) \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            lam = np.random.beta(1.0, 1.0) \n",
    "            rand_index = torch.randperm(inputs.size()[0])\n",
    "            shuffled_targets_gra = targets_gra[rand_index]\n",
    "            shuffled_targets_vow = targets_vow[rand_index]\n",
    "            shuffled_targets_con = targets_con[rand_index]\n",
    "            \n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            # 픽셀 비율과 정확히 일치하도록 lambda 파라메터 조정  \n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            grapheme = logits[:,:168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "            \n",
    "            loss1 = loss_fn(grapheme, targets_gra) * lam + loss_fn(grapheme, shuffled_targets_gra) * (1. - lam)\n",
    "            loss2 = loss_fn(vowel, targets_vow) * lam + loss_fn(vowel, shuffled_targets_vow) * (1. - lam)\n",
    "            loss3 = loss_fn(cons, targets_con) * lam + loss_fn(cons, shuffled_targets_con) * (1. - lam)\n",
    "        \n",
    "        loss = 0.5 * loss1 + 0.25 * loss2 + 0.25 * loss3    \n",
    "        trn_loss.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Printing vital information\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            s = f'[Epoch {epoch_id} Batch {batch_id+1}/{len(trn_loader)}] ' \\\n",
    "            f'loss: {running_loss / log_interval:.4f}'\n",
    "            print(s)\n",
    "            running_loss = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    # Measure how long this epoch took.\n",
    "    trn_time = format_time(time.time() - t0)        \n",
    "\n",
    "    ################################################################################\n",
    "    # ==> Validation phase\n",
    "    ################################################################################\n",
    "    val_loss = []\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in vld_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            logits = model(inputs)\n",
    "            grapheme = logits[:,:168]\n",
    "            vowel = logits[:, 168:179]\n",
    "            cons = logits[:, 179:]\n",
    "\n",
    "            loss= 0.5* loss_fn(grapheme, targets[:,0]) + 0.25*loss_fn(vowel, targets[:,1]) + \\\n",
    "            0.25*loss_fn(vowel, targets[:,2])\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "\n",
    "            grapheme = grapheme.cpu().argmax(dim=1).data.numpy()\n",
    "            vowel = vowel.cpu().argmax(dim=1).data.numpy()\n",
    "            cons = cons.cpu().argmax(dim=1).data.numpy()\n",
    "\n",
    "            val_true.append(targets.cpu().numpy())\n",
    "            val_pred.append(np.stack([grapheme, vowel, cons], axis=1))                \n",
    "\n",
    "    val_true = np.concatenate(val_true)\n",
    "    val_pred = np.concatenate(val_pred)\n",
    "    val_loss = np.mean(val_loss)\n",
    "    trn_loss = np.mean(trn_loss)\n",
    "\n",
    "    score_g = recall_score(val_true[:,0], val_pred[:,0], average='macro')\n",
    "    score_v = recall_score(val_true[:,1], val_pred[:,1], average='macro')\n",
    "    score_c = recall_score(val_true[:,2], val_pred[:,2], average='macro')\n",
    "    final_score = np.average([score_g, score_v, score_c], weights=[2,1,1])\n",
    "   \n",
    "    # Printing vital information\n",
    "    print('== Start Validation ==')\n",
    "    s = f'[Epoch {epoch_id}] ' \\\n",
    "    f'trn_loss: {trn_loss:.4f}, vld_loss: {val_loss:.4f}, score: {final_score:.4f}, ' \\\n",
    "    f'score_each: [{score_g:.4f}, {score_v:.4f}, {score_c:.4f}]'          \n",
    "    print(s)\n",
    "\n",
    "    ################################################################################\n",
    "    # ==> Save checkpoint and training stats\n",
    "    ################################################################################        \n",
    "    if final_score > best_score:\n",
    "        best_score = final_score\n",
    "        state_dict = model.cpu().state_dict()\n",
    "        model = model.cuda()\n",
    "        torch.save(state_dict, 'model.pt')\n",
    "        \n",
    "    # Record all statistics from this epoch\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_id + 1,\n",
    "            'trn_loss': trn_loss,\n",
    "            'trn_time': trn_time,            \n",
    "            'val_loss': val_loss,\n",
    "            'score': final_score,\n",
    "            'score_g': score_g,\n",
    "            'score_v': score_v,\n",
    "            'score_c': score_c            \n",
    "        }\n",
    "    )            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>score_c</th>\n",
       "      <th>score_g</th>\n",
       "      <th>score_v</th>\n",
       "      <th>trn_loss</th>\n",
       "      <th>trn_time</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8163</td>\n",
       "      <td>0.7981</td>\n",
       "      <td>0.7921</td>\n",
       "      <td>0.8829</td>\n",
       "      <td>1.5630</td>\n",
       "      <td>0:04:43</td>\n",
       "      <td>1.4987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8587</td>\n",
       "      <td>0.8546</td>\n",
       "      <td>0.8274</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>1.2241</td>\n",
       "      <td>0:04:45</td>\n",
       "      <td>1.3153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8915</td>\n",
       "      <td>0.8876</td>\n",
       "      <td>0.8701</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>1.0880</td>\n",
       "      <td>0:04:37</td>\n",
       "      <td>1.1834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9209</td>\n",
       "      <td>0.9395</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>1.0103</td>\n",
       "      <td>0:04:41</td>\n",
       "      <td>1.1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9199</td>\n",
       "      <td>0.9285</td>\n",
       "      <td>0.9026</td>\n",
       "      <td>0.9460</td>\n",
       "      <td>0.9635</td>\n",
       "      <td>0:04:40</td>\n",
       "      <td>1.1583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score  score_c  score_g  score_v  trn_loss trn_time  val_loss\n",
       "epoch                                                                \n",
       "1      0.8163   0.7981   0.7921   0.8829    1.5630  0:04:43    1.4987\n",
       "2      0.8587   0.8546   0.8274   0.9255    1.2241  0:04:45    1.3153\n",
       "3      0.8915   0.8876   0.8701   0.9382    1.0880  0:04:37    1.1834\n",
       "4      0.9209   0.9395   0.8961   0.9521    1.0103  0:04:41    1.1960\n",
       "5      0.9199   0.9285   0.9026   0.9460    0.9635  0:04:40    1.1583"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "display(df_stats)\n",
    "\n",
    "#model.load_state_dict(torch.load('./model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
